[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking notes and exercises",
    "section": "",
    "text": "Preface\nFor Summer 2022, I decided to read “Statistical Rethinking” by Richard McElreath (McElreath (2020)). This book contains my notes on the chapter and my solutions to the exercises. I typically don’t take very detailed notes, so those will likely only be what I consider the key points of the chapter or the parts that I find difficult to remember.\nI will attempt to complete every exercise, but if I get stuck for too long or I’m on some other deadline (see my now page to see what else I might be working on), that may not happen.\nMost of all, I plan to “have fun with it.”\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. New York: Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "cp1.html#chapter-notes",
    "href": "cp1.html#chapter-notes",
    "title": "1  The Golem of Prague",
    "section": "1.1 Chapter notes",
    "text": "1.1 Chapter notes\n\nThe statistical model as a golem: models cannot think or see context. They are designed to do specific jobs and carry out their job exactly as they are instructed to, without regard for the consequences. Golems can make numbers, but we have to make golems and interpret the results.\nStatistics often lacks a coherent epistemiology. We need to understand how statistical models related to causal models, and how causal models relate to scientific hypotheses.\n“Folk Popperism:” many scientists believe that null hypothesis significance testing reflects Popper’s belief that scientific hypotheses must be falsifiable. But really, one statistical model is related to multiple process models, and one process model is related to many scientific hypotheses.\nAdditionally, the NHST paradigm often ignores the fallibility of measurements and the idea that falsification can be spurious. And continuous hypotheses cannot simply be falsified.\nMy favorite quote from this chapter (and it is full of great quotes): “So, if attempting to mimic falsification is not a genreally useful approach to statistical methods, what are we to do? We are to model.”\nThe rest of the chapter details the approaches that the book will take with respect to causal modeling and bayesian analysis methods.\nAn equally good title for this chapter might be “the statistical nihilist’s manifesto.”",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "cp1.html#exercises",
    "href": "cp1.html#exercises",
    "title": "1  The Golem of Prague",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\n(This space intentionally left blank.)",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "cp2.html#chapter-notes",
    "href": "cp2.html#chapter-notes",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.1 Chapter notes",
    "text": "2.1 Chapter notes\n\nDistinction between the small world, the self-contained world of the model, and the large world, the context in which the model is used. Remember that golems can’t see context.\nBayesian Inference Is Just Counting – explanation using tree diagrams. I think this is a good example for simple problems but I don’t think it generalizes well to real-life data analysis examples.\nBayesian updating: if you observe more data, you can use Bayes’ Rule to update your plausibilities for each of the possible results.\n“There is no free lunch…a Bayesian golem must choose an initial plausibility, and a frequentist golem must choose an estimator. Both golems pay for lunch with their assumptions.”\nBayesian inference doesn’t distinguish between data and parameters in the same way that frequentist inference does. Instead, data are observed variable, and parameters are unobserved variables.\nLikelihoods, priors, posteriors, Bayes’ rule, and other things I didn’t take notes on were covered in this section.\nThere are multiple numerical techniques for approximating Bayesian models, different “motors” for the golems. These include grid approximation, quadratic approximation, and Markov Chain Monte Carlo. How you fit the model is part of the model (the engine is part of the golem), and different fitting routines have different compromises and advantages.\nGrid approximation: estimate the posterior probability of several different values of the parameter via brute force. I did a rough version on this in my blog post on Bayesian updating.\n\nDefine the grid of posterior values. You have to choose the set of points for evaluation.\nCompute the value of the prior at each parameter value on the grid.\nCompute the likelihood at each parameter value.\nCompute the unstandardized posterior at each parameter value, by multiplying the prior and the likliehood.\nStandardize the posterior by dividing each value by the sum of all values.\n\nQuadratic approximation: as the number of parameters increases, the number of evaluations becomes \\(\\text{number of points} ^ \\text{number of parameters}\\). So more efficient methods (that make more assumptions are needed.) Quap assumes that the posterior is approximately Gaussian near the peak, essentially representing the log-posterior density as a quadratic function. N.b. quadratic approximation improves with the number of data points.\n\nFind the posterior mode, usually accomplished by some optimization algorithm based on the gradient of the posterior. “The golem does not know where the peak is, but it does know the slope under its feet.”\nEstimate the curvature near the peak, which is sufficient to compute a quadratic approximation of the entire posterior distribution.\n\nSee pp 41–43 for grid and quadratic approximation examples.\nMarkov chain Monte Carlo: useful for computing many models that fail for grid or quadratic approximation, and may have thousands of parameters. The final posterior may not even have a closed form. MCMC techniques rely on sampling from the posterior distribution rather than directly attempting to approximate the posterior. Since McElreath didn’t run his MCMC example in the book, I’ve included it here because I wanted to see the result.\n\n\nset.seed(370)\nn_samples &lt;- 10000\np &lt;- rep(NA, n_samples)\np[1] &lt;- 0.5\nw &lt;- 6\nl &lt;- 3\nfor (i in 2:n_samples) {\n    p_new &lt;- rnorm(1, p[i-1], 0.1)\n    if (p_new &lt; 0) {p_new &lt;- abs(p_new)}\n    if (p_new &gt; 1) {p_new &lt;- 2 - p_new}\n    q0 &lt;- dbinom(w, w + l, p[i-1])\n    q1 &lt;- dbinom(w, w + l, p_new)\n    p[i] &lt;- ifelse(runif(1) &lt; q1/q0, p_new, p[i-1])\n}\n\nplot(density(p), xlim = c(0, 1))\ncurve(dbeta(x, w + 1, l + 1), lty = 2, add = TRUE)",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "cp2.html#exercises",
    "href": "cp2.html#exercises",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n2E1. The expression corresponding to the statement the probability of rain on Monday is \\[\\text{Pr}(\\text{rain} \\mid \\text{Monday}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{Monday})}.\\]\n2E2. The statement corresponding to the expression \\[\\text{Pr}(\\text{Monday} \\mid \\text{rain})\\] is the probability that it is Monday, given that it is raining.\n2E3. The expression correspondonding to the statement the probability that it is Monday, given that it is raining is \\[ \\text{Pr}(\\text{Monday} \\mid \\text{rain}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{rain})} = \\frac{\\text{Pr}(\\text{rain} \\mid \\text{Monday})\\text{Pr}(\\text{Monday})}{\\text{Pr}(\\text{rain})}\\]\n2E4. Based on Bruno de Finetti’s statement “PROBABILITY DOES NOT EXIST,” the statement the probability of water is 0.7 from the earlier example is a statement about our beliefs. We know that there are several other factors underlying the globe-tossing experiment, but we cannot measure all of those factors, but sweeping them under the rug, we believe that about 70% of the time, our result should be water. The frequentist interpretation of this is as a long-run average probability, but in the bayesian interpretation, this is our prior belief for the next time we perform the experiment.\n2M1. Assuming a uniform prior for \\(p\\), compute the grid approximate posterior for each of the following sets of observations: W, W, W, W, W, W, L, and L, W, W, L, W, W, W.\n\n# Define a function that computes the grid-approximate posterior with uniform\n# prior for p given a sampled number of water and land tosses.\nglobe_post &lt;- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid &lt;- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior &lt;- rep(1, times = length(p_grid))\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood &lt;- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior &lt;- likelihood * prior\n    \n    # Standardize the posterior\n    posterior &lt;- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post(3, 0)\nglobe_post(3, 1)\nglobe_post(5, 2)\n\n\n\n\n\n\n\n\n2M2. Repeat the grid approximate calculations assuming a prior for \\(p\\) of the form \\[f(p) = \\begin{cases} 0, & p &lt; 0.5 \\\\ k, & p \\geq 0.5\\end{cases}.\\] Note that for \\(\\int_0^1 f(p) \\ dp = 1,\\) we must have \\(k = 2\\).\n\nglobe_post_step_prior &lt;- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid &lt;- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior &lt;- ifelse(p_grid &lt; 0.5, 0, 1)\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood &lt;- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior &lt;- likelihood * prior\n    \n    # Standardize the posterior\n    posterior &lt;- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post_step_prior(3, 0)\nglobe_post_step_prior(3, 1)\nglobe_post_step_prior(5, 2)\n\n\n\n\n\n\n\n\n2M3. We want to compute \\(\\text{Pr}(\\text{Earth} \\mid \\text{land})\\) given the following information.\n\n\\(\\text{Pr}(\\text{land} \\mid \\text{Earth}) = 0.3\\)\n\\(\\text{Pr}(\\text{land} \\mid \\text{Mars}) = 1.0\\)\n\\(\\text{Pr}(\\text{Earth}) = \\text{Pr}(\\text{Mars}) = 0.5\\)\n\nWe can deduce that \\[\\begin{align*}\n\\text{Pr}(\\text{land}) &= \\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth}) + \\text{Pr}(\\text{land} \\mid \\text{Mars})\\cdot\\text{Pr}(\\text{Mars}) \\\\\n&= (0.3)(0.5) + (1.0)(0.5) = 0.65.\n\\end{align*}\\]\nSo we compute \\[\\begin{align*}\n\\text{Pr}(\\text{Earth} \\mid \\text{land}) &= \\frac{\\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth})}{\\text{Pr}(\\text{land})} \\\\\n&= \\frac{(0.3)(0.5)}{0.65} \\approx 0.23.\n\\end{align*}\\]\n2M4. We have a deck of three cards: one with two white sides, one with a black side and a white side, and one with two black sides. If we draw one card with the black side up, what is the probability that the other side is also black?\nWe can solve this by directly calculating the conditional probability.\n\\[\\begin{align*}\n\\text{Pr}(\\text{black down} \\mid \\text{black up}) &= \\frac{\\text{Pr}(\\text{black down}, \\text{black up})}{ \\text{Pr}(\\text{black up})} \\\\\n&= \\frac{1 / 3}{1 / 2} = \\frac{2}{3}.\n\\end{align*}\\]\nWe get \\(\\frac{1}{3}\\) for the joint probability since there are three cards, and only one of them has black on both sides. We get the individual probability of one black side being up as \\(\\frac{1}{2}\\) by noticing that there are 6 sides that could be facing up, and 3 of them are black sides.\nThe way that I think scales better to the rest of the problems in this section is by counting the number of ways to get this answer.\n\nIf the card we drew was white on both sides, there are 0 ways we could observe a black side facing up.\nIf the card we drew was white on one side, there is 1 way to observe a black side facing up.\nIf the card we drew was black on both sides, there are 2 ways to observe a black side facing up (it could be either side).\n\nSo out of the three possible ways to generate the situation we observed, two of them have a second black side on the bottom, giving us our \\(\\frac{2}{3}\\) probability.\n2M5. If we add an extra card with two black sides, we update our calculations.\n\nStill 0 ways if we draw the white/white card.\nThere’s still only 1 way to observe a black side facing up with a B/W card.\nHowever, there are now 4 different black sides we could observe facing up with a B/B card.\n\nSo out of 5 ways to observe a black side facing up, four of them have the other side black, giving us a \\(\\frac{4}{5}\\) probability.\n2M6. Now, we suppose that the black ink is heavy. For every one way to pull the B/B card, there are two ways to pull the B/W card and three ways to pull the W/W card.\nThe number of ways to get one black side up has not changed from problem 2M4. There’s still one way with a B/W card and 2 ways with a B/B card. However, now there are \\(2 \\times 1 = 2\\) ways to get the B/W card with the black side up, so the probability of the other side being black is now \\(\\frac{2}{4} = \\frac{1}{2}\\).\n2M7. Now suppose we draw one card and get a black side facing up, then we draw a second card and get a white side facing up. We want to find the probability that the first card was the B/B card.\n\nThere are two ways for a black side to face up if the first card is B/B (either side could be face up). If this is the case, there are three ways for the second card to show white (one way if it is B/W or two ways if it is W/W). So, there are \\(2 \\times 3 = 6\\) ways for us to observe what we did if this is true.\nIf the first card is B/W, there is only one way for a black side to be face up. Then, there are two ways for the second card to face up white (either side of the W/W card), giving \\(1 \\times 2 = 2\\) ways for our data to occur if this is the truth.\nThe W/W card cannot be the first card, the data we observed rules this out.\n\nTherefore, there is a \\(6 / 8 = 3 /4\\) probability that the first card is black on the bottom as well.\n2H1. There are two species of panda, A and B, that are equally likely in the wild. In species A, twins occur 10% of the time. In species B, twins occur 20% of the time. Both species only have twins or singleton infants. If a panda of unknown species gives birth to twins, what is the probability her next birth will also be twins?\nSo, the probability we are interested in is \\(P(\\text{twins} \\mid \\text{twins})\\). This is confusing, so I’ll say \\(P(\\text{twins}^* \\mid \\text{twins})\\).\nWe can calculate that \\[\\begin{align*}\nP(\\text{twins}) &= P(\\text{twins} \\mid A) P(A) + P(\\text{twins} \\mid B) P(B) \\\\\n&= (0.1)(0.5) + (0.2)(0.5) = 0.15.\n\\end{align*}\\]\nFrom the definition of conditional probability, we know that \\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{P(\\text{twins}^*, \\text{twins})}{P(\\text{twins})},\\] so it remains to calculate \\(P(\\text{twins}^*, \\text{twins})\\). If we assume that births are independent, we can calculate\n\\[\\begin{align*}\nP(\\text{twins}^*, \\text{twins}) &= P(\\text{twins}^*, \\text{twins} \\mid A)P(A) + P(\\text{twins}^*, \\text{twins} \\mid B)P(B) \\\\\n&= (0.1)^2(0.5) + (0.2)^2(0.5) = 0.025.\n\\end{align*}\\]\nThen, \\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{0.025}{0.15} = \\frac{1}{6}.\\]\nSo if a panda of unknown species gives birth to twins, the probability that her next birth will also be twins is \\(1/6\\), given the information we have.\n2H2. Now we want to find the probability that the panda is species A, given that she gave birth to twins, i.e. \\(P(A \\mid \\text{twins})\\). Recall that \\(P(\\text{twins}) = 0.15,\\) \\(P(\\text{twins} \\mid A) = 0.1,\\) and \\(P(A) = 0.5.\\) Then,\n\\[P(A \\mid \\text{twins}) = \\frac{(0.5)(0.1)}{0.15} = \\frac{1}{3}.\\]\n2H3. Suppose the panda has a second birth, a singleton infant. What is \\(P(A)\\) now?\nSince we’ve already estimated \\(P(A)\\) for the first birth, I’ll call this \\(P(A_\\text{prior})\\), instead of working out the entire conditional probability, we can update this estimate with our new information. What we want to calculate is\n\\[P(A_\\text{posterior}) = \\frac{P(A_\\text{prior}) P(\\text{singleton} \\mid A)}{P(\\text{singleton})}.\\]\nNow, the probability of having a singleton is mutually exclusive with the probability of having twins (since we know that these pandas never have more than twins), so \\[P(\\text{singleton} \\mid A) = 1 - 0.1 = 0.9. \\]\nWe get the singleton probability for pandas of species B in the same way. Next, we need to calculate the probability of a singleton, taking our prior probability that the panda is species A into account. We get \\[P(\\text{singleton}) = \\frac{1}{3} (0.9) + \\frac{2}{3} (0.8) = \\frac{5}{6}.\\]\nTherefore, \\[P(A_\\text{posterior}) = \\frac{(1/3)(0.9)}{5/6} = 0.36.\\]\n2H4. The new test for panda species has probabilities \\(P(\\text{test } A \\mid A) = 0.8\\) and \\(P(\\text{test } B \\mid B) = 0.65.\\) We want to know the probability that our panda is species A, given that her test result was A.\nFirst we will ignore our prior probability and calculate the probability that any random panda is species A given that they test A.\nWe calculate that \\(P(\\text{test } A \\mid B) = 1 - 0.65 = 0.35\\) and therefore that \\[\\begin{align*}\nP(\\text{test } A) &= P(A)P(\\text{test } A \\mid A) + P(B)P(\\text{test } B \\mid B) \\\\\n&= (0.5)(0.8) + (0.5)(0.35) = 0.575.\n\\end{align*}\\]\nThen, \\[P(A \\mid \\text{test } A) = \\frac{(0.5)(0.8)}{0.575} \\approx 0.6957.\\]\nNow, if we use our prior probability, \\(P(A_\\text{prior}) = 0.36\\), we instead get that\n\\[\\begin{align*}\nP(\\text{test } A) &= (0.36)(0.8) + (0.64)(0.35) = 0.512 \\quad\\text{and } \\\\\nP(A \\mid \\text{test } A) &= \\frac{(0.36)(0.8)}{0.512} \\approx 0.5612.\n\\end{align*}\\]",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "cp3.html#chapter-notes",
    "href": "cp3.html#chapter-notes",
    "title": "3  Sampling the Imaginary",
    "section": "3.1 Chapter notes",
    "text": "3.1 Chapter notes\n\n“Fetishizing precision to the fifth decimal place will not improve your science.”\nThe HDPI and PI methods for constructing credible intervals are similar for bell-shaped curves, but will be different for highly skewed curves where the mode and mean are different. “If the choice of interval affects the inference, you are better off plotting the entire posterior distribution.”\nThe HDPI also has higher simulation variance, that is, it needs more samples than the PI to arrive at a stable result.\nChoosing a point estimate, such as the mean, median, or mode (maximum a posteriori value) can be difficult.\nImportantly, different loss functions imply different estimates. The absolute loss function, \\(L(\\theta, \\hat{\\theta}) = \\left| \\theta - \\hat{\\theta} \\right|\\) is minimized by the median; the quadratic loss function, \\(L(\\theta, \\hat{\\theta}) = \\left( \\theta - \\hat{\\theta} \\right)^2\\), is minimized by the mean; and the 0-1 loss function (different for discrete and continuous problems) is minimized by the mode and corresponds to maximizing the posterior likelihood.\nWhile frequentist methods rely on sampling distributions and the (theoretical) physical act of random sampling, Bayesian models do not! The “sampling” we are doing here is small world sampling – our samples are from the model, we don’t expect them to be “real.”\nDummy data generated by the prior predictive distribution, the distribution of the parameters of interest using only the priors and not the data, help us build models. These simulations can tell us whether the priors are reasonable or not.\nOnce we update the model with the data, we can generate samples from the posterior predictive distribution. These samples can help us check how accurate the model is or how well it fit. This distribution is “honest” because it propagates the uncertainty embodied in the posterior distribution of the parameter of interest.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "cp3.html#exercises",
    "href": "cp3.html#exercises",
    "title": "3  Sampling the Imaginary",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\nFor the easy exercises, we need the following code given in the book.\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000)\nlikelihood &lt;- dbinom(6, size = 9, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\n3E1. How much posterior probability lies below \\(p = 0.2\\)?\n\nmean(samples &lt;= 0.2)\n\n[1] 4e-04\n\n\n3E2. How much posterior probability lies below \\(p = 0.8\\)?\n\nmean(samples &lt;= 0.8)\n\n[1] 0.8884\n\n\n3E3. How much posterior probability lies between \\(p = 0.2\\) and \\(p = 0.8\\).\nWe could calculate this directly.\n\nmean((samples &gt;= 0.2) & (samples &lt;= 0.8))\n\n[1] 0.888\n\n\nOr if we had stored the previous calculations, we could have used those instead.\n\nmean(samples &lt;= 0.8) - mean(samples &lt;= 0.2)\n\n[1] 0.888\n\n\n3E4. 20% of the posterior probability lies below which value of \\(p\\)?\n\nquantile(samples, probs = c(0.2))\n\n      20% \n0.5185185 \n\n\n3E5. 20% of the posterior probability lies above which value of \\(p\\)?\n\nquantile(samples, probs = c(0.8))\n\n      80% \n0.7557558 \n\n\n3E6. Which values of \\(p\\) contain the narrowest interval equal to 66% of the posterior probability?\n\nrethinking::HPDI(samples, prob = 0.66)\n\n    |0.66     0.66| \n0.5085085 0.7737738 \n\n\n3E7. Which values of \\(p\\) contain 66% of the posterior probability, assuming equal posterior probability both above and below the interval?\n\nrethinking::PI(samples, prob = 0.66)\n\n      17%       83% \n0.5025025 0.7697698 \n\n\n3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\n3M2. Draw 10000 samples from the grid approximate prior abnove. Then use the samples to calculate the 90% HDPI for \\(p\\).\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = 0.90)\n\n     |0.9      0.9| \n0.3343343 0.7217217 \n\n\n3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in \\(p\\). What is the probability of observing 8 water in 15 tosses?\n\nppc &lt;- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.1499\n\n\n3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.\n\nppc2 &lt;- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.1842\n\n\n3M5. Start over at 3M1, this time using a prior that is zero below \\(p = 0.5\\) and a constant above \\(p = 0.5\\).\nFirst we approximate the posterior and take samples.\n\n# I should name these different things but I am not going to.\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- ifelse(p_grid &lt; 0.5, 0, 2)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nNow we’ll do the first posterior predictive check and estimate the probability. Note that the true probability (if \\(p = 0.7\\)) is \\(0.08113\\).\n\nppc &lt;- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.163\n\n\nIt looks like this estimate is actually slightly worse with this prior than it was with the uniform prior. However, they are fairly similar.\nAnd the second check. Note that the true probability is \\(0.2668279\\)\n\nppc2 &lt;- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.2353\n\n\nThis estimate is much closer to the true value than the previous estimate was. It seems that this prior allows us to more accurate estimate probabilities close to the true value (\\(p = 0.7\\)), but not near the lower boundary for the prior. We can examine the histogram.\n\nrethinking::simplehist(ppc)\n\n\n\n\n\n\n\n\nWe can see that the low values are extremely low, but so are the high values. We would expect the mode to be around 10 or 11, but since we observed 8 / 15, it makes sense that we get a higher estimate of the probability of this occurring than what we “know” is true.\n3M6. We want to construct a 99% percentile interval of the posterior distribution of \\(p\\) that is only 0.05 wide. How many times will we have to toss the globe to do this?\nTo me, this question seems phrased in the general, but I think it is impossible to answer in general. So we’ll do our best. First let’s look at the width of the current PI.\n\nrethinking::PI(samples, prob = 0.99) |&gt; diff()\n\n     100% \n0.3243243 \n\n\nThat’s much larger than what we want, but we only tossed the ball 15 times. So we’ll need to do some simulating to solve this problem. I know this is not the “true” probably, but for the sake of keeping with this model, I’ll make sure all of our larger samples have (approximately) the same \\(8/15\\) probability of water.\nI’ll also continue using the flat prior. The answer to this question depends on both the “true” value of \\(p\\) and the prior that we used.\n\none_sim &lt;- function(N) {\n    likelihood &lt;- dbinom(floor(N * (8/15)), size = N, prob = p_grid)\n    posterior &lt;- likelihood * prior\n    posterior &lt;- posterior / sum(posterior)\n\n    set.seed(100)\n    samples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n    \n    out &lt;- rethinking::PI(samples, prob = 0.99) |&gt; diff()\n    return(out)\n}\n\nmy_n &lt;- seq(from = 10, to = 5000, by = 10)\nsim_res &lt;- purrr::map_dbl(my_n, one_sim)\nplot(sim_res ~ my_n, type = \"l\")\nabline(h = 0.05, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nVisually, we can see that around 3000 samples are necessary, let’s get the exact estimate.\n\nindex &lt;- min(which(sim_res &lt; 0.05))\ncat(\"n: \", my_n[index], \"; width: \", sim_res[index], sep = \"\")\n\nn: 2740; width: 0.04905405",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "cp3.html#hard-problems",
    "href": "cp3.html#hard-problems",
    "title": "3  Sampling the Imaginary",
    "section": "3.3 Hard Problems",
    "text": "3.3 Hard Problems\nFor the hard problems, we need to load the indicated data set.\n\ndata(homeworkch3, package = \"rethinking\")\ncombined &lt;- c(birth1, birth2)\n\n3H1. Use grid approximation to compute the posterior distribution for the probability of a birth being a boy.\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, times = 1000) # Uniform prior\nlikelihood &lt;- dbinom(sum(combined), size = length(combined), prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nplot(posterior ~ p_grid, type = \"l\")\n\n\n\n\n\n\n\n\n3H2. Draw 10000 random samples from the posterior, and use these to estimate the 50, 89, and 97 percent HDPIs.\n\nset.seed(100)\nsamples &lt;- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = c(0.5, 0.89, 0.97))\n\n    |0.97     |0.89      |0.5      0.5|     0.89|     0.97| \n0.4824825 0.4994995 0.5265265 0.5725726 0.6076076 0.6296296 \n\n\n3H3. Simulate 10,000 replicates of 200 births. Compare the distribution of predicted counts to the actual count. Does it look like the model fits the data well?\n\nset.seed(100)\nppc &lt;- rbinom(1e4, size = 200, prob = samples)\nrethinking::simplehist(ppc)\nabline(v = sum(combined), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nIn this particular simulation, the observed value (111 boys) is a central, likely outcome of the posterior predictive distribution. The model seems to fit the data well, although there is a fairly large amount of spread.\n3H4. Now compare 10,000 counts of boys from 100 simulated firstborns only to the number of boys in the first births.\n\nset.seed(100)\nb1_samp &lt;- rbinom(10000, size = 100, prob = samples)\nrethinking::simplehist(b1_samp)\nabline(v = sum(birth1), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nThe model seems to overestimate the number of firstborn boys. This could potentially be because our observed count of boys is slightly higher than 50% (\\(0.56 \\%\\)) and this overestimation becomes more prominent in the smaller sample size. However, the true value is not in the tails of our distribution, so we would probably capture it in a CI.\n3H5. Our model assumes that sex of first and second births are independent. We can check this assumption by focusing on second births that followed female firstborns. Compare 10,000 simulated counts of boys to only those second births that followed girls.\n\n# Get the correct count\nn &lt;- sum(birth2[birth1 == 0])\n\n# Run the simulation\n\nb2_samp &lt;- rbinom(1e4, size = n, prob = samples)\n\n# Plot the results\nrethinking::simplehist(b2_samp, xlim = c(min(b2_samp), max(n, max(b2_samp))))\nabline(v = n, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nWow, the number of boys who follow girls is much larger than our model predicts. Either our sample is far away from the “real” value (although this is really more of a frequentist notion), or more likely, the assumption of our model is wrong.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "cp4.html#chapter-notes",
    "href": "cp4.html#chapter-notes",
    "title": "4  Geocentric Models",
    "section": "4.1 Chapter notes",
    "text": "4.1 Chapter notes\n\nNormal distributions arise from sums of random fluctuations. Lognormal distributions arise from products of random fluctuations. This property explains why normal distributions are so good at modeling real world data (ontological justification).\nNormal distributions can also be justified by the principle of maximum entropy – if all we are willing to specify about a distribution is its mean and variance, then the normal distribution contains the least amount of information (epistemological justification).\nIndex coding (as opposed to dummy coding or similar methods) makes specification of priors for categorical variables easier.\nThe prior predictive simulation, drawing samples from the distribution implied by the priors, is essential for ensuring that our priors are reasonable. Note that we should not compare the prior predictive simulation to the observed data, only to our preexisting knowledge of constraints on the model.\nMany models which are written in the “plus epsilon” form (see pg 81) (typical for linear models) can be rewritten in this more general framework, which will be easier for non-Gaussian models.\nQuadratic approximation, estimating the peak of the posterior distribution with a multivariate normal distribution, is easier than grid approximation and works well when the posterior is approximately Gaussian (many simple examples are). The peak of the quadratic approximate posterior is the maximum a posteriori estimate.\nRecall that even though grid and quadratic approximate posteriors provide an actual estimate of the posterior distribution, we can (and should) still sample from the posterior. This mimics the process for inference on more complicated models that must be fit with MCMC algorithms.\nA linear model fits the mean, \\(\\mu\\), of an outcome as a linear function of the predictor variable(s) and some parameters. These models are often geocentric – they provide good answers, but often say nothing about causality.\nPlotting simulations of the posterior distribution can provide a lot of information about the posterior (see pg 99), often much more than tables of calculations alone.\nThese types of Bayesian models incorporate two different types of uncertainty – uncertainty in parameter values, which is based on the plausibility of parameter values after seeing the data, and uncertainty from sampling processes.\nWe can extend linear models to fit curved patterns in datas in several ways, but two of the easiest are polynomials and b-splines. Priors can be difficult to fit to both, as these models are also geocentric. They can accurately fit curves, but do not describe the mechanism or process that generates curved data in the first place. See pg. 119 for an example of fitting a b-spline model using rethinking. One further extension is the generalized additive model (GAM) which incorporates smoothing over continuous predictor variables.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "cp4.html#exercises",
    "href": "cp4.html#exercises",
    "title": "4  Geocentric Models",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nThe first few questions are about the following model. \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n4.2.1 4E1\nIn the model shown, the line \\(y_i \\sim \\mathrm{Normal}(\\mu, \\sigma)\\) is the likelihood.\n\n\n4.2.2 4E2\nThe model contains two parameters (\\(\\mu\\) and \\(\\sigma\\)).\n\n\n4.2.3 4E3\nTo use Bayes’ theorem to calculate the posterior, we would write \\[\n\\frac{\\prod_{i} \\mathrm{Normal}(y_i \\mid \\mu, \\sigma) \\times \\mathrm{Normal}(\\mu \\mid 0, 10) \\times \\mathrm{Exponential}(\\sigma \\mid 1)}{\\int\\int \\prod_{i} \\mathrm{Normal}(y_i \\mid \\mu, \\sigma) \\times \\mathrm{Normal}(\\mu \\mid 0, 10) \\times \\mathrm{Exponential}(\\sigma \\mid 1) \\ \\mathrm{d}\\mu \\ \\mathrm{d}\\sigma}.\n\\]\n\n\n4.2.4 4E4\nIn the model shown below, the linear model is the line \\(\\mu_i = \\alpha + \\beta x_i\\). \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 1) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(2)\n\\end{align*}\\]\n\n\n4.2.5 4E5\nThere are three parameters in the posterior distribution of the model shown (\\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\)) – \\(\\mu\\) is no longer a parameter of the model since it is calculated deterministically.\n\n\n4.2.6 4M1\nFor the model definition below, simulate observed \\(y\\) values from the prior. \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n# Do the simulation\nmu &lt;- rnorm(1000, mean = 0, sd = 10)\nsigma &lt;- rexp(1000, rate = 1)\ny &lt;- rnorm(1000, mu, sigma)\n\n# Plot the results\nlayout(matrix(c(1, 2, 3), ncol = 3))\nhist(y, breaks = \"FD\", main = \"y\")\nhist(mu, breaks = \"FD\", main = \"mu\")\nhist(sigma, breaks = \"FD\", main = \"sigma\")\n\n\n\n\n\n\n\n\n\n\n4.2.7 4M2\nTranslate the model into a quap formula.\n\ny ~ dnorm(mu, sigma),\nmu ~ dnorm(0, 10),\nsigma ~ dexp(1)\n\n\n\n4.2.8 4M3\nTranslate the quap model formula below into a mathematical model definition.\n\ny ~ dnorm(mu, sigma),\nmu &lt;- a + b * x,\na ~ dnorm(0, 10),\nb ~ dunif(0, 1),\nsigma ~ dexp(1)\n\n\\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= a + b * x_i \\\\\na &\\sim \\mathrm{Normal}(0, 10) \\\\\nb &\\sim \\mathrm{Uniform}(0, 1) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n\n4.2.9 4M4\nA sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model, defending any priors you choose.\n\\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta * (\\mathrm{year}_i - \\min \\mathrm{year}_i) \\\\\n\\alpha &\\sim \\mathrm{Normal}(120, 30) \\\\\n\\beta &\\sim \\mathrm{Log-Normal}(0, 1.5) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(0, 0.2)\n\\end{align*}\\]\nTo create the priors, I assumed that height was measured in centimeters. If it is not, transforming either the data or the prior coefficients is a simple linear transformation. The prior for the intercept is centered at a relatively small height (around four feet) with a large spread to allow for differences in biological sex or age distributions in the population, since these were not specified in the question. I subtracted the minimum year in the model so that the years would be scaled as 0, 1, 2, 3, instead of the actual numeric value of the year.\nIn general, we know that height increases over time, so I used a lognormal prior for the slope to enforce a positivity constraint. The prior has a location parameter of 0, allowing for the chance of no growth over the three years, but a wide spread was chosen by experimenting until the prior predictive simulation represented a wide range of possible trajectories with very few trajectories appearing to be biologically unreasonable. Again, the value was chosen by experimenting with prior predictive simulations until the result appeared to capture a large variety of biologically meaningful trajectories.\nI assigned an exponential prior to sigma to reflect the fact that all variances are positive, and most tend to be small-ish, but can be large. I had a difficult time with this one in particular because this model structure seems to imply that people can shrink in-between years. This would be possible with measurement error, but I think it would be quite difficult to have measurement error this severe in something like height, which is easy to measure. However, in order to include a constraint that height can only increase, I think we would need to change the likelihood in the model, which we haven’t discussed yet in the book, so I didn’t want to worry about that. (For example, we could make height lognormally distributed, so random fluctuations would only increase height, as opposed to the current model where random fluctuations can decrease height.)\n\nset.seed(100)\n\n# Prior predictive simulation\na &lt;- rnorm(1000, 120, 30)\nb &lt;- rlnorm(1000, 0, 1.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma &lt;- rexp(1000, 1 / 5)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\n\n\n\n\n\n\n4.2.10 4M5\nIf I were reminded that every student got taller each year, this would not really change my choice of priors, but it does make me consider the same issues. I already accounted for this in the prior for \\(\\beta\\). However, it does make me think more about the likelihood I used – I really dislike that this likelihood allows for fluctuations that show up like this. Height is not measured perfectly, but large variations are uncommon. So perhaps it makes more sense to have quite a small variance parameter (\\(\\sigma\\)). Or perhaps we could structure the model so that each student has a common variance parameter that does not change every year. We could also consider making the effect of \\(\\beta\\) stronger, so that there is an assumed growth effect and not growing each year is more rare. So perhaps we could consider the following adjusted priors.\n\nset.seed(100)\n\n# Prior predictive simulation\na &lt;- rnorm(1000, 120, 30)\nb &lt;- rlnorm(1000, 2, 0.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma &lt;- rexp(1000, 1)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\n\n\n\n\nThese priors still represent a wide range of biologically accurate values, but there is much less internal (within-subject) fluctuation in height between years, and on average, the slope is steeper.\n\n\n4.2.11 4M6\nIf I had the information that variance among heights for students of the same age is never more than 64cm, this would change my choice of variance prior (assuming that this is a priori information and not measured from our sample). If this were measured from the sample, I would not change my priors. But we could consider changing the prior for \\(\\sigma\\) like so. Using the empirical rule for normal distributions, I reasoned that 21 is a reasonable constraint for an upper bound on a Uniform prior for sigma – the resultant likelihood will have approximately 99% of observations within \\(3 \\times 21 = 63\\) cm of the mean.\n\nset.seed(100)\n\n# Prior predictive simulation\na &lt;- rnorm(1000, 120, 30)\nb &lt;- rlnorm(1000, 2, 0.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma &lt;- runif(1000, 0, 21)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\n\n\n\n\nThis variance is quite large, which I think intensifies the problem I was previously discussing. Since we consider each annual measurement to be independent, without any “clustering” (we haven’t used that word yet so I don’t want to use it wrong) by inviduals, following this model to the letter allows for wide fluctuations within the predicted trajectory – to me it just doesn’t make sense for the simulated trajectory to be lower in year \\(N + 1\\) than in year \\(N\\).\n\n\n4.2.12 4M7\nRefit model m4.3 but omit the mean weight xbar. Compare the new model’s posterior to that or the original model, then compare the posterior predictions.\n\nset.seed(100)\nlibrary(rethinking)\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nLoading required package: ggplot2\n\n\nrstan (Version 2.21.5, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Zane/Documents/.cmdstan/cmdstan-2.30.1\n\n\n- CmdStan version: 2.30.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.21)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(Howell1)\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18, ]\nm_4m7 &lt;-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu &lt;- a + b * weight,\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = d2\n    )\n\nFirst we’ll look at the posterior summary.\n\nprecis(m_4m7)\n\n             mean         sd        5.5%       94.5%\na     114.5302928 1.89715027 111.4982802 117.5623053\nb       0.8908259 0.04174484   0.8241096   0.9575422\nsigma   5.0711281 0.19109884   4.7657152   5.3765409\n\n\nThe intercept estimate is quite different – the previous model reported the following statistics for a: mean 154.6, sd 0.27, 5.5% 154.17, 94.5% 155.05. So our parameter is much smaller with a larger variance. However, the estimates for b and sigma are almost exactly the same as the estimates given for the previous model (see book pg 99). In particular, we should look at the covariance matrix according to the question.\n\nvcov(m_4m7) |&gt; round(digits = 3)\n\n           a      b sigma\na      3.599 -0.078 0.009\nb     -0.078  0.002 0.000\nsigma  0.009  0.000 0.037\n\n\nThe variance for a is much higher, while the variance for b and sigma is exactly the same as the book reports. However, there is now some covariance between a and b, and between a and sigma (but not between b and sigma). Next I’ll plot a sample of prior predictions. Since the uncertainty is so narrow, I decided to only plot 100 posterior samples. The red line shows the maximum a posteriori estimate.\n\npost &lt;- extract.samples(m_4m7, n = 100)\nlayout(1)\nplot(\n    x = d2$weight, y = d2$height,\n    xlim = range(d2$weight),\n    ylim = range(d2$height),\n    col = rangi2,\n    xlab = \"weight\", ylab = \"height\"\n)\nmtext(\"Sampled posterior predictions\")\n\n# Plot the lines\nfor (i in 1:length(post$a)) {\n    curve(post$a[i] + post$b[i] * x,\n                col = col.alpha(\"black\", 0.1),\n                add = TRUE)\n}\n\nabline(\n    a = mean(post$a),\n    b = mean(post$b),\n    col = \"red\",\n    lty = 2\n)\n\n\n\n\n\n\n\n\nThe posterior predictions look about the same to me. So I guess that even when we use different parametrizations (thus changing the interpretation and scale of our alpha parameter), the predictions still work out to be about the same. I think this is foreshadowing MCMC convergence diagnostics with different parametrizations in the future.\n\n\n4.2.13 4M8\nRefit the cherry blossom spline and experiment with changing the number of knots and the width of the prior on the weights. What do you think the combination of these controls?\nFirst we’ll just refit the example.\n\n# Data import\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nd2 &lt;- d[complete.cases(d$doy), ]\n\nset.seed(100)\n# Create the splits\nnk &lt;- 15\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm1 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m1)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"Original model\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nNow let’s fit an example with more knots, say 30. This is a dramatic increase but I really want to see the effect.\n\nset.seed(100)\n# Create the splits\nnk &lt;- 30\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm2 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m2)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"More knots\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nNow we’ll also increase the width of the prior for \\(w\\). Again, I’ll increase this dramatically to make the effect easier to see.\n\nset.seed(100)\n# Create the splits\nnk &lt;- 20\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm3 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 70),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\nCaution, model may not have converged.\n\n\nCode 1: Maximum iterations reached.\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m3)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"More knots and wider prior\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nHmm, it’s hard to see a difference. Let’s try plotting the three lines on top of each other.\n\nlayout(1)\nplot(\n    x = d2$year, y = d2$doy,\n    xlab = \"year\", ylab = \"Day in year\",\n    xlim = range(d2$year), ylim = range(d2$doy),\n    col = col.alpha(rangi2, 0.3), pch = 16\n)\nlines(d2$year, y = apply(link(m1), 2, mean), col = \"black\")\nlines(d2$year, y = apply(link(m2), 2, mean), col = \"blue\")\nlines(d2$year, y = apply(link(m3), 2, mean), col = \"red\")\nlegend(\n    x = \"top\",\n    c(\"original\", \"more knots\", \"more knots and wider prior\"),\n    col = c(\"black\", \"blue\", \"red\"),\n    lty = c(1, 1, 1)\n)\n\n\n\n\n\n\n\n\nYep, from this plot the difference is pretty easy to see. The number of knots and the prior for the weights controls the “wiggliness” (the smoothness or penalty) of the splines. More knots or a wider prior allows for more local variation in the spline curve, whereas constraining the number of knots (or shrinking the weights toward 0) constrains the spline, forcing it to vary less. I guess that the prior on the weights here is equivalent to increasing the penalty term of some other kind of spline, and a higher number of knots allows for a higher degree of interpolation as more inflection points are included.\n\n\n4.2.14 4H1\nWe want to estimate the expected heights of the 5 individuals from the !Kung census with weights given in the text. Since we already saw that the predictions from our fitted model earlier were the same as the predictions in the book, I’ll use that model to do this. All we need to do is create a data frame with the heights of these individuals and pass it to link with the model.\n\n# Create the data for the new individuals\nnew_indiv &lt;-\n    tibble::tribble(\n        ~individual, ~weight,\n        1, 46.95,\n        2, 43.72,\n        3, 64.78,\n        4, 32.59,\n        5, 54.63\n    )\n\n# Get the samples from the posterior at the given weights\npreds &lt;- rethinking::link(fit = m_4m7, data = new_indiv)\n\n# Get the means and round it\nnew_indiv$means &lt;- apply(preds, 2, function(x) round(mean(x), 2))\n\n# Calculate the PI and format it\nPIs &lt;- apply(preds, 2, PI, 0.89)\nnew_indiv$PIs &lt;- apply(PIs, 2, function(x) paste0(round(x, 2), collapse = \", \"))\n\n# Format names and create the table\nnames(new_indiv) &lt;- c(\"Individial\", \"weight\", \"expected height\", \"89% interval\")\nknitr::kable(\n    new_indiv,\n    caption = paste0(\"Expected heights with 89% equal-tailed posterior intervals\",\n                                     \" for the five individuals wiuthout recorded heights in the\",\n                                     \" !Kung census data, using link.\")\n)\n\n\nExpected heights with 89% equal-tailed posterior intervals for the five individuals wiuthout recorded heights in the !Kung census data, using link.\n\n\nIndividial\nweight\nexpected height\n89% interval\n\n\n\n\n1\n46.95\n156.35\n155.88, 156.77\n\n\n2\n43.72\n153.47\n153.04, 153.92\n\n\n3\n64.78\n172.21\n170.85, 173.53\n\n\n4\n32.59\n143.57\n142.62, 144.51\n\n\n5\n54.63\n163.18\n162.4, 163.89\n\n\n\n\n\nAfter talking to my colleague Juliana, I also decided to rerun this using sim() – with predicted heights that include variance from the mean. I think this is probably “more correct” so credit to Juliana for pointing that out to me.\n\nnew_indiv &lt;-\n    tibble::tribble(\n        ~individual, ~weight,\n        1, 46.95,\n        2, 43.72,\n        3, 64.78,\n        4, 32.59,\n        5, 54.63\n    )\n# Get the samples from the posterior at the given weights\npreds &lt;- rethinking::sim(fit = m_4m7, data = new_indiv)\n\n# Get the means and round it\nnew_indiv$means &lt;- apply(preds, 2, function(x) round(mean(x), 2))\n\n# Calculate the PI and format it\nPIs &lt;- apply(preds, 2, PI, 0.89)\nnew_indiv$PIs &lt;- apply(PIs, 2, function(x) paste0(round(x, 2), collapse = \", \"))\n\n# Format names and create the table\nnames(new_indiv) &lt;- c(\"Individial\", \"weight\", \"expected height\", \"89% interval\")\nknitr::kable(\n    new_indiv,\n    caption = paste0(\"Expected heights with 89% equal-tailed posterior intervals\",\n                                     \" for the five individuals wiuthout recorded heights in the\",\n                                     \" !Kung census data, using sim.\")\n)\n\n\nExpected heights with 89% equal-tailed posterior intervals for the five individuals wiuthout recorded heights in the !Kung census data, using sim.\n\n\nIndividial\nweight\nexpected height\n89% interval\n\n\n\n\n1\n46.95\n156.36\n148.29, 164.14\n\n\n2\n43.72\n153.79\n145.86, 161.95\n\n\n3\n64.78\n172.15\n164.02, 181.17\n\n\n4\n32.59\n143.91\n135.7, 152.15\n\n\n5\n54.63\n163.11\n154.61, 171.11\n\n\n\n\n\nSo we can see that the results are not that different, but the 89% intervals are a bit wider, which is good. The new intervals incorporate an additional component of the variance that my previous estimates did not, so if I were publishing this model, I would definitely publish the sim intervals instead. I guess these are analogous to frequentist prediction intervals as opposed to confidence intervals.\n\n\n4.2.15 4H2\nSelect out all the rows in the Howell1 data with ages below 18 years of age. Fit a linear regression to these data using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?\nOk, first we’ll subset the data and make a quick plot.\n\nd &lt;- Howell1[Howell1$age &lt; 18, ]\nplot(d$weight, d$height, xlab = \"weight\", ylab = \"height\")\n\n\n\n\n\n\n\nd_xbar &lt;- mean(d$weight)\n\nNow let’s fit the model. I’ll use the same priors as before, since I already looked at the data. We probably want to actually change the prior on the intercept since children have a lower mean height than adults, but I think it will work out alright.\n\nm4h2 &lt;-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu &lt;- a + b * (weight - d_xbar),\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = d\n    )\nprecis(m4h2)\n\n            mean         sd       5.5%      94.5%\na     108.383621 0.60867543 107.410840 109.356402\nb       2.716654 0.06831783   2.607469   2.825839\nsigma   8.437466 0.43060814   7.749271   9.125661\n\n\nWe can see that the estimate slope is \\(\\beta = 2.69\\). So for every 10 units of increase in weight, our model predicts that a child’s height will increase by \\(26.9\\) units.\nNext, we’ll plot the MAP prediction line with an 89% interval, along with the 89% prediction interval for heights.\n\nplot(d$weight, d$height, xlab = \"weight\", ylab = \"height\")\n\n# Weight values to predict over\nweight_seq &lt;- seq(from = min(d$weight), to = max(d$weight), by = 0.5)\n\n# Get the posterior predictions\nset.seed(100)\nlink_4h2 &lt;- link(m4h2, data = data.frame(weight = weight_seq))\nsim_4h2  &lt;-  sim(m4h2, data = data.frame(weight = weight_seq))\n\n# Summarize the samples\nmu_mean &lt;- apply(link_4h2, 2, mean)\nmu_CI &lt;- apply(link_4h2, 2, PI, 0.89)\nmu_PI &lt;- apply(sim_4h2, 2, PI, 0.89)\n\n# Plot the results -- we want to go backwards, starting with the wider\n# interval first\nshade(mu_PI, weight_seq, col = col.alpha(\"black\", 0.5))\nshade(mu_CI, weight_seq, col = col.alpha(\"lightgray\", 0.75))\nlines(weight_seq, y = mu_mean, col = \"red\", lty = 2)\n\n\n\n\nThe observed height and weight of the children in the !Kung census data (circular points) with the MAP linear regression line (red dashed line), 89% equal-tailed posterior interval of this line (light gray shading), and 89% equal-tailed posterior interval for predicted heights ( dark gray shading).\n\n\n\n\nThe main issue with the model is that there is clearly some curvature to the trend in the data. There seems to be an inflection point around a weight of 30 where the trend is no longer linear, which will make the entire model fit worse (particularly because this region has high leverage). There is also some curvature in the lower weight values as well. Perhaps it would be better to fit this model with a spline or with a dummy variable for “age categories” that interacts with the slope term, allowing the model to “bend” at particular points – the same thing could be accomplished using splines with degree 1. Normally I hate discretization, but in this case these trends are likely to map to our approximate knowledge of human growth. Babies grow faster than toddlers, and teenagers grow faster than younger children. So perhaps including more information in our model could help solve this problem as well. But if we don’t want to include age, a spline could probably work better.\n\n\n4.2.16 4H3\nBased on our colleague’s suggestion, we want to use the natural logarithm of weight to model height. We’ll do that using the entire Howell1 dataset. I’ll use the same priors as before.\n\nm4h3 &lt;-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu &lt;- a + b * log(weight),\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = Howell1\n    )\nprecis(m4h3)\n\n            mean        sd       5.5%      94.5%\na     -22.874317 1.3342911 -25.006772 -20.741862\nb      46.817789 0.3823240  46.206761  47.428816\nsigma   5.137088 0.1558847   4.887954   5.386222\n\n\nThese estimates indicate that an individual from the census with mean weight is expected to have a height of \\(138.27\\) units. The slope (b) indicates that for every 1 unit increase in log-weight, height is expected to increase by about 47 units. I think that log-weight units are a bit difficult to think about, this would probably be easier to understand if we’d used log2 or something without an \\(e\\) in it. This coefficient means that if we multiply an individual’s weight by \\(e\\) (approximately 2.72), we would expect to see a 47 unit increase in their height. So, e.g., if we used log2, we could interpret this as doubling their weight, but we can’t do that now.\n\n# Plot the raw data\nplot(height ~ weight, data = Howell1, col = col.alpha(rangi2, 0.5))\n\n# Get the mean and height samples -- I couldn't get sim to work so I did link\n# and then did the sim part myself\nweight_seq &lt;- seq(from = min(Howell1$weight),\n                                    to = max(Howell1$weight),\n                                    by = 1)\nset.seed(100)\nlink_4h3 &lt;- link(m4h3, data = data.frame(weight = weight_seq))\nsim_4h3  &lt;-  sim(m4h3, data = data.frame(weight = weight_seq))\n\n# Summarize the samples\nmu    &lt;- apply(link_4h3, 2, mean)\nmu_CI &lt;- apply(link_4h3, 2, PI, 0.97)\nmu_PI &lt;- apply( sim_4h3, 2, PI, 0.97)\n\n# Add the predictions to the plot\nlines(weight_seq, y = mu_PI[1, ], lty = 2)\nlines(weight_seq, y = mu_PI[2, ], lty = 2)\nshade(mu_CI, weight_seq, col = col.alpha(\"darkgray\", 0.75))\nlines(weight_seq, y = mu, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nWell, it does look like our colleague’s suggestion produced a better model!\n\n\n4.2.17 4H4\nPlot the prior predictive distribution for the parabolic polynomial regression model in the chapter. We want to modify the prior distributions of the parameters so that the prior predictions lie within the reasonable outcome space we defined.\n\n# Data setup since I keep overwriting names\nd &lt;- Howell1\nd2 &lt;- d[d$age &gt;= 18, ]\n\n# Standardize weight\nd2$weight_s &lt;- scale(d2$weight)\nd2$weight_s2 &lt;- d2$weight_s ^ 2\n\nxbar &lt;- mean(d2$weight)\nsx &lt;- sd(d2$weight)\n\n# Setup\nlayout(matrix(c(1, 2), ncol = 2))\nset.seed(100)\nN &lt;- 100\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 178, 20)\nb1 &lt;- rlnorm(N, 0, 1)\nb2 &lt;- rnorm(N, 0, 1)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"Original priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * ((x - xbar) / sx) + b2[i] * (((x - xbar) / sx) ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 178, 20)\nb1 &lt;- rnorm(N, 0, 4)\nb2 &lt;- rnorm(N, 0, 4)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"New priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * ((x - xbar) / sx) + b2[i] * (((x - xbar) / sx) ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n\n\n\n\n\n\n\nWell, when I looked at the original priors they were all already within the reasonable model space. But since we have a parabola now, we don’t need to constrain the linear part to be positive – we could get an always positive derivative even with a negative linear term (\\(2ax + b &gt; 0\\) for \\(ax^2 + bx + c\\), so e.g. if \\(a = 2, \\ b = -2\\) the derivative will be positive for all \\(x &gt; 1\\), and all of our values are larger than 30). So I changed the priors to get trajectories that cover a bit more of the reasonable space. They approach the edges in some cases, but that’s okay. I wasn’t really sure what else to do with this question once I saw that the parabolas were all already in the reasonable space.\nMaybe I was supposed to NOT standardize the data, even though the chapter did this and the book didn’t explicitly say what model to use. However, assigning a biologically meaningful prior without even centering the data is quite difficult, because in this example it makes sense to constrain the (non-centered) intercept at 0 (i.e. a person with weight zero has height zero).\nHowever, we can quickly see how this model doesn’t make biological sense by thinking about where we should center the data. If we center at the mean, our parabola has to be either concave up or concave down (recall the second derivative of a parabola is constant), which doesn’t make any sense. Why would low weight people be taller, or high weight people be shorter? By centering the data at the mean we force the parabola to take one of these two options. So perhaps standardizing by some reference point would be good, but remember that we don’t want to look at the data to choose where to standardize. So we’re forced to make one of two biologically inaccurate assumptions: either the concavity must be unreasonable or the intercept must be unreasonable. So let’s set an unreasonable intercept and choose not to center the data. That way we can at least constrain the concavity to be positive.\n\n# Setup\nset.seed(100)\nlayout(matrix(c(1, 2), ncol = 2))\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 178, 20)\nb1 &lt;- rlnorm(N, 0, 1)\nb2 &lt;- rnorm(N, 0, 1)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"Original priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * x + b2[i] * (x ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n# Repeat the original priors listed\na &lt;- rnorm(N, 90, 20)\nb1 &lt;- rnorm(N, 0, 0.5)\nb2 &lt;- rnorm(N, 0.02, 0.005)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"New priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * x + b2[i] * (x ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n\n\n\n\n\n\n\nThere. Now our priors for the non-centered data fit inside the reasonable range. Are you happy now? They don’t really make any sense (why would a person with 0 weight have 90 height? why do we have to restrain the coefficient for the b terms so much?) but they fit inside the line. I think the better answer for this question is to either standardize the data or avoid using a quadratic model for this problem, which was maybe the entire point of the question.\n\n\n4.2.18 4H5\nReturn to the cherry blossoms data and model the association between blossom date (doy) and March temperature (temp). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. How well does temperature trend predict the blossom trend?\nI’m leaning towards splines for this question – I expect this relationship to be nonlinear, and given my frustration with polynomial models in the previous question, I think it’s okay to stick with a spline. So first we need to set up the splines. We haven’t talked about missing data at all yet in this book so I’ll just use the complete cases.\n\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms[ , c(\"temp\", \"doy\")]\nd2 &lt;- d[complete.cases(d), ]\n\nLet’s do a prior predictive simulation. I’ll start with the priors in the book for the previous model, but then adjust if they look unreasonable. I looked at the paper cited in the cherry blossoms data description, and it appears that the temperatures were measured in Kyoto. So I googled “Kyoto march temperature” and I guess we can assume that temperatures should be between 3 and 14 degrees celsius. McElreath says that the flowers can bloom any time from March to May, so as long as our predictions are between day 60 (first day of March in regular year) and day 152 (first day of May in leap year), I’ll consider that to be “reasonable”.\n\nN &lt;- 1000\nset.seed(100)\n\n# Create a range of temperature for the PPS splines\ntemp_range &lt;- seq(2, 14, 0.05)\nnk &lt;- 15\n\n# Create the splines\nB &lt;-\n    splines::bs(\n        x = temp_range,\n        knots = quantile(temp_range, probs = seq(0, 1, length.out = nk))[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\n# Simulate a since it is easy\na &lt;- rnorm(N, 100, 10)\n\n# Simulate the weights with a little trickery\nw &lt;- do.call(rbind, purrr::map(1:ncol(B), ~rnorm(N, 0, 15)))\n\n# Calculate the predicted means\nmu &lt;- a + B %*% w\n\n# Plot the pps\nlayout(1)\nplot(\n    NULL,\n    xlim = c(2, 14), ylim = c(40, 170),\n    xlab = \"temperature (Celsius)\", ylab = \"day of year\",\n    xaxs=\"i\", yaxs=\"i\"\n)\nabline(h =  60, lty = 2, lwd = 0.5)\nabline(h = 152, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    lines(x = temp_range, y = mu[, i], col = col.alpha(\"black\", 0.05))\n}\n\n\n\n\n\n\n\n\nYeah, the default priors look pretty good to me. The only change I made was to increase the width of the prior for \\(w\\), to potentially allow for more wiggliness. There is a bit of wiggling out of the expected day of year range, but it’s so rare that I think it will be fine, especially after the model sees the data. I think it will be harder to set the correct prior on \\(w\\) with a prior predictive simulation, so I think we better go ahead and fit the model.\nNow let’s fit a quap model. Note that I had to turn up the maximum number of iterations run by the underlying optim call to ensure convergence criteria were met.\n\n# Create the real splines\nB_real &lt;-\n    splines::bs(\n        x = d2$temp,\n        knots = quantile(temp_range, probs = seq(0, 1, length.out = nk))[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\n# Fit the model\nm4h6 &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 15),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B_real),\n        start = list(w = rep(0, ncol(B_real))),\n        control = list(maxit = 200)\n    )\n\nrethinking::precis(m4h6, depth = 2)\n\n             mean         sd       5.5%      94.5%\nw[1]    0.0000000 14.9999514 -23.972819  23.972819\nw[2]    0.0000000 14.9999514 -23.972819  23.972819\nw[3]    0.0000000 14.9999514 -23.972819  23.972819\nw[4]    6.9549283  5.2048778  -1.363472  15.273328\nw[5]    4.7825332  4.8284596  -2.934278  12.499344\nw[6]    5.3724394  4.7117796  -2.157895  12.902773\nw[7]    1.1616676  4.6557218  -6.279075   8.602410\nw[8]   -0.1986208  4.8130958  -7.890877   7.493636\nw[9]   -3.4414421  5.1367824 -11.651012   4.768128\nw[10]  -2.3424900  5.7891404 -11.594655   6.909674\nw[11]  -6.1560988  7.2908763 -17.808327   5.496130\nw[12]  -0.4530242  6.7393765 -11.223849  10.317801\nw[13]   0.0000000 14.9999514 -23.972819  23.972819\nw[14]   0.0000000 14.9999514 -23.972819  23.972819\nw[15]   0.0000000 14.9999514 -23.972819  23.972819\nw[16]   0.0000000 14.9999514 -23.972819  23.972819\nw[17]   0.0000000 14.9999514 -23.972819  23.972819\na     102.5223225  4.5314729  95.280154 109.764491\nsigma   5.8829498  0.1474653   5.647272   6.118628\n\n\nAlright, now we can get the actual predictions and plot them.\n\nplot(d2$doy ~ d2$temp, xlab = \"Temperature (Celsius)\", ylab = \"Day of year\",\n         col = col.alpha(rangi2, 0.4), pch = 16)\n\n# Get the predictions and interval -- have to use the basis over the\n# interpolated temperature data!\nlink_m4h6 &lt;- link(m4h6, data = list(B = B))\nmu &lt;- apply(link_m4h6, 2, mean)\nmu_pi1 &lt;- apply(link_m4h6, 2, PI, 0.97)\nmu_pi2 &lt;- apply(link_m4h6, 2, PI, 0.89)\nmu_pi3 &lt;- apply(link_m4h6, 2, PI, 0.61)\n\n# lines(temp_range, mu_pi3[1, ], lty = 2)\n# lines(temp_range, mu_pi3[2, ], lty = 2)\n# lines(temp_range, mu_pi2[1, ], lty = 2)\n# lines(temp_range, mu_pi2[2, ], lty = 2)\n# lines(temp_range, mu_pi1[1, ], lty = 2)\n# lines(temp_range, mu_pi1[2, ], lty = 2)\nshade(mu_pi1, temp_range, col = col.alpha(\"black\", 0.5))\nshade(mu_pi2, temp_range, col = col.alpha(\"darkgray\", 0.5))\nshade(mu_pi3, temp_range, col = col.alpha(\"gray\", 0.5))\nlines(temp_range, mu, lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\nOK, so I had some confusion here with plotting the predictions. I forgot that I need to specify a separate \\(B\\) matrix of basis functions for the model fitting and for this part of the predictions. But now it is working and we can see that there is definitely a decent association between warmer temperatures and earlier blooming. However, we would need more high-temperature data to confirm this trend (and high-temperature data is something we wish we didn’t have more of, really).\n\n\n4.2.19 4H6\nQ. Simulate the prior predictive distirbution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weights is doing?\nOk, first we need to simulate the PPD. I’m pretty sure I already have a handle on this from the previous two spline exercises, but it should be fun. (In fact, we really already did this exact exercise, but whatever I guess.)\nLet’s simulate with maybe six different widths: 1, 5, 10 (chosen in the textbook), 15, 20, and 50. I’ll get the predictions from each of these simulations and then plot them.\n\n# Get ready for simulation.\n# Set parameters for sim\nwidths &lt;- c(1, 5, 10, 15, 20, 50)\nN &lt;- 250\nset.seed(100)\nlayout(matrix(c(1, 2, 3, 4, 5, 6), ncol = 3, nrow = 2))\n\n# Create the splits\nnk &lt;- 15\nyear_range &lt;- seq(800, 2010, 1)\nknot_list &lt;- quantile(year_range, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = year_range,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\nsim_res &lt;- vector(mode = \"list\", length = length(widths))\n\ndo_pps &lt;- function(width) {\n    # Simulate a since it is easy\n    a &lt;- rnorm(N, 100, 10)\n    \n    # Simulate the weights with a little trickery\n    w &lt;- do.call(rbind, purrr::map(1:ncol(B), ~rnorm(N, 0, width)))\n    \n    # Calculate the predicted means\n    mu &lt;- a + B %*% w\n    \n    plot(\n        NULL,\n        xlim = c(800, 2010), ylim = c(40, 170),\n        xlab = \"year\", ylab = \"day of year\",\n        xaxs=\"i\", yaxs=\"i\",\n        main = paste(\"Width:\", width)\n    )\n    \n    for (i in 1:N) {\n        lines(x = year_range, y = mu[, i], col = col.alpha(\"black\", 0.01))\n    }\n        \n    abline(h =  60, lty = 2, lwd = 0.5)\n    abline(h = 152, lty = 2, lwd = 0.5)\n    \n    return(NULL)\n}\n\nout &lt;- sapply(widths, do_pps)\n\n\n\n\n\n\n\n\nYep, just like I said in the other two exercises about this, the width of the spline prior determines the amount of smoothness that the spline should have. If we increase the width, the spline is encouraged to vary more, and will “wiggle” (spike up and down) with larger fluctuations. If we constrain this parameter, the spline is encouraged to vary less, staying closer to the intercept.\n\n\n4.2.20 4H8\nQ. The cherry blossom split in the chapter used an intercept \\(\\alpha\\), but technically it doesn’t require one. The first basis functions could substitute for the intercept. Try refitting the cherry blossom spline without the intercept. What else about the model do you need to change to make this work.\nThis question is worded a bit confusingly to me. I assume he means the intercept parameter in the model, because the basis splines have a separate intercept that we also set to be true. Either way, let’s go ahead and try refitting a model without the intercept.\n\n# Data import\ndata(\"cherry_blossoms\")\nd &lt;- cherry_blossoms\nd2 &lt;- d[complete.cases(d$doy), ]\n\nset.seed(100)\n# Create the splits\nnk &lt;- 15\nknot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB &lt;- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- B %*% w,\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\n\nlayout(1)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nWell, I’ll be honest. I’m not really sure what else we’re supposed to change here because that model looks perfectly fine to me. It has some curvature at the lowest year values though, which we maybe should have expected since we basically set the intercept to zero. I think we could improve this by adjusting the prior for \\(w\\) to have a mean that was the same mean as the previous intercept prior (that is, 100). Maybe that’s the other thing we need to change. Let’s try it.\n\n# Save the previous results or whatever\nold_mu &lt;- mu\nold_mu_PI &lt;- mu_PI\n\n# Fit the model\nm &lt;-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- B %*% w,\n            w ~ dnorm(100, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Model with intercept\nma &lt;-\n        quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\na_mu &lt;- link(m)\na_mu_PI &lt;- apply(mu, 2, PI, 0.97)\n\n\n# Get the posterior predictions and plot them\nmu &lt;- link(m)\nmu_PI &lt;- apply(mu, 2, PI, 0.97)\n\n# Plot the results with both lines\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\")\nlines(d2$year, y = apply(a_mu, 2, mean), col = \"green\", lty = 2)\nlines(d2$year, y = apply(old_mu, 2, mean), col = \"blue\", lty = 2)\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\nlegend(\n    x = \"top\",\n    legend = c(\"intercept\", \"no intercept mean 0\", \"no intercept mean 100\"),\n    lty = c(2, 2, 2),\n    col = c(\"green\", \"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\nYes indeed, if you look closely, you can see that the red line and the green line are almost exactly the same. Which means that if we adjust the mean of the spline prior, we can fit the exact same model without an intercept. Yay.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "cp5.html#chapter-notes",
    "href": "cp5.html#chapter-notes",
    "title": "5  The Many Variables and the Spurious Waffles",
    "section": "5.1 Chapter notes",
    "text": "5.1 Chapter notes\n\nThis chapter is mainly concerned with the issue of confounding, although it doesn’t used the same technical terms that I learned in my epidemiology classes.\nSpecifically, we are concerned with spurious associations (positive confounding) where a third variable causes the relationship between two variables to appear stronger than it is;\nAnd with masked relationships (negative confounding), where a third variable causes the relationship between two variables to appear weaker than it is.\nThis chapter also introduces Directed Acyclic Graphs (DAGs) as heuristic graphical causal models for understanding the causal relationships between variables.\nThe conditional independencies of DAGs are disccused, which are statements of which variables should be associated with each other in the data or not, given that the DAG is an accurate causal model. DAGs with the same variables and implied conditional independencies are called Markov Equivalent.\nThis chapter also discusses three ways to visualize the results of a regression model: predictor residual plots, posterior prediction plots, and counterfactual plots. In this case, the counterfactual plot does not necessarily mean what I am used to it meaning, it just means we are predicting values which may not have been observed using the model (so in some sense they are counterfactual to our observed data).\nFinally, this chapter also discusses categorical variables and index coding, which is used by the rethinking package (and later by Stan) rather than the dummy coding used by most R models.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables and the Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "cp5.html#exercises",
    "href": "cp5.html#exercises",
    "title": "5  The Many Variables and the Spurious Waffles",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\n\n5.2.1 5E1\nThe linear models\n\\[\\mu_i = \\beta_x x_i + \\beta_z z_i\\] and \\[\\mu_i = \\alpha + \\beta_x x_i + \\beta_z z_i\\]\nare both multiple regression models. The first model \\[\\mu_i = \\alpha + \\beta x_i\\] is a simple linear regression model and while the third model \\[\\left( \\mu_i = \\alpha + \\beta (x_i - z_i) \\right)\\] involves both \\(x\\) and \\(z\\), the model only has one coefficient and treats their difference as a single explanatory variable.\n\n\n5.2.2 5E2\nWe could evaluate the claim animal diversity is linearly related to latitude, but only after controlling for plant diversity using the linear model \\[\\begin{align*}\n\\text{animal diversity}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\mathrm{latitude} \\right) + \\beta_2 \\left( \\text{plant diversity} \\right)\n\\end{align*}\\]\nwhere suitable priors are assigned and other appropriate parameters are given for the likelihood function.\n\n\n5.2.3 5E3\nWe could evaluate the claim neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree using the multiple linear regression \\[\\begin{align*}\n\\text{time to PhD}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\text{amount of funding} \\right) + \\beta_2 \\left( \\text{size of laboratory} \\right)\n\\end{align*}\\]\nwith suitable priors, etc. The slope of both \\(\\beta_j\\) should be positive. Classically, I would probably be inclined to include an interaction term in this model, but we haven’t talked about that yet in the book so I didn’t.\n\n\n5.2.4 5E4\nIf we have a single categorical variable with levels \\(A,\\) \\(B,\\) \\(C,\\) and \\(D,\\) (represented as indicator variables), the following linear models are inferentially equivalent: \\[\\begin{align*}\n\\mu_i &= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha_A A_i + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i, \\quad \\text{ and }\\\\\n\\mu_i &= \\alpha_A \\left( 1 - B_i - C_i - D_i \\right) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i.\n\\end{align*}\\]\n\n\n5.2.5 5M1\nAn example of a spurious correlation: I am happy on days when it is sunny outside, and when I get to leave work early. Both of these things individually make me happy, but the weather doesn’t determine whether I get to leave work early. (At least not fully anyways. The weather definitely determines how much work I get done, but other external factors control the amount of work I have and the deadlines I need to meet.)\n\n\n5.2.6 5M2\nAn example of a masked relationship that I like: supposed you have multiple measurements of how far the accelerator is pressed in a car and the car’s speed (taken simultaneously) at multiple time points, and you see no correlation. However, you are then given the measurements of the slope of the road at each of those time points, and you see that, when the slope of the road is taken into account, the two variables are correlated. When the car is going uphill, the accelerator is always pressed further and the speed is always lower, but pressing the accelerator still increases the speed.\n\n\n5.2.7 5M3\nI guess a higher divorce rate could cause a higher marriage rate by making more people available to be married. If divorced people tend to get remarried (potentially to non-divorced people), then the overall marriage rate could go up. Addressing this using a multiple linear regression model would be quite difficult, as you would need more data on remarriage and divorce status. It could be difficult to incorporate remarriages into the regression model, maybe an agent-based model would be more intuitive for this.\n\n\n5.2.8 5M4\nI found this list of percent LDS population by state. So if we want to add this as a predictor to the divorce rate model, first I’ll join these data to the WaffleDivorce data from the rethinking package.\n\npct_lds &lt;- readr::read_csv(here::here(\"static/pct_lds.csv\"))\n\nRows: 50 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): State\ndbl (3): mormonPop, mormonRate, Pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlibrary(rethinking)\n\nLoading required package: rstan\nLoading required package: StanHeaders\nLoading required package: ggplot2\nrstan (Version 2.21.5, GitRev: 2e1f913d3ca3)\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\nLoading required package: cmdstanr\nThis is cmdstanr version 0.5.3\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n- CmdStan path: C:/Users/Zane/Documents/.cmdstan/cmdstan-2.30.1\n- CmdStan version: 2.30.1\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\nLoading required package: parallel\nrethinking (Version 2.21)\n\nAttaching package: 'rethinking'\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(\"WaffleDivorce\")\n\ndat_5m4 &lt;-\n    dplyr::left_join(\n        WaffleDivorce,\n        pct_lds,\n        by = c(\"Location\" = \"State\")\n    )\n\ndat_5m4[\n    which(dat_5m4$Location == \"District of Columbia\"),\n    \"mormonRate\"\n] &lt;-0.0038\n\ndat_5m4_l &lt;-\n    dat_5m4 |&gt;\n    dplyr::transmute(\n        D = Divorce,\n        M = Marriage,\n        A = MedianAgeMarriage,\n        L = mormonRate * 100\n    ) |&gt;\n    as.list()\n\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2))\npurrr::walk2(dat_5m4_l, names(dat_5m4_l), ~hist(.x, main = .y, breaks = \"FD\"))\n\n\n\n\n\n\n\nlayout(1)\n\nWhen I tried to do this the first time, it turns out that the WaffleDivorce dataset has D.C. in it, but not the state of Nevada? And the table of percent LDS populations I found has Nevada, but not D.C. So I just googled it, and on the Wikipedia page I saw that the percentage was 0.38% in 2014, which is good enough for government work, so I filled it in manually. I didn’t transform any of the predictors, but standardizing and transforming them would probably be a good idea. I think a logit transformation would probably be suitable for the percent LDS population but as long as quap() converges I won’t worry about it too much.\n\\[\\begin{align*}\n\\text{Divorce rate}_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot \\text{Marriage rate}_i + \\beta_2 \\cdot \\text{Median age at marriage}_i \\\\\n&\\quad\\quad + \\beta_3 \\cdot \\text{Percent LDS}_i \\\\\n\\beta_0 &\\sim \\mathrm{Unif}(0, 1000); \\\\\n\\beta_j &\\sim \\mathrm{Normal}(0, 10); \\quad j = \\{0, \\ldots, 3\\} \\\\\n\\sigma &\\sim \\mathrm{Exp}(0.5)\n\\end{align*}\\]\nwhere \\(i\\) indexes the states. Since the outcome is in units of divorces per 1000 people, I decided to let the intercept be anything from 0 to 1000 people. It will probably be quite small but that shouldn’t be too much of a problem I think. Then, I assigned weakly uninformative priors to the slope coefficients and a simple positive prior to the standard deviation. It would be better to do a prior predictive simulation and figure out some better assumptions. Now we’ll fit th model with quap (quadratic approximation).\n\nset.seed(100)\nfit_5m4 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            # We could rewrite this so we didn't have to write out all the identical\n            # priors but this is easier and I am lazy\n            mu &lt;- b0 + bM * M + bA * A + bL * L,\n            b0 ~ dunif(0, 100),\n            bM ~ dnorm(0, 10),\n            bA ~ dnorm(0, 10),\n            bL ~ dnorm(0, 10),\n            sigma ~ dexp(0.5)\n        ),\n        data = dat_5m4_l\n    )\n\nrethinking::precis(fit_5m4)\n\n              mean         sd       5.5%       94.5%\nb0    38.634820524 6.90452240 27.6000602 49.66958086\nbM     0.003608067 0.07550132 -0.1170576  0.12427376\nbA    -1.105494100 0.22397859 -1.4634551 -0.74753306\nbL    -0.066488235 0.02396230 -0.1047846 -0.02819184\nsigma  1.331562503 0.13193156  1.1207104  1.54241461\n\n\nWe can see that for every one percentage increase in the LDS population of a state, the model predicts that the divorce rate will decrease by -0.07 units. Since the divorce rate is in percentage units, this means we would need slightly less than a 15% increase in LDS population for a state’s divorce rate to decrease by 1%.\nThis estimate is probably biased by Utah, which is a strong outlier with 63% LDS population (far more than the second highest state, Idaho, with 24% of the population identifying as LDS).\n\n\n5.2.9 5M5\nFor this exercise, I’ll call the price of gas \\(G\\), obesity \\(O\\), exercise \\(E\\), and eating at restaurants \\(R\\). The dag for this hypothesis looks like this.\n\ndag &lt;- dagitty::dagitty(\n    \"dag {\n        G -&gt; E\n        G -&gt; R\n        G -&gt; O\n        E -&gt; O\n        R -&gt; O\n    }\"\n)\n\nplot(dagitty::graphLayout(dag))\n\n\n\n\n\n\n\n\nTherefore, \\(G\\) confounds the relationship of \\(E\\) on \\(O\\) and also confounds the relationship of \\(R\\) on \\(O\\). (The direct causal effect of \\(G\\) on \\(O\\) represents the total impact of any other pathways that we have not measured.) We could capture both of these pathways simultaneously with the multiple regression \\[\\mu_i = \\alpha + \\beta_E E_i + \\beta_R R_i + \\beta_G G_i,\\] where \\(\\mu_i\\) is the conditional mean of \\(O_i\\). This model “controls for” the price of gas, and additionally controls for the effect of \\(E\\) and \\(R\\) on each other. If we are certain that \\(E\\) and \\(R\\) are related only though \\(G\\), we could fit the two regression models \\[\\mu_i = \\alpha + \\beta_E E_i + \\beta_G G_i\\] and \\[\\mu_i = \\alpha + \\beta_R R_i + \\beta_G G_i.\\] Using these two models, we control for the effect of gas price and obtain the direct causal effect of \\(E\\) and \\(R\\), assuming that \\(E\\) and \\(R\\) do not affect each other at all.\n\n\n5.2.10 5H1\nAssuming the DAG for the divorce problem is \\(M \\to A \\to D\\). The DAG only has one conditional independency: \\(D\\) and \\(M\\) are uncorrelated if we condition on \\(A\\). We can check this using dagitty as well.\n\ndag2 &lt;- dagitty::dagitty(\"dag {M -&gt; A -&gt; D}\")\ndagitty::impliedConditionalIndependencies(dag2)\n\nD _||_ M | A\n\n\nNow we can check if the data are consistent with this DAG. (We already know that it is, because this is the same conditional independency set as one of the previous example DAGs).\n\nd &lt;-\n    WaffleDivorce |&gt;\n    dplyr::select(\n        D = Divorce,\n        M = Marriage,\n        A = MedianAgeMarriage\n    ) |&gt;\n    dplyr::mutate(dplyr::across(tidyselect::everything(), standardize))\n\n# Fit the model without age so we can see the unconditional estimate\nmodel_noage &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\n# Fit the model with age only\nmodel_ageonly &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\n# Fit the model with age to see the estimate after conditioning\nmodel_5h1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M + bA * A,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\ncoeftab_plot(\n    coeftab(model_noage, model_ageonly, model_5h1),\n    par = c(\"bA\", \"bM\")\n)\n\n\n\n\n\n\n\n\nWe can see that before we add age to the model, the effect of marriage is quit large. But then when we condition on age, the effect is close to zero with a large amount of uncertainty. So it appears that our data are consistent with the conditional independencies of the model. The coefficient for age does not change when we condition it on marriage rate, so this supports our conclusion.\n\n\n5.2.11 5H2\nAssuming that this is the true DAG for the divorce example, we want to fit a new model and estimate the counterfactual effect of halving a state’s marriage rate \\(M\\).\n\nset.seed(100)\nm_5h2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            ## M -&gt; A\n            A ~ dnorm(mu_A, sigma_A),\n            mu_A &lt;- b0_A + bM * M,\n            b0_A ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma_A ~ dexp(1),\n            \n            ## A -&gt; D\n            D ~ dnorm(mu_D, sigma_D),\n            mu_D &lt;- b0_D + bA * A,\n            b0_D ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma_D ~ dexp(1)\n        ),\n        data = d\n    )\n\nrethinking::precis(m_5h2)\n\n                 mean         sd       5.5%      94.5%\nb0_A    -6.922370e-08 0.08684788 -0.1387998  0.1387996\nbM      -6.947376e-01 0.09572699 -0.8477278 -0.5417474\nsigma_A  6.817373e-01 0.06758016  0.5737312  0.7897434\nb0_D    -4.353665e-07 0.09737877 -0.1556305  0.1556296\nbA      -5.684028e-01 0.10999981 -0.7442038 -0.3926019\nsigma_D  7.883257e-01 0.07801134  0.6636485  0.9130029\n\n# Get the halved marriage rate for each state and standardize to model units\nwith(\n    WaffleDivorce,\n    M_seq &lt;&lt;- c(\n        ((Divorce / 2) - mean(Divorce)) / sd(Divorce),\n        d$D\n    )\n)\n\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\nres &lt;-\n    s |&gt;\n    # stack the matrix columns on top of each other into one vector\n    lapply(as.vector) |&gt;\n    tibble::as_tibble() |&gt;\n    dplyr::mutate(\n        M = rep(M_seq, each = nrow(s$A))\n    )\n    \nres_diff &lt;-\n    tibble::tibble(\n        A = s$A[, 2] - s$A[, 1],\n        D = s$D[, 2] - s$D[, 1]\n    )\n\ntest &lt;-\n    lapply(s, \\(x) tibble::tibble(\n        mean = colMeans(x[, 51:100] - x[, 1:50]),\n        lwr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[1, ],\n        upr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[2, ]\n    ))\n\ntest2 &lt;- test$D\ntest2 &lt;- cbind(test2, WaffleDivorce)\n\nlibrary(ggplot2)\nggplot(test2, aes(y = forcats::fct_reorder(Location, mean),\n                                    x = mean, xmin = lwr, xmax = upr)) +\n    geom_pointrange() +\n    scale_x_continuous(\n        labels = function(x) scales::label_number()((x * sd(WaffleDivorce$Divorce)))\n    ) +\n    labs(\n        x = \"Counterfactual effect on divorce rate of halving marriage rate (mean, 89% CI)\",\n        y = NULL\n    ) +\n    zlib::theme_ms()\n\n\n\n\n\n\n\n\nFrom the precis, we can derive that for every 1 unit increase in the marriage rate, we expect \\(\\beta_M\\) units of change in the median age of marriage, and thus \\(\\beta_A \\beta_M\\) units of change in the divorce rate, which works out to approximately \\((-0.69)(-0.57) = 0.3933\\). So if the marriage rate increases by 1 standard deviation, we expect the divorce rate to increase by about 0.4 standard deviations.\nWe could also (much more easily, in fact, I just didn’t think about it until after I did this the hard way) compute the counterfactual effect for a range of values, and then look up whatever we wanted. The divorce rate measurements in the original data are only measured to the nearest tenth, so we just need to simulate the counterfactual effect of every one-tenth unit change in divorce rate over the range of observed rates.\n\n# Generate the sequence\nM_seq &lt;-\n    seq(\n        from = 4,\n        to = 16,\n        by = 0.01\n    )\n# Restandardize it using the original values for model units\nM_seq &lt;- (M_seq - mean(WaffleDivorce$Marriage)) /\n    sd(WaffleDivorce$Marriage)\n\n# Do the simulation\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\n\n# Clean up the results\nD_res &lt;-\n    tibble::tibble(\n        M = round(M_seq * sd(WaffleDivorce$Marriage) + mean(WaffleDivorce$Marriage),\n                            digits = 2),\n        mean = colMeans(s$D),\n        lwr = apply(s$D, 2, rethinking::PI)[1, ],\n        upr = apply(s$D, 2, rethinking::PI)[2, ]\n    ) |&gt;\n    dplyr::mutate(\n        dplyr::across(c(mean, lwr, upr), \\(x) x * sd(WaffleDivorce$Divorce) +\n                                        mean(WaffleDivorce$Divorce) |&gt; round(digits = 2))\n    )\n\nmanipulated &lt;-\n    WaffleDivorce |&gt;\n    dplyr::transmute(\n        Location, Loc, M = Marriage / 2, orig = Marriage, out = Divorce\n    ) |&gt;\n    dplyr::left_join(D_res, by = \"M\")\n\n# Make the plot\nD_res |&gt;\n    ggplot(aes(x = M, y = mean, ymin = lwr, ymax = upr)) +\n    geom_ribbon(fill = \"gray\") +\n    geom_line(size = 0.75) +\n    geom_point(\n        data = manipulated,\n        fill = \"white\",\n        color = \"black\",\n        shape = 21,\n        stroke = 1.5,\n        size = 3\n    ) +\n    labs(\n        x = \"Manipulated marriage rate\",\n        y = \"Counterfactual divorce rate\"\n    ) +\n    zlib::theme_ms()\n\n\n\n\n\n\n\n# manipulated |&gt;\n#   dplyr::mutate(id = factor(dplyr::row_number())) |&gt;\n#   ggplot() +\n#   geom_segment(\n#       aes(x = orig, xend = M, y = out, yend = mean, color = id),\n#       show.legend = FALSE,\n#       alpha = 0.5\n#   ) +\n#   geom_point(\n#       aes(x = M, y = mean, shape = \"Counterfactual\", fill = id),\n#       size = 3, color = \"black\"\n#   ) +\n#   geom_point(\n#       aes(x = orig, y = out, shape = \"Observed\", fill = id),\n#       size = 3, color = \"black\"\n#   ) +\n#   guides(\n#       fill = guide_none()\n#   ) +\n#   scale_shape_manual(values = c(21, 22)) +\n#   labs(\n#       x = \"Marriage rate\",\n#       y = \"Divorce rate\"\n#   ) +\n#   zlib::theme_ms()\n\nThere we go. The white points here show each of the states if their divorce rate were halved (and the model is true). I didn’t label them because I had already spend too much time on this project. There’s a lot more I could think of to do on this problem, but instead I decided to move on to the next one instead.\n\n\n5.2.12 5H3\nWe are given the following DAG for the milk energy problem.\n\ndag3 &lt;- dagitty::dagitty(\"dag {K &lt;- M -&gt; N -&gt; K}\")\n\nWe want to compute the counterfactual effect on K of doubling M, accounting for both the direct and indirect paths of causation.\n\ndata(\"milk\")\nm &lt;-\n    milk |&gt;\n    dplyr::transmute(\n        N = standardize(neocortex.perc),\n        M = standardize(log(mass)),\n        K = standardize(kcal.per.g)\n    ) |&gt;\n    tidyr::drop_na()\n\nm_5h3 &lt;-\n    rethinking::quap(\n        flist = alist(\n            # M -&gt; N\n            N ~ dnorm(mu_n, sigma_n),\n            mu_n &lt;- a_n + b_m * M,\n            a_n ~ dnorm(0, 0.2),\n            b_m ~ dnorm(0, 0.5),\n            sigma_n ~ dexp(1),\n            # M -&gt; K &lt;- N\n            K ~ dnorm(mu_k, sigma_k),\n            mu_k &lt;- a_k + b_m * M + b_n * N,\n            a_k ~ dnorm(0, 0.2),\n            b_n ~ dnorm(0, 0.5),\n            sigma_k ~ dexp(1)\n        ),\n        data = m\n    )\n\nrethinking::precis(m_5h3)\n\n                mean        sd       5.5%     94.5%\na_n     -0.008405637 0.1274276 -0.2120595 0.1952482\nb_m      0.417649823 0.1672193  0.1504011 0.6848986\nsigma_n  0.681045996 0.1323715  0.4694908 0.8926012\na_k      0.026405034 0.1660113 -0.2389131 0.2917232\nb_n     -0.138437700 0.2789888 -0.5843156 0.3074402\nsigma_k  1.223352397 0.2214232  0.8694753 1.5772295\n\n\nIn this case, the predictor we want the counterfactual effect of (\\(M\\)) is on a log scale, so we should be able to get the effect of halving it (since changes will be proportional on a multiplicative scale). But I’m not sure I formally understand what the “total counterfactual effect” is well enough to do that. So I’ll simulate instead.\n\n# Sequence in log units\nM_seq &lt;- seq(from = -3, to = 5, by = 0.01)\n\n# Standardize with original values to model units\nM_seq &lt;- (M_seq - mean(log(milk$mass), na.rm = TRUE)) /\n    sd(log(milk$mass), na.rm = TRUE)\n\n# Simulate the predictions\nsim_dat &lt;- data.frame(M = M_seq)\ns &lt;- sim(m_5h3, data = sim_dat, vars = c(\"N\", \"K\"))\n\nplot_data &lt;-\n    tibble::tibble(\n        M = exp(sim_dat$M * attr(m$M, \"scaled:scale\") +\n            attr(m$M, \"scaled:center\")),\n        K = colMeans(s$K) * attr(m$K, \"scaled:scale\") +\n            attr(m$K, \"scaled:center\")\n    )\nplot_PI &lt;- apply(s$K, 2, PI) * attr(m$K, \"scaled:scale\") +\n            attr(m$K, \"scaled:center\")\n\n# Plot the counterfactual effect\nplot(\n    plot_data$M, plot_data$K,\n    type = \"l\",\n    xlab = \"manipulated mass (kg)\", ylab = \"counterfactual kilocalories per gram of milk\",\n    xlim = exp(c(-3, 5)),\n    ylim = c(0.1, 1.1)\n)\nshade(plot_PI, plot_data$M)\nmtext(\"Total counterfactual effect of M on K\")\n\n\n\n\n\n\n\n\nAgain, the question didn’t say what number to double, but you can get any of them from a simulation like this.\n\n\n5.2.13 5H4\nThis is an open-ended problem where we will consider how to add the indicator of being in the South to the marriage problem. There are a few possibilities for the causal implications of this.\n\nSomething about Southerness directly affects age at marriage, marriage rate, and divorce rate all at the same time. (Or divorce rate, and one of the two predictors) That is, Southerness has both direct and indirect effects on divorce rate.\nSomething about Souttherness directly affects age at marriage and marriage rate, having only indirect effects on divorce rate. Alternatively, Southerness only impacts one of these two variables.\nSomething about Southernness directly affects divorce rate, with no indirect effects.\n\nMy first instinct is to say that age at marriage and marriage rate are primarily influenced by socioeconomic factors. Due to multiple historical factors (including slavery and an enduring legacy of systemic racism), the southern US, on average, has lower education rates and higher poverty rates as a region. Increased socioeconomic status tends to be associated with higher age of marriage, but a higher marriage rate and lower divorce rate (according to what I read while googling this).\nHowever, the southern U.S. also has a unique subculture (which varies widely across regions of the south), which could be a sociological cause of differences in some of these variables. For example, I think it is reasonable to say that traditional Southern culture encourages women to marry young, and also encourages women to get married in general – there is definitely a stereotype about older, unmarried women in traditional Southern culture.\nSo, based on both the socioeconomic reasons and the “culture” argument, there are two models I would like to examine. First, the model that posits a direct effect of Southerness on divorce rate, as well as indirect effects on both age at marriage and marriage rate. Then, I’d also like to examine the model where there is no direct effect of Southerness on divorce rate, and Southerness acts on divorce rate via age at marriage and marriage rate.\nLet’s examine the model with no direct effects first. We’ll consider the conditional independencies of this DAG.\n\ndag_indirect &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A; D &lt;- M -&gt; A; A -&gt; D}\")\nplot(dag_indirect)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_indirect)\n\nD _||_ S | A, M\n\n\nSo, if the data are consistent with this DAG, then the divorce rate should be independent of being in the South after we control for age at marriage and marriage rate. So let’s fit a model that does that. Since Southern status is an indicator variable, I wasn’t quite sure how to handle it in this model. In a frequentist framework, I would probably want to include an interaction term – this was briefly mentioned at the beginning of this chapter, but has not been covered in detail so I’ll just use a simple additive-effects-only model for now.\nBut first let’s consider the conditional independencies of the DAG with direct effects.\n\ndag_direct &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A; D &lt;- M -&gt; A; A -&gt; D; S -&gt; D}\")\nplot(dag_direct)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_direct)\n\nThere are no conditional independencies for this graph! So we should expect to see a relationship for all three parameters in the model that conditions on all three of our predictor variables.\nFortunately for us, we can use the same model to evaluate both of these DAGs – we just need to see which of the conditional independencies are supported by the result of predicting \\(D\\) using all three of them.\n\nd &lt;-\n    WaffleDivorce |&gt;\n    dplyr::transmute(\n        D = standardize(Divorce),\n        M = standardize(Marriage),\n        A = standardize(MedianAgeMarriage),\n        S = South\n    )\n\nm_s_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bS * S,\n            a ~ dnorm(0, 0.2),\n            bS ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_m_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_a_only &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bA * A,\n            a ~ dnorm(0, 0.2),\n            bA ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nm_all &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bM * M + bA * A + bS * S,\n            a ~ dnorm(0, 0.2),\n            bM ~ dnorm(0, 0.5),\n            bA ~ dnorm(0, 0.5),\n            bS ~ dnorm(0, 0.5),\n            sigma ~ dexp(1)\n        ),\n        data = d\n    )\n\nrethinking::precis(m_all)\n\n             mean         sd         5.5%      94.5%\na     -0.07590505 0.10600330 -0.245318792  0.0935087\nbM    -0.04224048 0.14778943 -0.278436533  0.1939556\nbA    -0.56163739 0.15090185 -0.802807694 -0.3204671\nbS     0.34998019 0.21572095  0.005216455  0.6947439\nsigma  0.76290395 0.07580295  0.641756198  0.8840517\n\ncoeftab_plot(\n    coeftab(\n        m_a_only, m_s_only, m_m_only, m_all\n    ),\n    pars = c(\"bA\", \"bS\", \"bM\")\n)\n\n\n\n\n\n\n\n\nSo, interestingly, from the estimated coefficients, we see that the coefficient for being in the South does not change that much when we control for both of the other variables. Of course, the CI crosses zero in the new model, so if we were doing bad statistics we would say that the effect has disappeared, but it was significant in the S-only model, which means that there is no direct effect of S. Since we are not doing bad statistics though, it seems unreasonable to claim that – there appears to be an effect of both \\(A\\) and \\(S\\) in the final model.\nRecall our previous model which was consistent with the idea that marriage rate only impacts divorce rate through the effect of age of marriage. So another potential DAG is like this.\n\ndag_med &lt;- dagitty::dagitty(\"dag {M &lt;- S -&gt; A -&gt; D; M -&gt; A -&gt;; S -&gt; D}\")\nplot(dag_med)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\ndagitty::impliedConditionalIndependencies(dag_med)\n\nD _||_ M | A, S\n\n\nSo the only casual independency of this DAG is whether D is independent of M after controlling for A and S, and after fitting our previous model we see that the data are consistent with this DAG. Arguably the data are not consistent with the other DAGs since those DAGs do not have any conditional independencies involving M, but the best model should still be chosen by a domain expert, not based on what the data are consistent with after observing the data.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables and the Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "cp6.html#chapter-notes",
    "href": "cp6.html#chapter-notes",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.1 Chapter notes",
    "text": "6.1 Chapter notes\n\nThe Selection-distortion effect (AKA Berkson’s/Berksonian bias, generalized to the idea of collider bias) occurs when the selection of a sample changes the relationship between the observed variables. (I.e. there is/isn’t a relationship between the two variables on the sample, but in the larger population, there isn’t/is a relationship.) Berkson’s bias refers to the particular effect that when selecting from a population on two desirable traits, there often appears to be a negative correlation between the desirable traits in the selected sample.\nMulticollinearity refers to a very strong association between two or more predictor variables, conditional on the other variables in the model. When variables are multicollinear, the posterior distribution will seem to suggest that none of the multicollinear variables are truly associated with the outcome, even if the reality is that they are all strongly associated.\nPost-treatment bias: a form of included variable bias where variables that are also causal descendents of the treatment, are controlled for when assessing the response. That is, you measure something that is not the outcome of interest and is also affected by the treatment, and you adjust for that quantity when analyzing the outcome. This induces collider bias.\nControlling for a collider on a DAG induces D-separation, meaning that the DAG is no longer connected.\nWhen you condition on a collider (a common descendent), it creates statistical, although not necessarily causal, relationships between the ancestors.\nEven unmeasured causes can induce collider bias. Selection bias in a study can often be interpreted as conditioning on a collider during the sampling process. See the parents and grandparents example in section 6.3.2.\nThere are four types of elemental confounds: DAG structures that allow us to determine causal and non-causal pathways.\n\nThe fork: two variables have a common cause (Z -&gt; X; Z -&gt; Y).\nThe pipe: one variable is intermediate in the causal relationship between two others (X -&gt; Z -&gt; Y).\nThe collider: two variables have a common descenent (X -&gt; Z; Y -&gt; Z).\nThe descendant: a variable which descends from another, capturing part of the ancestor’s statistical signal (in the previous example, if we also have Z -&gt; D, D will appear to be a collider as well, even if it is just a descendant).\n\nEvery DAG is built out of these four types of relationships, and we can use specific rules for DAGs to determine what variables need to be included in models for a causal effect.\n“Multiple regression is no oracle, but only a golem.”",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#exercises",
    "href": "cp6.html#exercises",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\n\n6.2.1 6E1\nThree mechanisms that can produce false inferences about causal effects in a multiple regression model are: multicollinearity, the selection- distortion effect, and post-treatment bias.\n\n\n6.2.2 6E2\nFor an example of post-treatment bias, consider a vaccine efficacy trial for influenza (or I guess any disease). Suppose we have a known surrogate of protection, an immunological measurement that is strongly associated with protection from the disease. For influenza, one potential biomarker is hemagglutinin inhibition (HI) titer, which is typically measured before and after (around 21 to 28 days) vaccination. If one did a challenge study or a long followup period of surveillance, we can record which individuals are ultimately infected with influenza. Including participants’ HI titers before vaccination when modeling vaccine protection is OK depending on the context, but including participants HI titer after vaccination would induce post-treatment bias, because vaccination directly affects HI titer.\n\n\n6.2.3 6E3\n\nThe fork (Z -&gt; X; Z -&gt; Y): X ⫫ Y | Z\nThe pipe (X -&gt; Z -&gt; Y): X ⫫ Y | Z\nThe collider (X -&gt; Z; Y -&gt; Z): X ⫫ Y; X !⫫ Y | Z\nThe descendent: conditional independencies are the same as the parent\n\n\n\n6.2.4 6E4\nSuppose we have two variables (call one the exposure and the other the outcome) which are both causes of a third variable. If that third variable determines which observations we observe (for example, a restaurant existing or a patient agreeing to participate in a study), in our observed sample we will see a correlation between the exposure and the outcome, just because we are only seeing observations where the third variable is already specified.\nIn the funded grants example, a funded grant must be high in at least one of newsworthiness or trustworthiness, otherwise it will not be funded. If we could see all grants, we would not see a correlation between newsworthiness and trustworthiness. But when we only look at funded grants, we condition on a common descendant of both variables (a collider), which makes a spurious relationship appear.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#m1",
    "href": "cp6.html#m1",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.3 6M1",
    "text": "6.3 6M1\nThe new DAG including \\(V\\) as an unobserved common cause of \\(C\\) and \\(Y\\) looks like this.\n\ndag_6m1 &lt;- dagitty::dagitty(\n    \"dag {\n        U [unobserved]\n        V [unobserved]\n        X -&gt; Y\n        X &lt;- U &lt;- A -&gt; C -&gt; Y\n        U -&gt; B &lt;- C\n        C &lt;- V -&gt; Y\n    }\"\n)\ndagitty::coordinates(dag_6m1) &lt;-\n    list(\n        x = c(U = 1, V = 4, X = 1, Y = 3, A = 2, B = 2, C = 3),\n        y = c(U = 1.5, V = 2.5, X = 3, Y = 3, A = 1, B = 2, C = 1.5)\n    )\ndagitty::exposures(dag_6m1) &lt;- \"X\"\ndagitty::outcomes(dag_6m1) &lt;- \"Y\"\nplot(dag_6m1)\n\n\n\n\n\n\n\n\nWe still have all of the same paths from the previous example, i.e.:\n\n\\(X \\to Y\\),\n\\(X \\leftarrow U \\leftarrow A \\to C \\to Y\\), and\n\\(X \\leftarrow U \\to B \\leftarrow C \\to Y\\).\n\nThe first path is the direct path. The second is an open backdoor path through \\(A\\). The third is a closed backdoor path, as it passes through the collider \\(B\\). By adding the unobserved confounder \\(V\\), we create two new backdoor paths,\n\n\\(X \\leftarrow U \\leftarrow A \\to C \\leftarrow V \\to Y\\), and\n\\(X \\leftarrow U \\to B \\leftarrow C \\leftarrow V \\to Y\\).\n\nThe first path is an open backdoor path, and the second path is a closed backdoor path. We can check which paths exist and are open with dagitty.\n\ndagitty::paths(dag_6m1)\n\n$paths\n[1] \"X -&gt; Y\"                     \"X &lt;- U -&gt; B &lt;- C -&gt; Y\"     \n[3] \"X &lt;- U -&gt; B &lt;- C &lt;- V -&gt; Y\" \"X &lt;- U &lt;- A -&gt; C -&gt; Y\"     \n[5] \"X &lt;- U &lt;- A -&gt; C &lt;- V -&gt; Y\"\n\n$open\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\n\nNow we need to close the two open paths, without opening either of the closed paths. Conditioning on \\(C\\) would close both of the open paths, but would also open the fifth path. However, conditioning on \\(A\\) will close both open paths without opening either of the closed paths. So \\(\\{A\\}\\) is our sufficient adjustment set. We can verify this with dagitty.\n\ndagitty::adjustmentSets(dag_6m1)\n\n{ A }",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#m2",
    "href": "cp6.html#m2",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.4 6M2",
    "text": "6.4 6M2\nFirst we’ll do the simulation: we want \\(X\\) and \\(Z\\) to be highly correlated.\n\nset.seed(101)\nX &lt;- rnorm(100, 0, 1)\nZ &lt;- rnorm(100, X, 0.5)\nY &lt;- rnorm(100, Z, 1)\n\ncor(cbind(X, Z, Y))\n\n          X         Z         Y\nX 1.0000000 0.8925919 0.6779960\nZ 0.8925919 1.0000000 0.7998096\nY 0.6779960 0.7998096 1.0000000\n\n\nWe can see that all of the variables are strongly associated, but \\(X\\) and \\(Z\\) have a particularly strong correlation. But now we want to use a model that adjusts for both.\n\nfit_6m1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            Y ~ dnorm(mu, sigma),\n            mu &lt;- a + bX * X + bZ * Z,\n            a ~ dnorm(0, 0.5),\n            c(bX, bZ) ~ dnorm(0, 1),\n            sigma ~ dexp(1)\n        ),\n        data = list(X = X, Y = Y, Z = Z)\n    )\n\ncoeftab(fit_6m1) |&gt;\n    coeftab_plot(pars = c(\"bX\", \"bZ\"))\n\n\n\n\n\n\n\n\nInterestingly, we can see that we do not get the same problem as the previous multicollinearity example. The confidence intervals appear to be reasonable, and we see a strong effect of \\(Z\\) but no effect of \\(Y\\). Intuitively, this make sense – \\(Z\\) and \\(Y\\) have a stronger correlation than \\(X\\) and \\(Y\\), so after we control for \\(Z\\), the model “finds” all of the signal, and then does not find an effect of \\(X\\). So if we interpreted this model without considering the causal framework, we would still be mislead by the multicollinearity, but there is nothing obviously wrong – the entire causal effect of \\(X\\) on \\(Z\\) is through \\(Z\\), so this estimate of the direct causal effect of \\(X\\) makes sense.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#m3",
    "href": "cp6.html#m3",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.5 6M3",
    "text": "6.5 6M3\nHere are the adjustment sets for each of the DAGs shown.\n\nTop left DAG: Z only\nTop right: nothing\nBottom left: nothing\nBottom right: A only\n\nI also checked using dagitty to verify my answers.\n\ndag1 &lt;- dagitty::dagitty(\"dag {Z -&gt; X -&gt; Y; A -&gt; Z -&gt; Y; A -&gt; Y}\")\ndag2 &lt;- dagitty::dagitty(\"dag {X -&gt; Z -&gt; Y; A -&gt; Z -&gt; Y; A -&gt; Y}\")\ndag3 &lt;- dagitty::dagitty(\"dag {X -&gt; Y -&gt; Z; A -&gt; X -&gt; Z; A -&gt; Z}\")\ndag4 &lt;- dagitty::dagitty(\"dag {A -&gt; X -&gt; Z; A -&gt; Z -&gt; Y; X -&gt; Y}\")\n\nlapply(list(dag1, dag2, dag3, dag4), dagitty::adjustmentSets, \"X\", \"Y\")\n\n[[1]]\n{ Z }\n\n[[2]]\n {}\n\n[[3]]\n {}\n\n[[4]]\n{ A }",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#h1",
    "href": "cp6.html#h1",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.6 6H1",
    "text": "6.6 6H1\nUsing the Waffle House data, we want to find the total causal influence of number of Waffle Houses on divorce rate. First, let’s look at what we have to work with.\n\ndata(\"WaffleDivorce\")\nhead(WaffleDivorce)\n\n    Location Loc Population MedianAgeMarriage Marriage Marriage.SE Divorce\n1    Alabama  AL       4.78              25.3     20.2        1.27    12.7\n2     Alaska  AK       0.71              25.2     26.0        2.93    12.5\n3    Arizona  AZ       6.33              25.8     20.3        0.98    10.8\n4   Arkansas  AR       2.92              24.3     26.4        1.70    13.5\n5 California  CA      37.25              26.8     19.1        0.39     8.0\n6   Colorado  CO       5.03              25.7     23.5        1.24    11.6\n  Divorce.SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860\n1       0.79          128     1     435080         964201           0.45\n2       2.05            0     0          0              0           0.00\n3       0.74           18     0          0              0           0.00\n4       1.22           41     1     111115         435450           0.26\n5       0.24            0     0          0         379994           0.00\n6       0.94           11     0          0          34277           0.00\n\n\nOK, so of course we will assume that there is a direct causal effect of number of Waffle Houses (\\(W\\)) on divorce rate (\\(D\\)). From previous work, we know our data are consistent with a \\(M \\to A \\to D\\) DAG structure, for \\(M\\) the marriage rate and \\(A\\) the median age at marriage, so we’ll incorporate this into our DAG. We also saw that the data are consistent with being in the South affecting \\(M\\) and \\(A\\), so we’ll include that in our DAG, and of course we expect to see \\(S \\to W\\). Finally, since we’re trying to find the total causal effect of \\(W\\), we’ll include \\(A \\leftarrow W \\rightarrow M\\) as a sub-DAG as well. Putting it all together, our DAG looks like this.\n\ndag_6h1 &lt;-\n    dagitty::dagitty(\n        \"dag {\n        M -&gt; A -&gt; D\n        M &lt;- S -&gt; A\n        S -&gt; W\n        W -&gt; D\n        M &lt;- W -&gt; A\n        }\"\n    )\ndagitty::exposures(dag_6h1) &lt;- \"W\"\ndagitty::outcomes(dag_6h1) &lt;- \"D\"\ndagitty::coordinates(dag_6h1) &lt;-\n        list(\n        x = c(S = 1, W = 2, D = 2.7, M = 1, A = 2),\n        y = c(S = 1, W = 1, D = 1.5, M = 2, A = 2)\n    )\n\nplot(dag_6h1)\n\n\n\n\n\n\n\n\nLet’s now figure out what needs to be in our model.\n\ndagitty::adjustmentSets(dag_6h1)\n\n{ S }\n\n\nWe see that to get the total cause effect of \\(W\\) on \\(D\\), we need only adjust for \\(S\\), being in the South. I should probably worry about things like transformations and zero-inflation, but for this exercise I am not going to do that.\n\nwd &lt;- \n    list(\n        W = standardize(WaffleDivorce$WaffleHouses),\n        D = standardize(WaffleDivorce$Divorce),\n        S = WaffleDivorce$South + 1\n    )\n\nNow we’ll fit the model. I’ll allow the intercept to be different for Southern and non-Southern states, but because we’re interested in the total causal effect of Waffle House Numbers, I’ll force the effect to be the same across both groups.\n\nset.seed(100)\nfit_6h1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a[S] + b * W,\n            a[S] ~ dnorm(0, 1),\n            b ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd\n    )\n\nrethinking::precis(fit_6h1, depth = 2)\n\n             mean         sd        5.5%     94.5%\na[1]  -0.18668305 0.16832424 -0.45569771 0.0823316\na[2]   0.46329565 0.29995319 -0.01608748 0.9426788\nb      0.05197162 0.17641807 -0.22997853 0.3339218\nsigma  0.92058385 0.09085178  0.77538516 1.0657825\n\n\nOK, I’m getting a warning but I don’t think it’s doing anything. So I’ll just ignore it. For this model, we can see that there is a strongly positive effect of \\(b\\). Let’s look at the posterior distribution.\n\npost_6h1 &lt;- extract.samples(fit_6h1)\ndens(post_6h1$b)\nabline(v = 0, lty = 2)\n\n\n\n\n\n\n\n\nWe can see that almost all of the posterior density is above zero, indicating that there is a positive effect of Waffle Houses on divorce rate. For every 1 standard deviation increase in the number of Waffle Houses in a state, the divorce rate is expected to increase by about 0.25 units. Let’s put that back from standardized units into real units.\n\n1 * attr(wd$W, \"scaled:scale\") + attr(wd$W, \"scaled:center\")\n\n[1] 98.12959\n\n0.25 * attr(wd$D, \"scaled:scale\") + attr(wd$D, \"scaled:center\")\n\n[1] 10.1432\n\n\nSo we see that we expect the divorce rate to increase by about \\(10\\%\\) for every 98 additional Waffle Houses, or approximately \\(0.1\\%\\) per Waffle House. This is the total causal effect based on our DAG, even though I would guess that the direct effect is zero and this entire effect is through location.\n\n6.6.1 6H2\nFirst, we need to check the implied causal independencies of the DAG.\n\ndagitty::impliedConditionalIndependencies(dag_6h1)\n\nD _||_ M | A, W\nD _||_ S | A, W\n\n\nTest one: \\(D\\) and \\(M\\) should be independent after adjusting for \\(A\\) and \\(W\\).\n\nset.seed(100)\nwd2 &lt;- c(\n    wd,\n    M = list(standardize(WaffleDivorce$Marriage)),\n    A = list(standardize(WaffleDivorce$MedianAgeMarriage))\n)\n\nfit_6h2_a1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bm * M + ba * A + bw * W,\n            a ~ dnorm(0, 1),\n            c(bm, ba, bw) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nfit_6h1_a2 &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bm * M,\n            a ~ dnorm(0, 1),\n            c(bm) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nrethinking::coeftab(\n    fit_6h2_a1,\n    fit_6h1_a2\n) |&gt;\n    rethinking::coeftab_plot(pars = c(\"bm\"))\n\n\n\n\n\n\n\n\nYes, we can see that if we only include \\(M\\) (fit_6h1_a2), we see an effect, but if we control for \\(A\\) and \\(W\\), as in fit_6h1_a1, we do not.\nNow let’s check the second test: \\(D\\) and \\(S\\) should be independent if we control for \\(A\\) and \\(W\\).\n\nset.seed(100)\nfit_6h2_b1 &lt;-\n    rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bs * S + ba * A + bw * W,\n            a ~ dnorm(0, 1),\n            c(bs, ba, bw) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nfit_6h1_b2 &lt;-\n        rethinking::quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu &lt;- a + bs * S,\n            a ~ dnorm(0, 1),\n            c(bs) ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = wd2\n    )\n\nrethinking::coeftab(\n    fit_6h2_b1,\n    fit_6h1_b2\n) |&gt;\n    rethinking::coeftab_plot(pars = c(\"bs\"))\n\n\n\n\n\n\n\n\nWe see exactly the same interpretation here: when only \\(S\\) is in the model, all of the posterior density is above 0, but when we control for \\(A\\) and \\(W\\), a significant portion is below zero and the mean is lower. So I think we can say that our data appear to be consistent with the conditional independencies that our DAG implies.\nWe can also do these tests automatically used the method recommended by dagitty. I don’t know that much about these results, but they agree with our modeling results, which is good!\n\ndagitty::localTests(\n    dag_6h1,\n    sample.cov = lavaan::lavCor(as.data.frame(wd2)),\n    sample.nobs = nrow(as.data.frame(wd2))\n)\n\n                  estimate   p.value       2.5%     97.5%\nD _||_ M | A, W -0.0855530 0.5650794 -0.3613429 0.2035528\nD _||_ S | A, W  0.1349902 0.3622409 -0.1550991 0.4044034",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#h3",
    "href": "cp6.html#h3",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.7 6H3",
    "text": "6.7 6H3\nNow we are back to the fox problems. I’ll reproduce the DAG first just so I have it in my notes.\n\nfox_dag &lt;-\n    dagitty::dagitty(\n        \"dag {\n        A -&gt; F -&gt; G -&gt; W\n        F -&gt; W\n        }\"\n    )\ndagitty::coordinates(fox_dag) &lt;-\n    list(\n        x = c(A = 2, F = 1, G = 3, W = 2),\n        y = c(A = 1, F = 2, G = 2, W = 3)\n    )\nplot(fox_dag)\n\n\n\n\n\n\n\n\nFirst, we want to infer the total causal influence of area (A) on weight (W). I’ll go ahead and set up the data, and standardize all the variables as McElreath recommends. I’ll also model the log weight instead of the raw weight so we can ensure that our predictions remain positive.\n\ndata(foxes)\nf2 &lt;-\n    foxes |&gt;\n    dplyr::transmute(\n        A = area,\n        F = avgfood,\n        G = groupsize,\n        W = log(weight)\n    ) |&gt;\n    as.list() |&gt;\n    lapply(FUN = rethinking::standardize)\n\nNow we’ll adopt a model for the weight. Since it is logged and standardized we’ll use a Gaussian likelihood function. To set priors, we first need to determine our adjustment set. There are two causal paths between \\(A\\) and \\(W\\):\n\n\\(A \\to F \\to W\\) and\n\\(A \\to F \\to G \\to W\\).\n\nIf we want the total causal effect of \\(A\\) on \\(W\\), there are no closed paths and we do not need to adjust for any other variables. We can confirm this with dagitty.\n\ndagitty::adjustmentSets(\n    fox_dag,\n    exposure = \"A\",\n    outcome = \"W\",\n    effect = \"total\"\n)\n\n {}\n\n\nWe get the empty set as our minimal sufficient adjustment set. So we’ll adopt the following model.\n\\[\n\\begin{align*}\n\\mathrm{Scale}\\left(\\log W \\right) &\\sim N(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\mathrm{Scale}(A)\\\\\n\\alpha &\\sim N(4.5, 0.5) \\\\\n\\beta &\\sim N(0, 0.25) \\\\\n\\sigma &\\sim \\mathrm{Exp}(10)\n\\end{align*}\n\\]\nI tuned the parameters for the prior distributions using a prior predictive simulation to ensure that the prior predictions stay within the expected outcome space. As usual, I think that making this model have an intercept of zero potentially makes more sense (a fox with no area should starve) and then we would not expect this relationship to be linear, but we will just mess with the priors until the predictions look reasonable.\n\nset.seed(101)\na &lt;- rnorm(1000, 4.5, 0.5)\nb &lt;- rnorm(1000, 0, 0.25)\nsigma &lt;- rexp(1000, 10)\n\nplot(\n    NULL,\n    xlim = c(1, 5.25),\n    ylim = c(1.5, 7.5),\n    xlab = \"Area\",\n    ylab = \"Weight\",\n    main = \"Prior predictive simulation\"\n)\n\nx &lt;- seq(min(f2$A), max(f2$A), length.out = 1000)\nxp &lt;- x * attr(f2$A, \"scaled:scale\") + attr(f2$A, \"scaled:center\")\nout &lt;- vector(length = 1000, mode = \"list\")\nfor (i in 1:1000) {\n    # Sample y's from their distribution\n    y &lt;- rnorm(1000, a[i] + b[i] * x, sigma[i])\n    \n    # Backtransform to original scale\n    yp &lt;- exp(y * attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))\n    out[[i]] &lt;- yp\n    \n    lines(x = xp, y = y, type = \"l\", col = rethinking::col.alpha(\"black\", 0.05))\n}\n\n\n\n\n\n\n\n\nI think that looks fine but I have to admit that I find assigning reasonable priors to standardized data quite difficult, it feels like just randomly picked numbers under the lines look ok. Now we can finally fit our regression and get the estimated causal effect.\n\nm_6h3 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + b * A,\n        a ~ dnorm(4.5, 0.5),\n        b ~ dnorm(0, 0.25),\n        sigma ~ dexp(10)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h3)\n\n            mean         sd         5.5%     94.5%\na     0.14011603 0.08988426 -0.003536369 0.2837684\nb     0.03384817 0.08470967 -0.101534245 0.1692306\nsigma 0.96550793 0.06089243  0.868190063 1.0628258\n\n\nLet’s look at the posterior distribution of \\(\\beta\\).\n\npost &lt;- extract.samples(m_6h3)\ndens(post$b, lwd = 2)\nabline(v = 0, lty = 2)\n\n\n\n\n\n\n\n\nWe can see that while the density of \\(\\beta\\) is more than 50% above zero, there is a substantial amount of the density on either side of zero. So in general, it seems that area size is not directly correlated to fox weight. If anything, there is a small positive effect, but it is not very strong. We can get predictions on the original scale as well.\n\nmodel_out &lt;- rethinking::sim(m_6h3, data = list(A = x), n = 10000)\nmodel_mu &lt;-\n    colMeans(model_out) |&gt;\n    (\\(x) x *attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))() |&gt;\n    exp()\n\nmodel_pi &lt;- apply(model_out, 2, \\(x) exp(rethinking::PI(x) * \n                                        attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\")))\n\nplot(\n    xp, model_mu,\n    type = \"l\",\n    xlim = range(xp),\n    ylim = range(foxes$weight),\n    xlab = \"Simulated area\",\n    ylab = \"Counterfactual weight\"\n)\nlines(xp, model_pi[1, ], lty = 2)\nlines(xp, model_pi[2, ], lty = 2)\n\n\n\n\n\n\n\n\nAs we would expect from the model estimates, there is a slight positive trend with incredibly wide credible intervals.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#h4",
    "href": "cp6.html#h4",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.8 6H4",
    "text": "6.8 6H4\nNext we want to infer the total causal effect of adding food. Since we want to know the total causal effect, we don’t need to adjust for anything else in the model, since \\(G\\) is a mediator on the causal path. This time I’ll just use standard priors since it doesn’t really matter that much.\n\nm_6h4 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + b * `F`,\n        a ~ dnorm(0, 1),\n        b ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h4)\n\n               mean         sd       5.5%     94.5%\na      9.224085e-10 0.09165572 -0.1464835 0.1464835\nb     -1.533812e-02 0.09205000 -0.1624518 0.1317756\nsigma  9.913351e-01 0.06467093  0.8879784 1.0946917\n\n\nOK, we see a similar thing here. There is a possibly a slight negative relationship, but it looks like there is really no relationship here.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#h5",
    "href": "cp6.html#h5",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.9 6H5",
    "text": "6.9 6H5\nNow we want to get the total causal effect of group size, \\(G\\). Now we have to also control for food, \\(F\\), because it is a confounder on one of the paths from \\(G \\to W\\).\n\nm_6h5 &lt;- rethinking::quap(\n    flist = alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- a + bF * `F` + bG * G,\n        a ~ dnorm(0, 1),\n        bF ~ dnorm(0, 1),\n        bG ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\nrethinking::precis(m_6h5)\n\n               mean         sd       5.5%      94.5%\na      1.467513e-05 0.08752970 -0.1398747  0.1399040\nbF     5.602422e-01 0.19672784  0.2458332  0.8746513\nbG    -6.435327e-01 0.19672896 -0.9579436 -0.3291218\nsigma  9.463559e-01 0.06178686  0.8476086  1.0451032\n\n\nOk, interestingly we can now see a strong positive effect of \\(F\\) and a strong negative effect of \\(G\\). If we plot the data stratified by group size, we can understand this effect a bit better. This is an example of a masked relationship. Overall, average food appears to have a negative effect on weight, which doesn’t seem to make sense.\nHowever, for groups of a given size, the more food there is, the heavier those foxes tend to be. But for healthier (heavier) foxes, it is likely that more new foxes are born, and despite the fact that the foxes are in a more abundant area, there is less food per fox. So within a group size, having more food is good. But if the group size expands without a simultaneous increase in food supply, there will be less food available for each fox. However, note that for the high group sizes, only one group was observed in each. So perhaps our estimates contain selection bias, if the groups that were recorded are not typical examples of foxes with high group sizes.\n\nggplot(foxes, aes(x = avgfood, y = weight, col = factor(groupsize))) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\", alpha = 0.5) +\n    zlib::theme_ms() +\n    geom_smooth(method = \"lm\", aes(group = 1, color = \"overall\")) +\n    scale_color_manual(\n        values = c(viridisLite::viridis(7), \"black\")\n    ) +\n    labs(x = \"Average food\", y = \"Weight\", col = \"Group size\")\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp6.html#h6-and-6h7",
    "href": "cp6.html#h6-and-6h7",
    "title": "6  The Haunted DAG and the Causal Terror",
    "section": "6.10 6H6 and 6H7",
    "text": "6.10 6H6 and 6H7\nI decided to skip the open research questions.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG and the Causal Terror</span>"
    ]
  },
  {
    "objectID": "cp7.html#chapter-notes",
    "href": "cp7.html#chapter-notes",
    "title": "7  Ulysses’ Compass",
    "section": "7.1 Chapter notes",
    "text": "7.1 Chapter notes\n\nWhen we think about Copernicus and the heliocentric model, we have to remember that the geocentric model makes very good predictions, despite being wrong. Models that are completely causally wrong can make great predictions.\nThe principle of parsimony is often used to distinguish between models which make good predictions, but even this is not always useful for us in science.\n\\(R^2\\) is a commonly used method for choosing the “best” regression model, but even this is not correct. In general, overfitting and underfitting are both dangerous and we must be wary of methods which can lead us to overfit. The notion of overfitting and underfitting is related to the bias-variance tradeoff.\nTo construct useful measures in a Bayesian framework, we need to consider the entropy of our models – that is, how much is our uncertainty reduced if we learn an outcome? The information entropy is the function \\[h(p) = -\\sum_{i=1}^n p_i \\cdot \\log(p_i).\\]\nWe can relate the entropy of the model to the accuracy of our predictions using the Kullback-Leibler divergence: the additional uncertainty induced by using probabilities from one distribution to describe another. The divergence is given by \\[D_{KL}(p, q) = \\sum_i p_i \\log \\left( \\frac{p_i}{q_i}\\right).\\]\nUsing the divergence, or the related deviance, we cannot estimate how close a model is to the truth. However, we can tell which of a set of models is closest to the truth, and how much better it is from the others.\nWe estimate the deviance as \\(-2 \\times \\texttt{lppd}\\) where \\(\\texttt{lppd}\\) is the log pointwise predictive density. The formula is omitted here but is on page 210 of the book.\nImportantly, we cannot simply score models on the data used to fit the models, because this leads to overfitting.\nWe can often use regularizing priors, which are skeptical and try to prevent the model from taking on extreme values, to improve our out-of-sample performance.\nThe traditional way to approximate out-of-sample error is by using Cross-Validation, specifically Leave-One-Out CV (LOOCV).\nSince LOOCV is computationally very expensive, we want to approximate it. One method is called Pareto-smoothed importance sampling (PSIS), and another is called the Widely Applicable Information Criterion (WAIC). See the text, section 7.4 for details on both methods.\nAs a sidenote, this chapter discusses robust regression using a \\(t\\) likelihood instead of a Gaussian likelihood on page 233.\nI really enjoyed the metaphor that “if we search hard enough, we are bound to found a Curse of Tippicanoe” – if we torture the data enough, it will confess.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "cp7.html#exercises",
    "href": "cp7.html#exercises",
    "title": "7  Ulysses’ Compass",
    "section": "7.2 Exercises",
    "text": "7.2 Exercises\n\n7.2.1 7E1\nThe three motivating criteria which define information entropy are\n\nEntropy should be a continuous-valued function;\nAs the size of the sample space increases, entropy should increase for events that are equally likely; and\nIf the entropy associated with the event \\(E_1\\) is \\(h_1\\) and the entropy associated with the event \\(E_2\\) is \\(h_2\\), then the entropy associated with the event \\(E_1 \\cup E_2\\) should be \\(h_1 + h_2\\).\n\n\n\n7.2.2 7E2\nIf a coin is weighted such that when the coin is flipped, the probability of heads is \\(70\\%\\) is given by \\[h = -\\left( 0.7 \\cdot \\log(0.7) + 0.3 \\cdot \\log(0.3) \\right) \\approx 0.61,\\] because the only other possibility is that the coin lands on tails, which occurs with probability \\(0.3\\).\n\n\n7.2.3 7E3\nSuppose that a four-sided die is weighted so that each possible outcome occurs with the frequency given in the following table.\n\n\n\nroll\np\n\n\n\n\n1\n0.20\n\n\n2\n0.25\n\n\n3\n0.25\n\n\n4\n0.30\n\n\n\nThe entropy is then \\[h = -\\sum_{i = 1}^4 p_i \\cdot \\log p_i \\approx 1.38.\\]\n\n\n7.2.4 7E4\nSuppose we have another 4-sided die when the sides 1, 2, and 3 occur equally often but the side 4 never occurs. If \\(X\\) is the random variable representing the result of the die roll, we calculate the entropy over the support of \\(X\\), which is \\(S(X) = \\{1, 2, 3\\}\\) and we leave the value of 4 out of the calculation entirely. The entropy is then\n\\[h = -3\\left( \\frac{1}{3}\\cdot\\log\\frac{1}{3}\\right) \\approx 1.10.\\]\n\n\n7.2.5 7M1\nThe definition of the AIC is \\[\\mathrm{AIC} = -2(\\mathrm{lppd} - p),\\] while the definition of the WAIC is \\[\\mathrm{WAIC} = -2 \\left(\\mathrm{lppd} - \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right)\\right).\\]\nBoth of these formulas involve comparing the lppd to some penalty term, which is more general for the WAIC than for the AIC. In order for the AIC to be similar to the WAIC, we need to make assumptions which lead to the equivalence \\[ p = \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right), \\] on average.\nAccording to the text, this will occur if we assume that the priors are flat (or are overwhelmed by the likelihood), the posterior distribution is approximately multivariate Gaussian, and the sample size is much greater than the number of parameters. So for models with complicated, hierarchical likelihoods, which are common in actual research questions, the AIC will likely not be a good approximation to the WAIC, and unfortunately the AIC gives us no diagnostic criteria to determine when it fails.\n\n\n7.2.6 7M2\nModel selection concerns selecting one model out of a group and discarding the others. Typically one would use the remaining model to make inferences or predictions. However, model comparison involves investigating the differences between multiple models, including comparing the criterion values and comparing the estimates and predictions. When we choose to do selection rather than comparison, we lose all of the information encoded in the differences between the model – knowing which differences in models change criterion values and estimates is crucial information which can inform our understanding of the system.\n\n\n7.2.7 7M3\nWe need to fit models to the exact same set of data points in order to compare them based on WAIC because changing the data points will change the WAIC even if nothing about the model changes. If we were to use the same number of data points, but different sets of data, the WAIC will fluctuate based on properties of the data, and we could get different results when we compare the same models.\nThe penalty parameter of the WAIC also depends on the value of \\(N\\), the number of data points. If we were to drop data points (for example, due to missing data in some models, but not others), we would expect the WAIC to increase (become worse) because we have less information relative to the complexity of the models. Conversely, if we increased the number of data points the WAIC could be better just because of that. We could then make an incorrect decision by comparing the WAICs from models fit on different data.\n\n\n7.2.8 7M4\nAs the width of the priors decreases (i.e. the priors become more concentrated), the WAIC penalty term shrinks. The WAIC penalty term is based on the variances of individual probability estimates across samples. As the width of a prior is narrowed, the model will tend to produce samples for each individual that are closer together on average and thus the penalty term will decrease. However, since the lppd also changes when we change the priors we cannot say for sure whether this increases or decreases the overall WAIC.\n\n\n7.2.9 7M5\nWhen we use informative priors, we make the model more skeptical of extreme values in the data, and less trustworthy of values that would pull the model away from the priors. The data thus need to contain a large amount of evidence to make extreme values more likely in the posterior distribution. Under these conditions, the model is “excited” less by the training sample – and thus, the model fitting process is more robust to variations in the training sample due to sample error. Ideally, the model will capture less of the noise in the data while still capturing strong underlying trends, improving the performance of the model at explaining novel data from the same data generating process.\n\n\n7.2.10 7M6\nOverly informative priors, or in the worst case, degenerate priors, will dominate the model and prevent the model from learning from the data. If a prior is too narrow, the data cannot provide enough evidence to move the model away from the priors. Such a model is so skeptical of the data that it does not pick up the noise from sampling variability in the data, nor does it pick up any signal from the underlying trends either. Because the model has learned nothing from the data, we could make predictions just as good by making up random numbers.\n\n\n7.2.11 7H1\nFor this exercise, we want to fit a curve to the Laffer data. First let’s load and plot the data. Note that I’ve gone ahead and standardized the independent variable, tax rate.\n\ndata(Laffer)\nd &lt;- list(\n    x = standardize(Laffer$tax_rate),\n    y = Laffer$tax_revenue\n)\n\nplot(\n    d,\n    xlab = \"Standardized tax rate\",\n    ylab = \"Tax revenue\"\n)\n\n\n\n\n\n\n\n\nFor this exercise, I’ll fit three models to the data: a regular linear model, a quadratic polynomial model, and a cubic polynomial model. For a real research question, I would also probably consider a spline model, but that is too much work for no payoff here. I messed around for a bit and tried to get a model like the curve from the image in the book, but that is laughably wrong and impossible to fit, so it didn’t work out.\n\nset.seed(123123)\nm1 &lt;- rethinking::quap(\n    alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- b0 + b1 * x,\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\nm2 &lt;- rethinking::quap(\n    alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- b0 + b1 * x + b2 * I(x ^ 2),\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        b2 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\nm3 &lt;- rethinking::quap(\n    alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- b0 + b1 * x + b2 * I(x ^ 2) + b3 * I(x ^ 3),\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        b2 ~ dnorm(0, 2),\n        b3 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\n\nThe first thing we probably want to do is plot the predictions of these models.\n\nx_m &lt;- attr(d$x, \"scaled:center\")\nx_s &lt;- attr(d$x, \"scaled:scale\")\nx_vec &lt;- list(x = seq(min(d$x), max(d$x), length.out = 500))\nx_tf &lt;- x_vec$x * x_s + x_m\n\nplot(\n    x = d$x * x_s + x_m,\n    y = d$y,\n    xlab = \"Tax rate\",\n    ylab = \"Tax revenue\"\n)\n\nm1_post &lt;- rethinking::link(m1, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m1_post,\n    lty = 1, col = \"black\",\n    lwd = 2\n)\n\nm2_post &lt;- rethinking::link(m2, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m2_post,\n    lty = 2, col = \"blue\",\n    lwd = 2\n)\n\nm3_post &lt;- rethinking::link(m3, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m3_post,\n    lty = 3, col = \"red\",\n    lwd = 2\n)\n\nlegend(\n    x = \"topleft\",\n    legend = c(\"Linear\", \"Quadratic\", \"Cubic\"),\n    col = c(\"black\", \"blue\", \"red\"),\n    lty = c(1, 2, 3),\n    lwd = c(2, 2, 2)\n)\n\n\n\n\n\n\n\n\nOK, so we can first of all see that none of those models are even anywhere close to what that editorial showed. None of them really get pulled towards the point with the high tax revenue, but we’ll check that better in the next question. The next thing that we need to do is compare the models. First let’s check the WAIC.\n\nrethinking::compare(m1, m2, m3)\n\n       WAIC       SE      dWAIC      dSE    pWAIC    weight\nm2 124.6255 24.81799 0.00000000       NA 7.640306 0.4111828\nm1 124.6665 22.90424 0.04095811 2.739979 6.513822 0.4028478\nm3 126.2125 24.78288 1.58691188 1.341284 8.774990 0.1859694\n\n\nAll of the models appear to perform about the same. So even though the linear model had technically the lowest WAIC, it’s probably preferable to use that model since the gains in performance are smaller than the standard errors of the WAIC estimates. Let’s check PSIS as well.\n\nrethinking::compare(m1, m2, m3, func = \"PSIS\")\n\nSome Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points.\n\n\n       PSIS       SE     dPSIS      dSE     pPSIS       weight\nm2 128.4209 28.46723  0.000000       NA  9.518310 0.6014412426\nm1 129.2479 27.64550  0.826972  2.08552  8.797377 0.3977576351\nm3 141.6631 38.33021 13.242141 10.10142 16.458178 0.0008011224\n\n\nWe’ll address the issues with the high Pareto \\(k\\) values in the next question. For now we can see that model 1, the linear model, has the lowest PSIS, although again we can see that the standard errors are much higher than the differences in performance. But since we have few data points, adopting the simpler model will likely be a better choice and it seems that there is some linear relationship between tax revenue and tax rate. Let’s look at the model summary for m1.\n\nsummary(m1)\n\n          mean        sd       5.5%    94.5%\nb0    3.228658 0.3069237 2.73813453 3.719181\nb1    0.566714 0.3116142 0.06869426 1.064734\nsigma 1.669183 0.2150402 1.32550725 2.012859\n\n\nWe see that, on average, we would expect for a 1 standard deviation increase in the tax rate, we would expect the tax revenue to increase by 1 unit, whatever those units are (I didn’t look them up). Almost all of the probability mass is above zero, so even if the quantitative effect value is somewhat larger or stronger, we can be reasonably confident that increasing the tax rate will increase the tax revenue by some amount.\n\n\n7.2.12 7H2\nThis question tells us that there is an outlier, but based on the previous Pareto \\(k\\) values that I already glanced at (not all listed out here), that seems to be correct. Point #12 is identified as incredibly influential in all three models (along with point 1 for the quadratic model, giving us another reason to trust that model less than the linear model regardless of what the WAIC tells us). Because of the outlier, we’ll refit the linear model using a Student’s \\(t\\) regression and see what happens.\n\nm4 &lt;- rethinking::quap(\n    alist(\n        y ~ dstudent(2, mu, sigma),\n        mu &lt;- b0 + b1 * x,\n        b0 ~ dnorm(0, 2),\n        b1 ~ dnorm(0, 2),\n        sigma ~ dexp(0.5)\n    ),\n    data = d\n)\n\nplot(\n    x = d$x * x_s + x_m,\n    y = d$y,\n    xlab = \"Tax rate\",\n    ylab = \"Tax revenue\"\n)\n\nm4_post &lt;- rethinking::link(m4, data = x_vec) |&gt; colMeans()\nlines(\n    x = x_tf, y = m1_post,\n    lty = 1, col = \"black\",\n    lwd = 2\n)\nlines(\n    x = x_tf, y = m4_post,\n    lty = 2, col = \"firebrick3\",\n    lwd = 2\n)\nlegend(\n    x = \"topleft\",\n    legend = c(\"Gaussian\", \"T(2 d.f.)\"),\n    lwd = c(2, 2),\n    lty = c(1, 2),\n    col = c(\"black\", \"firebrick3\")\n)\n\n\n\n\n\n\n\n\nInterestingly, we can see that if we use a Student’s \\(t\\) likelihood which is less susceptible to the influence of outliers, the line is flatter, reflecting the fact that the outlier was dragging the slope upwards. Excluding this outlier, which has an unusually high tax revenue compared to its tax rate, gives us a line that matches the rest of the points better.\nOf course, this change is not too dramatic. Let’s look at the summary.\n\nsummary(m4)\n\n           mean        sd       5.5%    94.5%\nb0    2.8746595 0.1931870 2.56590941 3.183410\nb1    0.3869709 0.2346150 0.01201094 0.761931\nsigma 0.8011000 0.1689198 0.53113354 1.071067\n\n\nThe change in the coefficient is within the standard error of that coefficient from the first model, so as long as we don’t fall victim to pointeffectism ( AKA point-estimate-is-the-effect syndrome), this doesn’t change our conclusions really at all. This is because while that point is an outlier, it is not incredibly influential. I.e., in standard frequentist linear regression theory, the leverage of that point would not be exceptionally high, indicating that it does not have that much power to influence the coefficient estimates on its own. An outlier with a more extreme \\(x\\) value would influence the estimates more.\n\n\n7.2.13 7H3\nThe first thing we need to do for this problem is type in the data.\n\nbirds &lt;- matrix(\n    c(0.2, 0.8, 0.05, 0.2, 0.1, 0.15, 0.2, 0.05, 0.7, 0.2, 0.025, 0.05,\n        0.2, 0.025, 0.05),\n    ncol = 5,\n    nrow = 3,\n    dimnames = list(\n        paste(\"Island\", 1:3),\n        paste(\"Species\", LETTERS[1:5])\n    )\n)\nbirds\n\n         Species A Species B Species C Species D Species E\nIsland 1      0.20      0.20      0.20     0.200     0.200\nIsland 2      0.80      0.10      0.05     0.025     0.025\nIsland 3      0.05      0.15      0.70     0.050     0.050\n\n\nNext, we want to compute the entropy of each island’s bird distribution.\n\nh &lt;- apply(birds, 1, \\(p) -sum(p * log(p)))\nround(h, 2)\n\nIsland 1 Island 2 Island 3 \n    1.61     0.74     0.98 \n\n\nIsland 1 has the highest entropy, followed by Island 3, and finally Island 2. This is because Island 1 has an even distribution of birds, which gives us the maximum possible entropy of an island with five bird species. We can think of this intuitively as a measurement of our uncertainty in which type of bird we will see. If we see a random bird on Island 1, there is an equally likely chance for it to be any of the species, so any guess we make should be quite uncertain.\nHowever, on Island 2 one scecies represents 80% of birds on the island, and similar on Island 3, one species represents 70% of birds on the island. So if we saw a random bird on Island 3, we could make a decent guess about the species, and an even stronger guess on Island 2. So the decreasing entropies reflect our lowered uncertainty about the type of bird we guess we might see.\nNext we want to compute the KL divergence between each pair of islands.\n\nkl &lt;- function(p, q) {return(sum(p * (log(p) - log(q))))}\n\npairs &lt;-\n    tidyr::expand_grid(p = 1:3, q = 1:3) |&gt;\n    dplyr::filter(p != q) |&gt;\n    dplyr::mutate(\n        div = purrr::map2_dbl(p, q, \\(x, y) kl(birds[x, ], birds[y, ]))\n    )\n\npairs\n\n# A tibble: 6 × 3\n      p     q   div\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     2 0.970\n2     1     3 0.639\n3     2     1 0.866\n4     2     3 2.01 \n5     3     1 0.626\n6     3     2 1.84 \n\n\nThese are the KL divergence values if we used the distribution of island \\(p\\) to predict the distribution of island \\(q\\). We see that it’s very difficult to predict island 2 from island 3 and vice versa. Predictions using Island 1 are always much better, regardless of the direction. However, it is much easier to predict Island 3 from Island 1 and vice versa, then trying to predict Island 1 using Island 2 (or vice versa). This is because Island 1 has the maximum entropy, so reflects the maximum amount of uncertainty in our predictions – therefore using Island 1 is the safest way to make a prediction in any other island, because we are expressing the highest amount of uncertainty that we can in this situation.\nIsland 3 and 1 are more compatible in predictions than Island 2 and 1 because the difference in entropy between Islands 1 and 3 is smaller than between Islands 1 and 2. Islands 2 and 3 are almost completely dominated by 2 bird species each, but the dominant species are different between the islands, so using one to predict the other is very wrong. That is, we are using one distribution with extreme values to predict another distribution with different extreme values, so our predictions are more wrong on average.\n\n\n7.2.14 7H4\nOK, the first thing we need to do is type in all that code from the book to recreate the models, so I’ll do that without comments.\n\nd &lt;- sim_happiness(seed = 1977, N_years = 1000)\nd2 &lt;- d[d$age &gt; 17, ]\nd2$A &lt;- (d2$age - 18) / (65 - 18)\nd2$mid &lt;- d2$married + 1\n\nm6.9 &lt;- quap(\n    alist(\n        happiness ~ dnorm(mu, sigma),\n        mu &lt;- a[mid] + bA * A,\n        a[mid] ~ dnorm(0, 1),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = d2\n)\n\nm6.10 &lt;- quap(\n    alist(\n        happiness ~ dnorm(mu, sigma),\n        mu &lt;- a + bA * A,\n        a ~ dnorm(0, 1),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = d2\n)\n\nSince McElreath says that WAIC and PSIS produce identical results here, we’ll go ahead and use WAIC since it’s like one second faster.\n\nrethinking::compare(m6.9, m6.10)\n\n          WAIC       SE    dWAIC      dSE    pWAIC       weight\nm6.9  2713.971 37.54465   0.0000       NA 3.738532 1.000000e+00\nm6.10 3101.906 27.74379 387.9347 35.40032 2.340445 5.768312e-85\n\n\nWe can see that the WAIC for m6.9 is much lower than the WAIC for m6.10, even if we consider the magnitude of the standard errors – the WAICs are multiple standard errors apart, so m6.9 should do a better job at generating predictions than m6.10. Let’s look at the model parameters.\n\nprecis(m6.9, depth = 2)\n\n            mean         sd       5.5%      94.5%\na[1]  -0.2350877 0.06348986 -0.3365568 -0.1336186\na[2]   1.2585517 0.08495989  1.1227694  1.3943340\nbA    -0.7490274 0.11320112 -0.9299447 -0.5681102\nsigma  0.9897080 0.02255800  0.9536559  1.0257600\n\nprecis(m6.10)\n\n               mean         sd       5.5%     94.5%\na      1.649248e-07 0.07675015 -0.1226614 0.1226617\nbA    -2.728620e-07 0.13225976 -0.2113769 0.2113764\nsigma  1.213188e+00 0.02766080  1.1689803 1.2573949\n\n\nOf course, we know from the previous chapter than m6.9 is conditioning on a collider! And m6.10 produces the correct causal inference. However, we have to remember the crucial fact that colliders and confounders contain information. As McElreath says earlier in this chapter, “highly confounded models can still make good predictions, at least in the short term.” So the model that makes the wrong causal conclusion has better predictive accuracy than the correct model, but this should not surprise us too much – this is why building a causal model is so important.\n\n\n7.2.15 7H5\nFor this exercise, we’ll go back to the foxes data. We have five models that we need to fit, using the fox weight as the outcome. I’ll go ahead and fit those models in the order indicated in the question. I did the same data processing as in the previous chapter, taking the log of the outcome and then standardizing all of the variables.\n\nset.seed(987865)\ndata(foxes)\nf2 &lt;-\n    foxes |&gt;\n    dplyr::transmute(\n        A = area,\n        F = avgfood,\n        G = groupsize,\n        W = log(weight)\n    ) |&gt;\n    as.list() |&gt;\n    lapply(FUN = rethinking::standardize)\n\nmf.1 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bF * F + bG * G + bA * A,\n        b0 ~ dnorm(0, 2),\n        bF ~ dnorm(0, 2),\n        bG ~ dnorm(0, 2),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.2 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bF * F + bG * G,\n        b0 ~ dnorm(0, 2),\n        bF ~ dnorm(0, 2),\n        bG ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.3 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bG * G + bA * A,\n        b0 ~ dnorm(0, 2),\n        bG ~ dnorm(0, 2),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.4 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bF * F,\n        b0 ~ dnorm(0, 2),\n        bF ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nmf.5 &lt;- rethinking::quap(\n    alist(\n        W ~ dnorm(mu, sigma),\n        mu &lt;- b0 + bA * A,\n        b0 ~ dnorm(0, 2),\n        bA ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = f2\n)\n\nrethinking::compare(mf.1, mf.2, mf.3, mf.4, mf.5)\n\n         WAIC       SE    dWAIC      dSE    pWAIC      weight\nmf.3 324.7503 17.91089 0.000000       NA 4.320544 0.481904455\nmf.1 325.8046 19.33267 1.054305 3.831498 6.108581 0.284460254\nmf.2 326.2658 19.75153 1.515478 7.641027 4.583343 0.225880692\nmf.4 334.3019 16.42091 9.551600 7.090994 2.946806 0.004063099\nmf.5 334.4937 16.14821 9.743426 6.923563 3.212804 0.003691501\n\n\nOk, so the main thing that I can see is that Model 1, 2, and 3 are all similar, and model 4 and 5 are similar. But following McElreath’s notion to exam the standard error of the differences, we can see that all the models are actually not too different. However, we can see that model 1 and model 3 are quite similar, whereas the others have larger standard errors.\nIf we look at the plot we can definitely see two groups of models.\n\nplot(rethinking::compare(mf.1, mf.2, mf.3, mf.4, mf.5))\n\n\n\n\n\n\n\n\nIf we look back at the DAG, maybe we can understand why there are two groups of models.\n\nfox_dag &lt;-\n    dagitty::dagitty(\n        \"dag {\n        A -&gt; F -&gt; G -&gt; W\n        F -&gt; W\n        }\"\n    )\ndagitty::coordinates(fox_dag) &lt;-\n    list(\n        x = c(A = 2, F = 1, G = 3, W = 2),\n        y = c(A = 1, F = 2, G = 2, W = 3)\n    )\nplot(fox_dag)\n\n\n\n\n\n\n\n\nSo the groups that are similar are: 1. (avgfood, groupsize, area) and (avgfood, groupsize) and (groupsize, area); 2. (avgfood only) and (area only).\nSo the main difference between the models is the inclusion of the groupsize variable, \\(G\\) in the DAG. We can see that \\(G\\) is a mediator of the relatioship between \\(F\\) and \\(W\\) here.\nWhen we fit only avgfood or only area, it makes sense that the two models are the same – the effect of area is entirely through its effect on avgfood, so those two models are essentially finding the same effect.\nHowever, when we include the mediator we get slightly different results. Interestingly, we’re still only capturing one causal effect, because the effect of everything else comes from area. But it seems that the predictive model is better when we control for groupsize – why would this be, if we are still only capturing descendants of area? I think that this is because, as we also know, conditioning on a variable which is the ancestor of the outcome can lead to increased precision in our estimates. So even though we aren’t getting any “new” signal here, and all of the models with groupsize in them perform similarly, we get slightly more efficient estimates, which can increase our predictive accuracy on average. It may also be the case that estimating the effect of the groupsize is a statistical issue – this variable is integer valued, and we’re modeling it like a continuous value, but because we only have a discrete set of actual observations, this may reduce the precision of our estimate, since there’s a lot of “space” in the groupsize-axis that isn’t covered by any measurements.\nWe can also see that model 3 had the best WAIC overall, and it specifically includes the two variables which are direct ancestors of the treatment, so maybe that supports my idea or maybe it is a coincidence, I’m not too sure.\nAnyways, I think this is mostly a statistical phenomenon. There are no confounders or colliders here, everything is just direct or indirect effects of the effect of area on weight.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "cp8.html#chapter-notes",
    "href": "cp8.html#chapter-notes",
    "title": "8  Conditional Manatees",
    "section": "8.1 Chapter notes",
    "text": "8.1 Chapter notes\n\nBullet holes in bombers and propeller scars on manatees – both are conditional on survival. This is the motivating example for the chapter.\nAn interaction is a statistical method for modeling interdependence between two features of a model.\nUsing an interaction term in a model is nearly always better than fitting stratified models.\nIn Bayesian models, it’s better to use an index-coding approach and have parameters vary by the level of a categorical variable, rather than using an indicator-coding approach, which makes assigning priors difficult.\nAn interaction is also just a slope which is conditional on another effect – the value of one variable modifies the effect of the other.\nLinear interactions are symmetrical. If variable \\(x\\) interacts with variable \\(y\\), then \\(y\\) interacts with \\(x\\). “There is just no way to specify a simple, linear interaction in which you can say the effect of some variable \\(x\\) depends on \\(z\\), but the effect of \\(z\\) does not depend upon \\(x\\).”\nContinuous interactions are harder to think about as conditional slopes, because we would need an uncountably infinite number of categories. Instead, we can think about interactions as nested linear models.\n\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\gamma_{W,i}W_i + \\beta_S S_i \\\\\n\\gamma_{W,i} &= \\beta_{W} + \\beta_{WS} S_i\n\\end{aligned}\n\\]\n\nWe could include nested terms for both variables, but the resultant model has unidentifiable parameters – in the final term below, only the sum \\((\\beta_{WS} + \\beta_{SW})\\) can be estimated.\n\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\gamma_{W,i}W_i + \\gamma_{S,i} S_i \\\\\n\\gamma_{W,i} &= \\beta_{W} + \\beta_{WS} S_i \\\\\n\\gamma_{S_i} &= \\beta_{S} + \\beta_{SW} W_i \\\\\n\\therefore \\mu_i &= \\alpha + \\left(\\beta_{W} + \\beta_{WS} S_i\\right)W_i + \\left(\\beta_{S} + \\beta_{SW} W_i\\right) S_i \\\\\n&= \\alpha + \\beta_W W_i + \\beta_S S_i + (\\beta_{WS} + \\beta_{SW}) W_iS_i\n\\end{aligned}\n\\]\n\nThe best way to understand interactions is to plot the predictions at multiple levels of the interacting variables.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "cp8.html#exercises",
    "href": "cp8.html#exercises",
    "title": "8  Conditional Manatees",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\n\n8.2.1 8E1\n\nBread dough rises because of yeast and temperature. Yeast amount and temperature interact to determine how much the bread dough rises.\nEducation and parents’ education could interact to determine higher income. While people with more education have higher salaries on average, people who are educated and also have educated parents are likely to have even higher average salaries at the same level of individual education.\nGasoline and pressing the accelerator make the car go. If you never press the accelerator, a full tank won’t do anything.\n\n\n\n8.2.2 8E2\nOnly statement one (Caramelizing onions requires cooking over a low heat and making sure the onions don’t dry out) involves an interaction. Both things must be true simultaneously, whereas the effects are independent of each other in the other statements.\n\n\n8.2.3 8E3\nOf course all of these models only make sense if we have a correct way to quantify those variables.\n\n\\(\\text{onion caramelization} = \\alpha + \\beta_1 \\cdot \\text{temperature} + \\beta_2 \\cdot \\text{moisture} + \\gamma_{12} \\cdot \\text{temperature} \\cdot \\text{moisture}\\)\n\\(\\text{car speed} = \\alpha + \\beta_2 \\cdot \\text{number of cylinders} + \\beta_2 \\cdot \\text{fuel injector quality}\\)\n\\(\\text{political beliefs} = \\alpha + \\beta_1 \\cdot \\text{parental beliefs} + \\beta_2 \\cdot \\text{friend beliefs}\\)\n\\(\\text{intelligence} = \\alpha + \\beta_1 \\cdot \\text{sociality} + \\beta_2 \\cdot \\text{manipulable appendages}.\\)\n\n\n\n8.2.4 8M1\nIn the tulips example, we saw that water and shade levels interact to affect tulip blooms. Tulips need both water and shade to produce blooms; at a low-light level, the effect of water decreases because no amount of water can replace the lost light. Similarly, if plants have no water, an adequate amount of sunlight will not produce blooms and might even become harmful.\nIf the hot temperature prevents blooms all together, then the hot temperature would modify the effect of shade, water, and their interaction to all become zero – no amount of shade or water can allow for blooms, and their interaction does not help in this context either.\n\n\n8.2.5 8M2\nThe linear model for the tulips example without heat was \\[\n\\mu_i = \\alpha + \\beta_W W_i + \\beta_S S_i + \\gamma_{SW} S_iW_i.\n\\]\nWe can make all of those terms dependent on \\(H_i\\), the heat treatment, in order to accomplish this.\n\\[\n\\mu_i = \\alpha_{H[i]} + \\beta^{W}_{H[i]} + \\beta^S_{H[i]} + \\gamma^{SW}_{H[i]} S_iW_i.\n\\]\nNow it is possible for these effects to all be zero (or much smaller) if \\(H[i] = 1\\), and have their normal values if \\(H[i] = 0\\). Another way to write this model could be something like \\[\n\\begin{aligned}\n\\mu_i &= \\lambda_i (1 - H_i) \\\\\n\\lambda_i &= \\alpha + \\beta_W W_i + \\beta_S S_i + \\gamma_{SW} S_iW_i\n\\end{aligned}\n\\] where \\(H_i\\) again takes on values of \\(0\\) (cold) and \\(1\\) (hot).\n\n\n8.2.6 8M3\nWe cannot create a data set where the raven population and wolf population have a linear statistical interaction, because a linear statistical interaction has at least two predictors. Here we only have an outcome (the raven population size) and a predictor (the wolf population size). This is more of an example of a differential equations type problem than a statistical interaction. In this model, the raven population size would have to vary with the wolf population size, and we do not know about the functional form of this effect, so an appropriate model would be something like\n\\[\n\\frac{dR}{dt} = f\\left(W(t)\\right),\n\\] where \\(f\\) is a function that takes the wolf population size at time \\(t\\) as an input, and returns the change in the raven population before the next time point.\n\n\n8.2.7 8M4\nWe’ll use the sample model for the tulip blooms without heat from the earlier exercise. The priors used in the chapter were\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Normal}(0, 0.25) \\\\\n\\beta_S &\\sim \\text{Normal}(0, 0.25) \\\\\n\\gamma_{SW} &\\sim \\text{Normal}(0, 0.25) \\\\\n\\end{aligned}\n\\]\nWe want to use new priors that constrain the effect of water to be positive and the effect of shade to be negative. At this point in the book, the distribution we learned about that has to be positive is lognormal, and we can force the effect of shade to be negative by taking the additive inverse of a lognormal prior. Since we know that having more water increases the effect of light (because if a tulip has plenty of water, getting enough sunshine is the new limiting factor on the blooms), we know that having more water should decrease the effect of shade, so we’ll make the interaction negative as well. Lognormal priors can be hard to calibrate, so we’ll adjust the parameters until the prior predictive simulation looks nice. The priors we’ll use are as follows.\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + \\beta_W W_i - \\beta_S S_i - \\gamma_{SW} S_iW_i \\\\\n\\alpha &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\beta_S &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\gamma_{SW} &\\sim \\text{Log-normal}(-3, 1)\n\\end{aligned}\n\\]\n\nset.seed(370)\n\n# Load the data\ndata(tulips)\nd &lt;- tulips\nd$blooms_std &lt;- d$blooms / max(d$blooms)\nd$water_cent &lt;- d$water - mean(d$water)\nd$shade_cent &lt;- d$shade - mean(d$shade)\n\n# Fit the model and extract the prior samples\nm_8m4 &lt;- rethinking::quap(\n    alist(\n        blooms_std ~ dnorm(mu, sigma),\n        mu &lt;- a + bw * water_cent - bs * shade_cent - bws * water_cent * shade_cent,\n        a ~ dnorm(0.5, 0.25),\n        bw  ~ dlnorm(-3, 1),\n        bs  ~ dlnorm(-3, 1),\n        bws ~ dlnorm(-3, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nprior &lt;- rethinking::extract.prior(m_8m4)\n\n# Plot the prpd\npar(mfrow = c(1, 3))\nfor (s in -1:1) {\n    idx &lt;- which(d$shade_cent == s)\n    plot(\n        d$water_cent[idx],\n        d$blooms_std[idx],\n        xlim = c(-1, 1),\n        ylim = c(-0.5, 1.5),\n        xlab = \"water\",\n        ylab = \"blooms\",\n        pch = 16,\n        col = rethinking::rangi2\n    )\n    abline(h = 0, lty = 2)\n    abline(h = 1, lty = 2)\n    mtext(paste0(\"shade = \", s))\n    mu &lt;- rethinking::link(\n        m_8m4,\n        data = data.frame(shade_cent = s, water_cent = -1:1),\n        post = prior\n    )\n    for (i in 1:20) lines(-1:1, mu[i, ], col = col.alpha(\"black\", 0.3))\n}\n\n\n\n\n\n\n\n\nI did a few different simulations and ultimately ended up with the prior simulation shown here. The slope priors are regularizing and skeptical, so we think that a smaller effect is more likely a priori – if the effects are large, the data can demonstrate that for us.\n\n\n8.2.8 8H1\nNow we want to add the bed variable to the tulips example, which we’ll denote with \\(B_i\\). We only want to include the bed effect as a main effect, which means we need to have a different intercept for each bed – so our model will assume that each bed can start at a different baseline, but the effects of water, shade, and their interaction, are homogeneous across the beds. This is probably an OK assumption in the context of a controlled greenhouse setting.\nThe model will be as follows.\n\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal} \\left(\\mu_i, \\sigma\\right) \\\\\n\\mu_i &= \\alpha_{B[i]} + \\beta_W W_i - \\beta_S S_i - \\gamma_{SW} S_iW_i \\\\\n\\alpha_{B[i]} &\\sim \\text{Normal}(0.5, 0.25) \\\\\n\\beta_W &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\beta_S &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\gamma_{SW} &\\sim \\text{Log-normal}(-3, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{aligned}\n\\]\nThat is, we’ll use the same constrained, regularizing priors, as we did for the previous problem, but we’ll have a separate intercept for each bed. In reality, it would probably be good to have the intercept parameters be correlated as well, but we haven’t gone over that in the book yet. Let’s go ahead and fit the model.\n\nset.seed(370)\n# For the index coding to work, we need a numeric version of the beds.\nd$b &lt;- as.integer(d$bed)\n\n# Fit the model\nm_8h1 &lt;- rethinking::quap(\n    alist(\n        blooms_std ~ dnorm(mu, sigma),\n        mu &lt;- a[b] + bw * water_cent - bs * shade_cent -\n            bws * water_cent * shade_cent,\n        a[b] ~ dnorm(0.5, 0.25),\n        bw  ~ dlnorm(-3, 1),\n        bs  ~ dlnorm(-3, 1),\n        bws ~ dlnorm(-3, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_8h1, depth = 2)\n\n           mean         sd       5.5%     94.5%\na[1]  0.2733479 0.03603110 0.21576327 0.3309326\na[2]  0.3964371 0.03601250 0.33888214 0.4539920\na[3]  0.4091452 0.03601137 0.35159204 0.4666983\nbw    0.2017188 0.02612291 0.15996936 0.2434683\nbs    0.1039759 0.02652450 0.06158467 0.1463672\nbws   0.1312346 0.03272451 0.07893448 0.1835347\nsigma 0.1091610 0.01511214 0.08500884 0.1333131\n\n\nFrom the precis, we can see that the first bed (bed 1, with parameter a[1]) has a lower intercept than the other two beds – maybe this bed is next to a drafty space, is the first bed in the water connection and gets the pipe sludge, or just had less bloomds for some reason. But after we account for the different baselines between beds, the estimates of the parameter effects are similar to the last model, so this should just improve the accuracy of our model predictions for the first bed. We can plot the predictions to see.\n\n# Plot the prpd\npar(mfrow = c(1, 3))\n\ncols &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\")\n\nfor (s in -1:1) {\n    idx &lt;- which(d$shade_cent == s)\n    plot(\n        d$water_cent[idx],\n        d$blooms_std[idx],\n        xlim = c(-1, 1),\n        ylim = c(-0.5, 1.5),\n        xlab = \"water\",\n        ylab = \"blooms\",\n        pch = 16,\n        col = cols[d$b[idx]]\n    )\n    abline(h = 0, lty = 2)\n    abline(h = 1, lty = 2)\n    mtext(paste0(\"shade = \", s))\n    \n    for (bd in 1:3) {\n        mu &lt;- rethinking::link(\n            m_8h1,\n            data = data.frame(shade_cent = s, water_cent = -1:1, b = bd)\n        )\n        for (i in 1:20) {\n            lines(\n                -1:1, mu[i, ],\n                col = col.alpha(cols[bd], 0.3)\n            )\n        }\n    }\n}\n\nlegend(\"topright\", c(\"Bed a\", \"Bed b\", \"Bed c\"), col = cols, lty = 1)\n\n\n\n\n\n\n\n\nWhile it’s actually quite difficult to make statistical conclusions without multiple replicates (here we have only one measurement per bed per treatment), we can see the clear difference between bed a and the other two beds in the model predictions. However, it also appears that our model predictions may not capture the true effect, as from the observed data it seems plausible that the effects of water and shade vary across beds. We would need actual replicates to be more certain of that, though.\n\n\n8.2.9 8H2\nNow we can compare the models with and without bed using WAIC.\n\nrethinking::compare(m_8m4, m_8h1)\n\n           WAIC        SE    dWAIC      dSE     pWAIC    weight\nm_8h1 -22.20890  9.935022 0.000000       NA 10.099017 0.7754177\nm_8m4 -19.73058 10.459586 2.478319 8.389834  7.390765 0.2245823\n\n\nWe see that the WAIC is smaller for the model with bed included, although the difference is small. This implies that adding the bed variable as a main effect increases the accuracy of our posterior predictions, although the improvement is not spectacular. As we saw by looking at summaries of the posterior distribution in the previous exercise, the difference in the estimated intercept for bed A vs. bed b and bed c, without any major changes in the estimates of the slope parameters, should account for this difference.\n\n\n8.2.10 8H3\nFor this question, we’ll focus on the ruggedness data.\n\ndata(\"rugged\")\n# Repeating data processing steps from the book\nd &lt;- rugged\nd$log_gdp &lt;- log(d$rgdppc_2000)\ndd &lt;- d[complete.cases(d$rgdppc_2000), ]\ndd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp)\ndd$rugged_std &lt;- dd$rugged / max(dd$rugged)\n\ndd$cid &lt;- ifelse(dd$cont_africa == 1, 1, 2)\n\nNow we need to recreate model m8.5 from the chapter. Well, at least that’s what the book says to do, but model m8.5 is about the tulips example, so we’ll recreate m8.3 instead.\n\nm8.3 &lt;- rethinking::quap(\n    alist(\n        log_gdp_std ~ dnorm(mu, sigma),\n        mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),\n        a[cid] ~ dnorm(1, 0.1),\n        b[cid] ~ dnorm(0, 0.3),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\nrethinking::precis(m8.3, depth = 2)\n\n            mean          sd        5.5%       94.5%\na[1]   0.8865629 0.015675157  0.86151094  0.91161480\na[2]   1.0505698 0.009936261  1.03468975  1.06644988\nb[1]   0.1325055 0.074201996  0.01391637  0.25109461\nb[2]  -0.1425764 0.054747543 -0.23007353 -0.05507924\nsigma  0.1094903 0.005934777  0.10000535  0.11897519\n\n\nWe got similar results to what’s in the book, which is good. Now we want to examine the model with PSIS to determine if the Seychelles are influential on the estimation of parameters for the Africa group.\n\nset.seed(370)\nm8.3_psis &lt;- rethinking::PSIS(m8.3, pointwise = TRUE, n = 20000)\nrownames(m8.3_psis) &lt;- dd$isocode\npsis_sort &lt;- m8.3_psis[order(m8.3_psis$k, decreasing = TRUE), ]\npsis_sort |&gt; head()\n\n          PSIS       lppd    penalty  std_err         k\nLSO -1.1417140  0.5708570 0.31929940 15.27576 0.4467221\nSYC  1.3274852 -0.6637426 0.63043059 15.27576 0.3947676\nCHE  2.8274338 -1.4137169 0.46756727 15.27576 0.3148571\nTJK  0.4998540 -0.2499270 0.30731105 15.27576 0.2365403\nGNQ  3.3713051 -1.6856526 0.21035122 15.27576 0.2249513\nMUS  0.8394823 -0.4197411 0.08626176 15.27576 0.1688589\n\n\nThe most influential country on the model fit, judging by the Pareto \\(k\\) values, are Lesotho and the Seychelles, which are both highly rugged nations in Africa.\n\npar(mfrow = c(1, 1))\ndd_sorted &lt;- dd[order(m8.3_psis$k, decreasing = TRUE), ]\nplot(\n    dd_sorted$rugged_std, psis_sort$k,\n    xlab = \"Ruggedness as prop. of maximum\",\n    ylab = \"PSIS Pareto k value\"\n)\n\n\n\n\n\n\n\n\nWe can see that highly rugged nations have the largest Pareto \\(k\\) values, indicating that they are the most influential variables. We also know that these values have a high leverage in a linear regression model, so that makes sense.\nNow that we know these nations with high ruggedness are having an oversized effect on the estimated trend, we can try to use robust regression to lower their influence. We’ll use the same model, but with a Student’s \\(t\\) distribution likelihood (with 2 d.f.) instead of a Normal likelihood. Personally I prefer 3 degrees of freedom (the variance of the distribution is infinite if the d.f. is not larger than 2, which is a prior belief that never makes sense in a physical context to me), but for now I’ll do what the textbook says.\n\nm8.3_r &lt;- rethinking::quap(\n    alist(\n        log_gdp_std ~ dstudent(nu = 2, mu, sigma),\n        mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215),\n        a[cid] ~ dnorm(1, 0.1),\n        b[cid] ~ dnorm(0, 0.3),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\nrethinking::precis(m8.3_r, depth = 2)\n\n             mean         sd        5.5%       94.5%\na[1]   0.86259888 0.01614146  0.83680170  0.88839606\na[2]   1.04577255 0.01097134  1.02823823  1.06330688\nb[1]   0.11241664 0.07503557 -0.00750470  0.23233797\nb[2]  -0.21378054 0.06352620 -0.31530767 -0.11225341\nsigma  0.08451473 0.00673094  0.07375738  0.09527207\n\n\nWe can see by comparing the two model summaries that a few of the parameters are slightly different. Let’s now compare the models using PSIS.\n\nset.seed(12312)\nrethinking::compare(m8.3, m8.3_r, func = PSIS, n = 20000)\n\n            PSIS       SE    dPSIS      dSE    pPSIS       weight\nm8.3   -258.8088 15.27974  0.00000       NA 5.318484 1.000000e+00\nm8.3_r -221.8080 18.11559 37.00089 5.884289 5.760308 9.233328e-09\n\n\nHere, we can see that the non-robust model actually appears to be giving us worse predictions than the non-robust model. However, we know that PSIS is just a measure of predictive performance, so it’s possible that our robust model is still a better conceptual model that provides more accurate inferences at the cost of appearing to underfit the data. Since the two models have every similar numbers of parameters, predictive accuracy criteria are likely to be more sensitive to this kind of “underfitting”, when we actually know outside of the statistics world that we’re reducing the impact of outlying values.\nAnyways, we can also look at the individual pareto \\(k\\) values for the new model.\n\nset.seed(370)\nm8.3r_psis &lt;- rethinking::PSIS(m8.3_r, pointwise = TRUE, n = 20000)\nrownames(m8.3r_psis) &lt;- dd$isocode\npsis_sort_r &lt;- m8.3r_psis[order(m8.3r_psis$k, decreasing = TRUE), ]\npsis_sort_r |&gt; head()\n\n         PSIS      lppd     penalty std_err          k\nEST -2.787885 1.3939424 0.007899841 18.1216 0.13309736\nLSO -1.553968 0.7769838 0.264018936 18.1216 0.11825246\nALB -2.720143 1.3600717 0.013599882 18.1216 0.11215073\nATG -2.777583 1.3887917 0.008395917 18.1216 0.10169758\nUGA -2.744650 1.3723251 0.010101504 18.1216 0.09032803\nBEN -2.552244 1.2761221 0.019180005 18.1216 0.08104976\n\n\nNow we can see that the pareto \\(k\\) values are all much lower. While Lesotho still appears in the top 6, it is overall much less influential, and Seychelles no longer appears in the top 6.\n\n\n8.2.11 8H4\nFor this problem, we’ll use the nettle data to examine the hypothesis that higher food security leads to a higher language diversity in a region.\nFirst we need to construct the outcome variable.\n\ndata(nettle)\nd &lt;- nettle\nd$lang.per.cap &lt;- d$num.lang / d$k.pop\n\n# The log of this will be our actual outcome variable\nd$log.lang.per.cap &lt;- log10(d$lang.per.cap)\n\n# Center the outcome\nybar &lt;- mean(d$log.lang.per.cap)\nd$std.log.lang.per.cap &lt;- d$log.lang.per.cap - ybar\n\n# We also need the log of the area\nd$log.area &lt;- log10(d$area)\n\nSince I don’t really know anything about this problem other than what the textbook tells me, I’ll follow the specified steps. The effects we want to evaluate here are the effects of mean.growing.season, which we’ll call \\(M\\), and sd.growing.season, on our model. We also need to consider \\(A\\), the log of area, as a potential cause. I’m not sure how we would work in the number of weather stations in our model, so for now we’ll leave that alone – although there is a noticeable trend in the data that as the number of measurement stations increased, so did the SD of the growing season length. So in a real academic paper, we would definitely need to think about how to model that.\nFirst let’s look at the bivariate relationship between each of these values and the outcome.\n\nlayout(matrix(c(1, 2, 3), nrow = 1))\nplot(\n    d$mean.growing.season, d$log.lang.per.cap,\n    xlab = \"Mean length of growing season (months)\",\n    ylab = \"log10 number of langauges per capita\"\n)\nplot(\n    d$sd.growing.season, d$log.lang.per.cap,\n    xlab = \"Standard deviation of length of growing season (months)\",\n    ylab = \"\"\n)\nplot(\n    d$log.area, d$log.lang.per.cap,\n    xlab = \"log10 area of country (sq. km.)\",\n    ylab = \"\"\n)\n\n\n\n\n\n\n\n\nIn general, none of these trends looks particularly strong, although there appear to be some trends with the growing season variables.\nNow, let’s try to fit a simple model that models the log languages per capita based on the mean length of the growing season. We’ll use a normal likelihood since the outcome variable is on the log scale. Since we aren’t using a count model, which would naturally constrict the domain of the outcome variable, I also chose to center the outcome variable before modeling to make assigning a prior for the intercept feasible.\nFor the intercept, we’ll use a generic prior centered at 0 (the mean after standardization). For the effect of the mean growing season length, we’ll use a regularizing, skeptical prior centered around 0.\n\n# Set seed for all of our models in this section\nset.seed(370)\nm_mgs &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_mgsl * mean.growing.season,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_mgs)\n\n             mean         sd        5.5%      94.5%\na      -0.5310682 0.17466211 -0.81021195 -0.2519244\nb_mgsl  0.0754357 0.02267654  0.03919421  0.1116772\nsigma   0.6095826 0.04980052  0.52999173  0.6891734\n\n\nWe can see from the summary that there is a small prior effect of mean growing season length on the outcome. Now, let’s check whether the area should be a coefficient in this model as well.\n\nm_mgs_a &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_mgsl * mean.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_mgs_a)\n\n             mean         sd        5.5%      94.5%\na       0.6533186 0.83024907 -0.67357977 1.98021696\nb_mgsl  0.0629484 0.02394189  0.02468463 0.10121217\nb_area -0.1952263 0.13386781 -0.40917288 0.01872034\nsigma   0.6009410 0.04909953  0.52247048 0.67941155\n\n\nThe effect of the area variable is negative and a large amount of the density lies below zero, which suggests that the area could be a real effect that we need to control for. Controlling for the area size also affects out estimate of the effect of mean growing season length. Let’s compare the models via WAIC and see if the area improves posterior predictions.\n\nrethinking::compare(m_mgs, m_mgs_a)\n\n            WAIC       SE     dWAIC      dSE    pWAIC   weight\nm_mgs   144.5949 15.49001 0.0000000       NA 3.778308 0.539875\nm_mgs_a 144.9146 16.01579 0.3196787 3.673048 5.049032 0.460125\n\n\nThe WAICs are extremely similar, and the WAIC for the model without area is slightly better, so I don’t think we need area in this model. If we think about the problem casually, I don’t understand why the area would be a confounder or a collider in this situation, because the area doesn’t casually determine the mean growing season length in a country. However, larger countries should have more variation in the growing season length (since larger area overall means they can cover more areas of varying latitude). But, I think that a larger area should mean there is more room for multiple communities to exist and become isolated, so a larger country should also have more languages on average.\nSo I think the DAG should look something like this.\n\nlayout(c(1))\ndag &lt;- dagitty::dagitty(\n    'DAG {\n    \"languages\" &lt;- \"mean length\"\n    \"languages\" &lt;- \"SD length\" &lt;- \"area\"\n    \"languages\" &lt;- \"area\"\n    }'\n)\ncoordinates(dag) &lt;- list(\n    x = c(\"languages\" = 0, \"mean length\" = 1, \"SD length\" = -1, \"area\" = -1),\n    y = c(\"languages\" = 0, \"mean length\" = 0, \"SD length\" = 0.5, \"area\" = -0.5)\n)\nplot(dag)\n\n\n\n\n\n\n\n\nSo, in our final model we’ll need to include area anyways, so we might as well leave it in there for now.\nNext we want to examine the effect of the SD of growing season length on languages. The area variable is a confounder in this casual structure, so we need to include that as well to avoid getting a biased estimate.\n\nm_sgs_a &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_sgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\nrethinking::precis(m_sgs_a)\n\n              mean         sd       5.5%      94.5%\na       1.43691137 0.79286148  0.1697656 2.70405716\nb_sgsl -0.09330776 0.07997807 -0.2211282 0.03451264\nb_area -0.22761855 0.15173599 -0.4701220 0.01488487\nsigma   0.62213062 0.05082317  0.5409054 0.70335587\n\n\nWe see that the SD of growing season length does appear to have a negative effective on the overall number of languages. We cannot rule out entirely the lack of an effect (assuming our causal structure is correct and linear models are appropriate), but there is likely to be a negative effect of the SD of growing length on number of languages.\nSo far we’ve seen that an average longer growing season leads to more languages per capita, meaning that more food abundance leads to smaller, more isolated social groups and the development of more languages. However, higher variation in the growing season length leads to lower languages per capita, suggesting the need to form larger social networks for insurance against short growing seasons. In both models, we saw a negative effect of area, indicating that as a country becomes larger, the number of languages becomes smaller, which is the opposite of what I would have thought. Perhaps larger countries, on average, have shorter growing seasons? We can examine that effect quickly.\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    d$mean.growing.season, d$log.area,\n    xlab = \"Mean length of growing season (months)\",\n    ylab = \"log10 area of country (sq. km.)\"\n)\nplot(\n    d$sd.growing.season, d$log.area,\n    xlab = \"Standard deviation of length of growing season (months)\",\n    ylab = \"\"\n)\n\n\n\n\n\n\n\n\nInterestingly, we can see slight patterns in both trends. Larger countries seem to have slightly smaller average growing seasons, and more uncertainty in their growing seasons. I think the effect on the mean length of the growing season is quite small, and is likely to not be causal, although we could postulate that larger countries tend to cover more sparsely inhabited territory which has a shorter growing season. There is a definite trend in the standard deviation of the growing season though – it’s hard to tell if this is an effect of covering more latitude areas, or a function of a number of measuring systems. We would need a variable on the range of latitude covered by each country to disentangle those effects.\nAnyways, now we can fit the main model. We’ll fit two models that include all three variables. One will include just main effects, and the other will include an interaction between the effects of mean and SD of growing season length. Then we can compare those models.\n\nm_noint &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_mgsl * mean.growing.season +\n            b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_sgsl ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_noint)\n\n              mean         sd        5.5%       94.5%\na      -0.19725369 0.91289412 -1.65623482  1.26172743\nb_mgsl  0.07583973 0.02418582  0.03718613  0.11449334\nb_sgsl -0.15778988 0.07809425 -0.28259957 -0.03298018\nb_area -0.01221519 0.15896098 -0.26626554  0.24183516\nsigma   0.58530486 0.04782868  0.50886538  0.66174433\n\n\nAfter including both the mean and SD of growing season length in the model, the effect of area goes away. This suggests that the effect of area on the number of languages per capita is completely explained by the effect of the growing season – we have no evidence here for a direct causal effect of area, meaning that more room for expansion doesn’t say anything about the number of languages we expect to see, unless we know how habitable the land is first. In this model, we see a positive effect of the mean and negative effect of the SD as we expect. Now we can look at a possible interaction.\n\nm_int &lt;- rethinking::quap(\n    flist = alist(\n        std.log.lang.per.cap ~ dnorm(mu, sigma),\n        mu &lt;- a + b_sgsl * sd.growing.season + b_mgsl * mean.growing.season +\n            b_intr * mean.growing.season * sd.growing.season + b_area * log.area,\n        a ~ dnorm(0, 5),\n        b_mgsl ~ dnorm(0, 1),\n        b_sgsl ~ dnorm(0, 1),\n        b_intr ~ dnorm(0, 1),\n        b_area ~ dnorm(0, 1),\n        sigma ~ dexp(1)\n    ),\n    data = d\n)\n\nrethinking::precis(m_int)\n\n               mean         sd        5.5%       94.5%\na      -0.607666693 0.90104859 -2.04771636  0.83238298\nb_mgsl  0.128691044 0.03267891  0.07646384  0.18091825\nb_sgsl  0.179426213 0.16417241 -0.08295301  0.44180543\nb_intr -0.046560229 0.02012491 -0.07872372 -0.01439674\nb_area -0.007638564 0.15385114 -0.25352240  0.23824527\nsigma   0.565320615 0.04620557  0.49147519  0.63916604\n\n\nIt certainly looks like the results are different, which is qualitatively important to understand. But let’s first check the WAIC to get an idea of how much better our interaction model is doing.\n\nrethinking::compare(m_noint, m_int)\n\n            WAIC       SE    dWAIC      dSE    pWAIC    weight\nm_int   139.8943 16.19209 0.000000       NA 6.821918 0.8329129\nm_noint 143.1071 16.09151 3.212828 4.717323 6.073994 0.1670871\n\n\nOK, it’s only a bit better in terms of predictive accuracy, but the estimates are so different that we need to try and understand what’s going on here.\nWhen we include an interaction term, the effect of the standard deviation on its own largely goes away, and the effect of the negative is negative with almost all of the probability mass below 0. This suggests that by itself, the SD is not important for determining the number of languages – we need to know the mean first. I think we need to make a plot to really understand this effect.\n\n# Plot the prpd\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE))\n\nsd_vals &lt;- c(0, 1.5, 3, 4.5)\nsample_n &lt;- 500\nncolors &lt;- 10\ncols &lt;- viridisLite::plasma(ncolors)\nrank &lt;- as.factor(as.numeric(cut(d$sd.growing.season, ncolors)))\n\nfor (i in 1:length(sd_vals)) {\n    sd &lt;- sd_vals[i]\n    idx &lt;- which(dplyr::between(d$sd.growing.season, sd - 1.5, sd + 1.5))\n    plot(\n        #d$mean.growing.season[idx],\n        #d$log.lang.per.cap[idx],\n        NULL, NULL,\n        xlim = c(0, 12),\n        ylim = 10 ^ c(-4, 0),\n        xaxs = \"i\",\n        yaxs = \"i\",\n        xlab = \"Mean growing season length (months)\",\n        ylab = \"Number of languages per capita\",\n        pch = 16,\n        log = \"y\"\n        #col = cols[rank[idx]]\n    )\n    mtext(paste0(\"SD of growing season = \", sd))\n    \n    # Calculate the posterior values\n    post_data &lt;- expand.grid(\n        mean.growing.season = seq(0, 12, 0.1),\n        sd.growing.season = sd,\n        log.area = mean(d$log.area)\n    )\n    \n    mu &lt;- rethinking::link(m_int, post_data, n = sample_n)\n    \n    for (i in 1:sample_n) {\n        lines(\n            seq(0, 12, 0.1), 10 ^ (mu[i, ] + ybar),\n            col = col.alpha(\"black\", 0.1)\n        )\n    }\n}\n\n\n\n\n\n\n\n\nFrom the posterior predictions, we can understand the effect of the interaction a lot easier. The posterior predictions shown all use the average value of the log land area, but the main thing we want to understand here is the qualitative way the effect changes. When the SD value is small (the points shown in the top right panel are from 0 to 1.5), there is little variation in the length of the growing season, and the effect of the average length of the growing season on language diversity is positive. However, as the variation in growing season length increases, the effect becomes smaller and then negative, indicating that for highly variable areas, there is little effect of the average growing season on language diversity. For extremely variable areas, a long growing season may even lead to less language diversity, but we do not have enough data to say that conclusively.\n\n\n8.2.12 8H5\nFor this exercise, we’ll build a model using the Wines2012 dataset.\n\ndata(\"Wines2012\")\nd &lt;- Wines2012\n\nIt looks like the wines are scores out of 20, so normally I would recommend a binomial (or beta-binomial) model here. But we don’t know that for sure and we haven’t learned that yet, so we’ll standardize the scores and model the \\(z\\)-scores instead. This is a fairly interesting problem even with the small amount of data we have, and I imagine will make quite an interesting multilevel modeling problem later in the book.\nFor this question, we just need to include effects of the judge and of the wine.\n\ndd &lt;- data.frame(\n    y = rethinking::standardize(d$score),\n    j = rethinking::coerce_index(d$judge),\n    w = rethinking::coerce_index(d$wine),\n    wine_amer = d$wine.amer,\n    judge_amer = d$judge.amer,\n    red = as.integer(ifelse(d$flight == \"red\", 1, 0))\n)\ndplyr::glimpse(dd)\n\nRows: 180\nColumns: 6\n$ y          &lt;dbl&gt; -1.57660412, -0.45045832, -0.07507639, 0.30030555, -2.32736…\n$ j          &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,…\n$ w          &lt;int&gt; 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 1, 3, 5, 7, 9, 11, 13, 1…\n$ wine_amer  &lt;int&gt; 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,…\n$ judge_amer &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ red        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nFirst we can try to visualize the data. It’s a bit difficult because we have two categorical variables and one continuous (perfect two-way anova design), but we can make two plots to try and see what’s going on.\n\nd$j2 &lt;- rethinking::coerce_index(d$judge) |&gt; factor()\np1 &lt;-\n    ggplot(d) +\n    aes(x = wine, y = score) +\n    geom_point() +\n    facet_wrap(~j2) +\n    hgp::theme_ms() +\n    theme(axis.text = element_text(size = 8, angle = 90))\n\np2 &lt;-\n    ggplot(d) +\n    aes(x = j2, y = score) +\n    geom_point() +\n    facet_wrap(~wine) +\n    hgp::theme_ms() +\n    theme(axis.text = element_text(size = 10)) + labs(x = \"judge\")\n\ncowplot::plot_grid(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\nOf course with data like these we just kind of have to eyeball them and make qualitative guesses as to what might be going on. But In general we can see there are some nicer judges (5, 3), and some meaner judges (4, 9), and one judge that got meaner as they tried more wines (8). However, most of the wines look fairly similar with the exception of a few like C2 and I2 that appeared to get worse bad reviews. I guess if we were following Agresti’s Categorical Data Analysis the next thing to do would be to get marginal and conditional mean scores, but we don’t need to do that now, we can start fitting models (which does that in an easier way, more or less).\nWe’ll use (as usual) a normal likelihood – a \\(t\\)-likelihood might be better if some of our judges or wines give outlying scores, but for now we’ll ignore that possibility. Next we need to assign priors. Fortunately for us this is easy and I’ll just assign a typical normal prior, I’m not too sure why it really needs to be justified. So let’s go ahead and fit the first model.\n\nset.seed(12312)\nm_w1 &lt;- rethinking::quap(\n    flist = alist(\n        y ~ dnorm(mu, sigma),\n        mu &lt;- a_w[w] + a_j[j],\n        a_w[w] ~ dnorm(0, 2),\n        a_j[j] ~ dnorm(0, 2),\n        sigma ~ dexp(1)\n    ),\n    data = dd\n)\n\nlayout(matrix(c(1, 2), nrow = 1))\nrethinking::precis(m_w1, depth = 2, pars = paste0(\"a_w[\", 1:20, \"]\")) |&gt;\n    precis_plot()\nmtext(\"Wine parameters\")\nrethinking::precis(m_w1, depth = 2, pars = paste0(\"a_j[\", 1:9, \"]\")) |&gt;\n    precis_plot()\nmtext(\"Judge parameters\")\n\n\n\n\n\n\n\n\nLike we thought from looking at the plots, the judges appear to be more different from each other than the wines are. We probably also want to know about how the judges and wines interact, but since we don’t have any replicates, we can’t really get good answers for that question, all we can do is look at the score in each cell.\n\nggplot(d) +\n    aes(x = wine, y = judge, fill = score) +\n    geom_tile() +\n    scale_fill_viridis_c(breaks = c(7, 10, 15, 19), limits = c(7, 19.5)) +\n    hgp::theme_ms() +\n    guides(fill = guide_colorbar(barwidth = 15))\n\n\n\n\n\n\n\n\n\n\n8.2.13 8H6\nNow instead of looking at the variability across judges and across wines, we want to try and use the characteristics of the judges and wines to understand the scores. For this problem, we won’t include any interactions. Again, we’ll use standard priors, and the variables we’ll include as main effects are flight (whether the wine is red or white), wine.amer (if the wine was made in America), and judge.amer (whether the judge is American).\nFor whatever reason, the book says to use indicator coding for this problem and the next problem (coding interactions between two categorical variables is annoying somehow, I tried it and couldn’t figure it out). So we’ll do that.\n\nm_w2 &lt;-\n    rethinking::quap(\n        flist = alist(\n            y ~ dnorm(mu, sigma),\n            mu &lt;- a + a_wa * wine_amer + a_ja * judge_amer + a_red * red,\n            a ~ dnorm(0, 2),\n            a_wa ~ dnorm(0, 2),\n            a_ja ~ dnorm(0, 2),\n            a_red ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = dd\n    )\n\nrethinking::precis(m_w2)\n\n              mean         sd        5.5%      94.5%\na     -0.020886459 0.15873255 -0.27457173 0.23279881\na_wa  -0.191038758 0.14889489 -0.42900156 0.04692404\na_ja   0.247752111 0.14683135  0.01308725 0.48241697\na_red -0.004204578 0.14595115 -0.23746271 0.22905355\nsigma  0.982341917 0.05156316  0.89993403 1.06474980\n\n\nOverall, the red and white wines were judged similarly without a large discrepancy between groups. American judges tended to be more generous, and American wines tended to be rated slightly worse. However, we have a great deal of uncertainty about all of these parameters.\n\n\n8.2.14 8H7\nApparently doing the interactions IS the reason for using indicator coding here, quap I guess can’t handle all the stuff that Stan can. Now we want to include all of the third-level interactions that we can make.\n\nm_w3 &lt;-\n    rethinking::quap(\n        flist = alist(\n            y ~ dnorm(mu, sigma),\n            mu &lt;- a + a_wa * wine_amer + a_ja * judge_amer + a_red * red +\n                # American wine/american judge\n                g_wawj * wine_amer * judge_amer +\n                # American red wines\n                g_rwa * wine_amer * red +\n                # Red wines / american judge\n                g_rja * judge_amer * red,\n            a ~ dnorm(0, 2),\n            a_wa ~ dnorm(0, 2),\n            a_ja ~ dnorm(0, 2),\n            a_red ~ dnorm(0, 2),\n            g_wawj ~ dnorm(0, 2),\n            g_rwa ~ dnorm(0, 2),\n            g_rja ~ dnorm(0, 2),\n            sigma ~ dexp(1)\n        ),\n        data = dd\n    )\n\nrethinking::precis(m_w3, depth = 2)\n\n              mean         sd       5.5%      94.5%\na      -0.21373267 0.21651035 -0.5597580  0.1322927\na_wa    0.15272583 0.26069131 -0.2639092  0.5693609\na_ja    0.29244412 0.26673524 -0.1338503  0.7187386\na_red   0.31208633 0.27442703 -0.1265011  0.7506737\ng_wawj -0.11070543 0.29175737 -0.5769901  0.3555792\ng_rwa  -0.56956098 0.29026516 -1.0334608 -0.1056612\ng_rja   0.04190135 0.28649695 -0.4159761  0.4997788\nsigma   0.97130887 0.05098788  0.8898204  1.0527973\n\n\nUsually I hate looking at tables, but this one is not too bad because the trend is pretty obvious. All of the parameters are zero-ish (lots of probably mass on either side of 0), except for one, which is g_rwa. This is the interaction term between American wines and red wines – so it looks like American red wines were much worse on average than non-american and/or non-red wines.\nIn the solutions guide, Richard actually gives a good reason for using more skeptical priors for the interaction terms than on the main effect terms. I need to remember that for the future.",
    "crumbs": [
      "Textbook",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "McElreath, Richard. 2020. Statistical Rethinking:\nA Bayesian Course with Examples in\nR and Stan. Second. New\nYork: Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608.",
    "crumbs": [
      "References"
    ]
  }
]