[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking notes and exercises",
    "section": "",
    "text": "For Summer 2022, I decided to read “Statistical Rethinking” by Richard McElreath (McElreath (2020)). This book contains my notes on the chapter and my solutions to the exercises. I typically don’t take very detailed notes, so those will likely only be what I consider the key points of the chapter or the parts that I find difficult to remember.\nI will attempt to complete every exercise, but if I get stuck for too long or I’m on some other deadline (see my now page to see what else I might be working on), that may not happen.\nMost of all, I plan to “have fun with it.”\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. New York: Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608."
  },
  {
    "objectID": "cp1.html",
    "href": "cp1.html",
    "title": "1  The Golem of Prague",
    "section": "",
    "text": "This chapter covers McElreath’s perspectives on statistical modeling and causal inference, and gives a description of how the book will address issues with current statistical practice and the tools that will be used."
  },
  {
    "objectID": "cp1.html#chapter-notes",
    "href": "cp1.html#chapter-notes",
    "title": "1  The Golem of Prague",
    "section": "1.1 Chapter notes",
    "text": "1.1 Chapter notes\n\nThe statistical model as a golem: models cannot think or see context. They are designed to do specific jobs and carry out their job exactly as they are instructed to, without regard for the consequences. Golems can make numbers, but we have to make golems and interpret the results.\nStatistics often lacks a coherent epistemiology. We need to understand how statistical models related to causal models, and how causal models relate to scientific hypotheses.\n“Folk Popperism:” many scientists believe that null hypothesis significance testing reflects Popper’s belief that scientific hypotheses must be falsifiable. But really, one statistical model is related to multiple process models, and one process model is related to many scientific hypotheses.\nAdditionally, the NHST paradigm often ignores the fallibility of measurements and the idea that falsification can be spurious. And continuous hypotheses cannot simply be falsified.\nMy favorite quote from this chapter (and it is full of great quotes): “So, if attempting to mimic falsification is not a genreally useful approach to statistical methods, what are we to do? We are to model.”\nThe rest of the chapter details the approaches that the book will take with respect to causal modeling and bayesian analysis methods.\nAn equally good title for this chapter might be “the statistical nihilist’s manifesto.”"
  },
  {
    "objectID": "cp1.html#exercises",
    "href": "cp1.html#exercises",
    "title": "1  The Golem of Prague",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\n(This space intentionally left blank.)"
  },
  {
    "objectID": "cp2.html",
    "href": "cp2.html",
    "title": "2  Small Worlds and Large Worlds",
    "section": "",
    "text": "In this chapter, the basics of Bayesian probability models and updating are described using a motivating example where the percent coverage of water on Earth is estimated by tossing an inflatable globe. This chapter covers the mechanical parts of Bayesian models."
  },
  {
    "objectID": "cp2.html#chapter-notes",
    "href": "cp2.html#chapter-notes",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.1 Chapter notes",
    "text": "2.1 Chapter notes\n\nDistinction between the small world, the self-contained world of the model, and the large world, the context in which the model is used. Remember that golems can’t see context.\nBayesian Inference Is Just Counting – explanation using tree diagrams. I think this is a good example for simple problems but I don’t think it generalizes well to real-life data analysis examples.\nBayesian updating: if you observe more data, you can use Bayes’ Rule to update your plausibilities for each of the possible results.\n“There is no free lunch…a Bayesian golem must choose an initial plausibility, and a frequentist golem must choose an estimator. Both golems pay for lunch with their assumptions.”\nBayesian inference doesn’t distinguish between data and parameters in the same way that frequentist inference does. Instead, data are observed variable, and parameters are unobserved variables.\nLikelihoods, priors, posteriors, Bayes’ rule, and other things I didn’t take notes on were covered in this section.\nThere are multiple numerical techniques for approximating Bayesian models, different “motors” for the golems. These include grid approximation, quadratic approximation, and Markov Chain Monte Carlo. How you fit the model is part of the model (the engine is part of the golem), and different fitting routines have different compromises and advantages.\nGrid approximation: estimate the posterior probability of several different values of the parameter via brute force. I did a rough version on this in my blog post on Bayesian updating.\n\nDefine the grid of posterior values. You have to choose the set of points for evaluation.\nCompute the value of the prior at each parameter value on the grid.\nCompute the likelihood at each parameter value.\nCompute the unstandardized posterior at each parameter value, by multiplying the prior and the likliehood.\nStandardize the posterior by dividing each value by the sum of all values.\n\nQuadratic approximation: as the number of parameters increases, the number of evaluations becomes \\(\\text{number of points} ^ \\text{number of parameters}\\). So more efficient methods (that make more assumptions are needed.) Quap assumes that the posterior is approximately Gaussian near the peak, essentially representing the log-posterior density as a quadratic function. N.b. quadratic approximation improves with the number of data points.\n\nFind the posterior mode, usually accomplished by some optimization algorithm based on the gradient of the posterior. “The golem does not know where the peak is, but it does know the slope under its feet.”\nEstimate the curvature near the peak, which is sufficient to compute a quadratic approximation of the entire posterior distribution.\n\nSee pp 41–43 for grid and quadratic approximation examples.\nMarkov chain Monte Carlo: useful for computing many models that fail for grid or quadratic approximation, and may have thousands of parameters. The final posterior may not even have a closed form. MCMC techniques rely on sampling from the posterior distribution rather than directly attempting to approximate the posterior. Since McElreath didn’t run his MCMC example in the book, I’ve included it here because I wanted to see the result.\n\n\nset.seed(370)\nn_samples <- 10000\np <- rep(NA, n_samples)\np[1] <- 0.5\nw <- 6\nl <- 3\nfor (i in 2:n_samples) {\n    p_new <- rnorm(1, p[i-1], 0.1)\n    if (p_new < 0) {p_new <- abs(p_new)}\n    if (p_new > 1) {p_new <- 2 - p_new}\n    q0 <- dbinom(w, w + l, p[i-1])\n    q1 <- dbinom(w, w + l, p_new)\n    p[i] <- ifelse(runif(1) < q1/q0, p_new, p[i-1])\n}\n\nplot(density(p), xlim = c(0, 1))\ncurve(dbeta(x, w + 1, l + 1), lty = 2, add = TRUE)"
  },
  {
    "objectID": "cp2.html#exercises",
    "href": "cp2.html#exercises",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n2E1. The expression corresponding to the statement the probability of rain on Monday is \\[\\text{Pr}(\\text{rain} \\mid \\text{Monday}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{Monday})}.\\]\n2E2. The statement corresponding to the expression \\[\\text{Pr}(\\text{Monday} \\mid \\text{rain})\\] is the probability that it is Monday, given that it is raining.\n2E3. The expression correspondonding to the statement the probability that it is Monday, given that it is raining is \\[ \\text{Pr}(\\text{Monday} \\mid \\text{rain}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{rain})} = \\frac{\\text{Pr}(\\text{rain} \\mid \\text{Monday})\\text{Pr}(\\text{Monday})}{\\text{Pr}(\\text{rain})}\\]\n2E4. Based on Bruno de Finetti’s statement “PROBABILITY DOES NOT EXIST,” the statement the probability of water is 0.7 from the earlier example is a statement about our beliefs. We know that there are several other factors underlying the globe-tossing experiment, but we cannot measure all of those factors, but sweeping them under the rug, we believe that about 70% of the time, our result should be water. The frequentist interpretation of this is as a long-run average probability, but in the bayesian interpretation, this is our prior belief for the next time we perform the experiment.\n2M1. Assuming a uniform prior for \\(p\\), compute the grid approximate posterior for each of the following sets of observations: W, W, W, W, W, W, L, and L, W, W, L, W, W, W.\n\n# Define a function that computes the grid-approximate posterior with uniform\n# prior for p given a sampled number of water and land tosses.\nglobe_post <- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid <- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior <- rep(1, times = length(p_grid))\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood <- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior <- likelihood * prior\n    \n    # Standardize the posterior\n    posterior <- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post(3, 0)\nglobe_post(3, 1)\nglobe_post(5, 2)\n\n\n\n\n2M2. Repeat the grid approximate calculations assuming a prior for \\(p\\) of the form \\[f(p) = \\begin{cases} 0, & p < 0.5 \\\\ k, & p \\geq 0.5\\end{cases}.\\] Note that for \\(\\int_0^1 f(p) \\ dp = 1,\\) we must have \\(k = 2\\).\n\nglobe_post_step_prior <- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid <- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior <- ifelse(p_grid < 0.5, 0, 1)\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood <- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior <- likelihood * prior\n    \n    # Standardize the posterior\n    posterior <- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post_step_prior(3, 0)\nglobe_post_step_prior(3, 1)\nglobe_post_step_prior(5, 2)\n\n\n\n\n2M3. We want to compute \\(\\text{Pr}(\\text{Earth} \\mid \\text{land})\\) given the following information.\n\n\\(\\text{Pr}(\\text{land} \\mid \\text{Earth}) = 0.3\\)\n\\(\\text{Pr}(\\text{land} \\mid \\text{Mars}) = 1.0\\)\n\\(\\text{Pr}(\\text{Earth}) = \\text{Pr}(\\text{Mars}) = 0.5\\)\n\nWe can deduce that \\[\\begin{align*}\n\\text{Pr}(\\text{land}) &= \\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth}) + \\text{Pr}(\\text{land} \\mid \\text{Mars})\\cdot\\text{Pr}(\\text{Mars}) \\\\\n&= (0.3)(0.5) + (1.0)(0.5) = 0.65.\n\\end{align*}\\]\nSo we compute \\[\\begin{align*}\n\\text{Pr}(\\text{Earth} \\mid \\text{land}) &= \\frac{\\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth})}{\\text{Pr}(\\text{land})} \\\\\n&= \\frac{(0.3)(0.5)}{0.65} \\approx 0.23.\n\\end{align*}\\]\n2M4. We have a deck of three cards: one with two white sides, one with a black side and a white side, and one with two black sides. If we draw one card with the black side up, what is the probability that the other side is also black?\nWe can solve this by directly calculating the conditional probability.\n\\[\\begin{align*}\n\\text{Pr}(\\text{black down} \\mid \\text{black up}) &= \\frac{\\text{Pr}(\\text{black down}, \\text{black up})}{ \\text{Pr}(\\text{black up})} \\\\\n&= \\frac{1 / 3}{1 / 2} = \\frac{2}{3}.\n\\end{align*}\\]\nWe get \\(\\frac{1}{3}\\) for the joint probability since there are three cards, and only one of them has black on both sides. We get the individual probability of one black side being up as \\(\\frac{1}{2}\\) by noticing that there are 6 sides that could be facing up, and 3 of them are black sides.\nThe way that I think scales better to the rest of the problems in this section is by counting the number of ways to get this answer.\n\nIf the card we drew was white on both sides, there are 0 ways we could observe a black side facing up.\nIf the card we drew was white on one side, there is 1 way to observe a black side facing up.\nIf the card we drew was black on both sides, there are 2 ways to observe a black side facing up (it could be either side).\n\nSo out of the three possible ways to generate the situation we observed, two of them have a second black side on the bottom, giving us our \\(\\frac{2}{3}\\) probability.\n2M5. If we add an extra card with two black sides, we update our calculations.\n\nStill 0 ways if we draw the white/white card.\nThere’s still only 1 way to observe a black side facing up with a B/W card.\nHowever, there are now 4 different black sides we could observe facing up with a B/B card.\n\nSo out of 5 ways to observe a black side facing up, four of them have the other side black, giving us a \\(\\frac{4}{5}\\) probability.\n2M6. Now, we suppose that the black ink is heavy. For every one way to pull the B/B card, there are two ways to pull the B/W card and three ways to pull the W/W card.\nThe number of ways to get one black side up has not changed from problem 2M4. There’s still one way with a B/W card and 2 ways with a B/B card. However, now there are \\(2 \\times 1 = 2\\) ways to get the B/W card with the black side up, so the probability of the other side being black is now \\(\\frac{2}{4} = \\frac{1}{2}\\).\n2M7. Now suppose we draw one card and get a black side facing up, then we draw a second card and get a white side facing up. We want to find the probability that the first card was the B/B card.\n\nThere are two ways for a black side to face up if the first card is B/B (either side could be face up). If this is the case, there are three ways for the second card to show white (one way if it is B/W or two ways if it is W/W). So, there are \\(2 \\times 3 = 6\\) ways for us to observe what we did if this is true.\nIf the first card is B/W, there is only one way for a black side to be face up. Then, there are two ways for the second card to face up white (either side of the W/W card), giving \\(1 \\times 2 = 2\\) ways for our data to occur if this is the truth.\nThe W/W card cannot be the first card, the data we observed rules this out.\n\nTherefore, there is a \\(6 / 8 = 3 /4\\) probability that the first card is black on the bottom as well.\n2H1. There are two species of panda, A and B, that are equally likely in the wild. In species A, twins occur 10% of the time. In species B, twins occur 20% of the time. Both species only have twins or singleton infants. If a panda of unknown species gives birth to twins, what is the probability her next birth will also be twins?\nSo, the probability we are interested in is \\(P(\\text{twins} \\mid \\text{twins})\\). This is confusing, so I’ll say \\(P(\\text{twins}^* \\mid \\text{twins})\\).\nWe can calculate that \\[\\begin{align*}\nP(\\text{twins}) &= P(\\text{twins} \\mid A) P(A) + P(\\text{twins} \\mid B) P(B) \\\\\n&= (0.1)(0.5) + (0.2)(0.5) = 0.15.\n\\end{align*}\\]\nFrom the definition of conditional probability, we know that \\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{P(\\text{twins}^*, \\text{twins})}{P(\\text{twins})},\\] so it remains to calculate \\(P(\\text{twins}^*, \\text{twins})\\). If we assume that births are independent, we can calculate\n\\[\\begin{align*}\nP(\\text{twins}^*, \\text{twins}) &= P(\\text{twins}^*, \\text{twins} \\mid A)P(A) + P(\\text{twins}^*, \\text{twins} \\mid B)P(B) \\\\\n&= (0.1)^2(0.5) + (0.2)^2(0.5) = 0.025.\n\\end{align*}\\]\nThen, \\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{0.025}{0.15} = \\frac{1}{6}.\\]\nSo if a panda of unknown species gives birth to twins, the probability that her next birth will also be twins is \\(1/6\\), given the information we have.\n2H2. Now we want to find the probability that the panda is species A, given that she gave birth to twins, i.e. \\(P(A \\mid \\text{twins})\\). Recall that \\(P(\\text{twins}) = 0.15,\\) \\(P(\\text{twins} \\mid A) = 0.1,\\) and \\(P(A) = 0.5.\\) Then,\n\\[P(A \\mid \\text{twins}) = \\frac{(0.5)(0.1)}{0.15} = \\frac{1}{3}.\\]\n2H3. Suppose the panda has a second birth, a singleton infant. What is \\(P(A)\\) now?\nSince we’ve already estimated \\(P(A)\\) for the first birth, I’ll call this \\(P(A_\\text{prior})\\), instead of working out the entire conditional probability, we can update this estimate with our new information. What we want to calculate is\n\\[P(A_\\text{posterior}) = \\frac{P(A_\\text{prior}) P(\\text{singleton} \\mid A)}{P(\\text{singleton})}.\\]\nNow, the probability of having a singleton is mutually exclusive with the probability of having twins (since we know that these pandas never have more than twins), so \\[P(\\text{singleton} \\mid A) = 1 - 0.1 = 0.9. \\]\nWe get the singleton probability for pandas of species B in the same way. Next, we need to calculate the probability of a singleton, taking our prior probability that the panda is species A into account. We get \\[P(\\text{singleton}) = \\frac{1}{3} (0.9) + \\frac{2}{3} (0.8) = \\frac{5}{6}.\\]\nTherefore, \\[P(A_\\text{posterior}) = \\frac{(1/3)(0.9)}{5/6} = 0.36.\\]\n2H4. The new test for panda species has probabilities \\(P(\\text{test } A \\mid A) = 0.8\\) and \\(P(\\text{test } B \\mid B) = 0.65.\\) We want to know the probability that our panda is species A, given that her test result was A.\nFirst we will ignore our prior probability and calculate the probability that any random panda is species A given that they test A.\nWe calculate that \\(P(\\text{test } A \\mid B) = 1 - 0.65 = 0.35\\) and therefore that \\[\\begin{align*}\nP(\\text{test } A) &= P(A)P(\\text{test } A \\mid A) + P(B)P(\\text{test } B \\mid B) \\\\\n&= (0.5)(0.8) + (0.5)(0.35) = 0.575.\n\\end{align*}\\]\nThen, \\[P(A \\mid \\text{test } A) = \\frac{(0.5)(0.8)}{0.575} \\approx 0.6957.\\]\nNow, if we use our prior probability, \\(P(A_\\text{prior}) = 0.36\\), we instead get that\n\\[\\begin{align*}\nP(\\text{test } A) &= (0.36)(0.8) + (0.64)(0.35) = 0.512 \\quad\\text{and } \\\\\nP(A \\mid \\text{test } A) &= \\frac{(0.36)(0.8)}{0.512} \\approx 0.5612.\n\\end{align*}\\]"
  },
  {
    "objectID": "cp3.html",
    "href": "cp3.html",
    "title": "3  Sampling the Imaginary",
    "section": "",
    "text": "This chapter discusses the basics of sampling – instead of directly approximate the density of the posterior distribution, we can draw samples from it. This seems silly for simple distributions, but scales to otherwise intractable problems. Once we have the samples, we can use those to estimate the posterior density."
  },
  {
    "objectID": "cp3.html#chapter-notes",
    "href": "cp3.html#chapter-notes",
    "title": "3  Sampling the Imaginary",
    "section": "3.1 Chapter notes",
    "text": "3.1 Chapter notes\n\n“Fetishizing precision to the fifth decimal place will not improve your science.”\nThe HDPI and PI methods for constructing credible intervals are similar for bell-shaped curves, but will be different for highly skewed curves where the mode and mean are different. “If the choice of interval affects the inference, you are better off plotting the entire posterior distribution.”\nThe HDPI also has higher simulation variance, that is, it needs more samples than the PI to arrive at a stable result.\nChoosing a point estimate, such as the mean, median, or mode (maximum a posteriori value) can be difficult.\nImportantly, different loss functions imply different estimates. The absolute loss function, \\(L(\\theta, \\hat{\\theta}) = \\left| \\theta - \\hat{\\theta} \\right|\\) is minimized by the median; the quadratic loss function, \\(L(\\theta, \\hat{\\theta}) = \\left( \\theta - \\hat{\\theta} \\right)^2\\), is minimized by the mean; and the 0-1 loss function (different for discrete and continuous problems) is minimized by the mode and corresponds to maximizing the posterior likelihood.\nWhile frequentist methods rely on sampling distributions and the (theoretical) physical act of random sampling, Bayesian models do not! The “sampling” we are doing here is small world sampling – our samples are from the model, we don’t expect them to be “real.”\nDummy data generated by the prior predictive distribution, the distribution of the parameters of interest using only the priors and not the data, help us build models. These simulations can tell us whether the priors are reasonable or not.\nOnce we update the model with the data, we can generate samples from the posterior predictive distribution. These samples can help us check how accurate the model is or how well it fit. This distribution is “honest” because it propagates the uncertainty embodied in the posterior distribution of the parameter of interest."
  },
  {
    "objectID": "cp3.html#exercises",
    "href": "cp3.html#exercises",
    "title": "3  Sampling the Imaginary",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\nFor the easy exercises, we need the following code given in the book.\n\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, times = 1000)\nlikelihood <- dbinom(6, size = 9, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\n3E1. How much posterior probability lies below \\(p = 0.2\\)?\n\nmean(samples <= 0.2)\n\n[1] 4e-04\n\n\n3E2. How much posterior probability lies below \\(p = 0.8\\)?\n\nmean(samples <= 0.8)\n\n[1] 0.8884\n\n\n3E3. How much posterior probability lies between \\(p = 0.2\\) and \\(p = 0.8\\).\nWe could calculate this directly.\n\nmean((samples >= 0.2) & (samples <= 0.8))\n\n[1] 0.888\n\n\nOr if we had stored the previous calculations, we could have used those instead.\n\nmean(samples <= 0.8) - mean(samples <= 0.2)\n\n[1] 0.888\n\n\n3E4. 20% of the posterior probability lies below which value of \\(p\\)?\n\nquantile(samples, probs = c(0.2))\n\n      20% \n0.5185185 \n\n\n3E5. 20% of the posterior probability lies above which value of \\(p\\)?\n\nquantile(samples, probs = c(0.8))\n\n      80% \n0.7557558 \n\n\n3E6. Which values of \\(p\\) contain the narrowest interval equal to 66% of the posterior probability?\n\nrethinking::HPDI(samples, prob = 0.66)\n\n    |0.66     0.66| \n0.5085085 0.7737738 \n\n\n3E7. Which values of \\(p\\) contain 66% of the posterior probability, assuming equal posterior probability both above and below the interval?\n\nrethinking::PI(samples, prob = 0.66)\n\n      17%       83% \n0.5025025 0.7697698 \n\n\n3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\n\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, times = 1000)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\n3M2. Draw 10000 samples from the grid approximate prior abnove. Then use the samples to calculate the 90% HDPI for \\(p\\).\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = 0.90)\n\n     |0.9      0.9| \n0.3343343 0.7217217 \n\n\n3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in \\(p\\). What is the probability of observing 8 water in 15 tosses?\n\nppc <- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.1499\n\n\n3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.\n\nppc2 <- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.1842\n\n\n3M5. Start over at 3M1, this time using a prior that is zero below \\(p = 0.5\\) and a constant above \\(p = 0.5\\).\nFirst we approximate the posterior and take samples.\n\n# I should name these different things but I am not going to.\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- ifelse(p_grid < 0.5, 0, 2)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nNow we’ll do the first posterior predictive check and estimate the probability. Note that the true probability (if \\(p = 0.7\\)) is \\(0.08113\\).\n\nppc <- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.163\n\n\nIt looks like this estimate is actually slightly worse with this prior than it was with the uniform prior. However, they are fairly similar.\nAnd the second check. Note that the true probability is \\(0.2668279\\)\n\nppc2 <- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.2353\n\n\nThis estimate is much closer to the true value than the previous estimate was. It seems that this prior allows us to more accurate estimate probabilities close to the true value (\\(p = 0.7\\)), but not near the lower boundary for the prior. We can examine the histogram.\n\nrethinking::simplehist(ppc)\n\n\n\n\nWe can see that the low values are extremely low, but so are the high values. We would expect the mode to be around 10 or 11, but since we observed 8 / 15, it makes sense that we get a higher estimate of the probability of this occurring than what we “know” is true.\n3M6. We want to construct a 99% percentile interval of the posterior distribution of \\(p\\) that is only 0.05 wide. How many times will we have to toss the globe to do this?\nTo me, this question seems phrased in the general, but I think it is impossible to answer in general. So we’ll do our best. First let’s look at the width of the current PI.\n\nrethinking::PI(samples, prob = 0.99) |> diff()\n\n     100% \n0.3243243 \n\n\nThat’s much larger than what we want, but we only tossed the ball 15 times. So we’ll need to do some simulating to solve this problem. I know this is not the “true” probably, but for the sake of keeping with this model, I’ll make sure all of our larger samples have (approximately) the same \\(8/15\\) probability of water.\nI’ll also continue using the flat prior. The answer to this question depends on both the “true” value of \\(p\\) and the prior that we used.\n\none_sim <- function(N) {\n    likelihood <- dbinom(floor(N * (8/15)), size = N, prob = p_grid)\n    posterior <- likelihood * prior\n    posterior <- posterior / sum(posterior)\n\n    set.seed(100)\n    samples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n    \n    out <- rethinking::PI(samples, prob = 0.99) |> diff()\n    return(out)\n}\n\nmy_n <- seq(from = 10, to = 5000, by = 10)\nsim_res <- purrr::map_dbl(my_n, one_sim)\nplot(sim_res ~ my_n, type = \"l\")\nabline(h = 0.05, col = \"red\", lty = 2)\n\n\n\n\nVisually, we can see that around 3000 samples are necessary, let’s get the exact estimate.\n\nindex <- min(which(sim_res < 0.05))\ncat(\"n: \", my_n[index], \"; width: \", sim_res[index], sep = \"\")\n\nn: 2740; width: 0.04905405"
  },
  {
    "objectID": "cp3.html#hard-problems",
    "href": "cp3.html#hard-problems",
    "title": "3  Sampling the Imaginary",
    "section": "3.3 Hard Problems",
    "text": "3.3 Hard Problems\nFor the hard problems, we need to load the indicated data set.\n\ndata(homeworkch3, package = \"rethinking\")\ncombined <- c(birth1, birth2)\n\n3H1. Use grid approximation to compute the posterior distribution for the probability of a birth being a boy.\n\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, times = 1000) # Uniform prior\nlikelihood <- dbinom(sum(combined), size = length(combined), prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nplot(posterior ~ p_grid, type = \"l\")\n\n\n\n\n3H2. Draw 10000 random samples from the posterior, and use these to estimate the 50, 89, and 97 percent HDPIs.\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = c(0.5, 0.89, 0.97))\n\n    |0.97     |0.89      |0.5      0.5|     0.89|     0.97| \n0.4824825 0.4994995 0.5265265 0.5725726 0.6076076 0.6296296 \n\n\n3H3. Simulate 10,000 replicates of 200 births. Compare the distribution of predicted counts to the actual count. Does it look like the model fits the data well?\n\nset.seed(100)\nppc <- rbinom(1e4, size = 200, prob = samples)\nrethinking::simplehist(ppc)\nabline(v = sum(combined), col = \"red\", lty = 2)\n\n\n\n\nIn this particular simulation, the observed value (111 boys) is a central, likely outcome of the posterior predictive distribution. The model seems to fit the data well, although there is a fairly large amount of spread.\n3H4. Now compare 10,000 counts of boys from 100 simulated firstborns only to the number of boys in the first births.\n\nset.seed(100)\nb1_samp <- rbinom(10000, size = 100, prob = samples)\nrethinking::simplehist(b1_samp)\nabline(v = sum(birth1), col = \"red\", lty = 2)\n\n\n\n\nThe model seems to overestimate the number of firstborn boys. This could potentially be because our observed count of boys is slightly higher than 50% (\\(0.56 \\%\\)) and this overestimation becomes more prominent in the smaller sample size. However, the true value is not in the tails of our distribution, so we would probably capture it in a CI.\n3H5. Our model assumes that sex of first and second births are independent. We can check this assumption by focusing on second births that followed female firstborns. Compare 10,000 simulated counts of boys to only those second births that followed girls.\n\n# Get the correct count\nn <- sum(birth2[birth1 == 0])\n\n# Run the simulation\n\nb2_samp <- rbinom(1e4, size = n, prob = samples)\n\n# Plot the results\nrethinking::simplehist(b2_samp, xlim = c(min(b2_samp), max(n, max(b2_samp))))\nabline(v = n, col = \"red\", lty = 2)\n\n\n\n\nWow, the number of boys who follow girls is much larger than our model predicts. Either our sample is far away from the “real” value (although this is really more of a frequentist notion), or more likely, the assumption of our model is wrong."
  },
  {
    "objectID": "cp4.html",
    "href": "cp4.html",
    "title": "4  Geocentric Models",
    "section": "",
    "text": "This chapter discusses basic Bayesian model fitting with grid and quadratic approximations. The title, “geocentric models,” refers to models that make good predictions, but do not provide causal information about a question. One of these examples is linear regression. This chapter gives examples of basic bayesian linear regression and includes details such as prior predictive checks and how to fit curves with polynomials and b-splines."
  },
  {
    "objectID": "cp4.html#chapter-notes",
    "href": "cp4.html#chapter-notes",
    "title": "4  Geocentric Models",
    "section": "4.1 Chapter notes",
    "text": "4.1 Chapter notes\n\nNormal distributions arise from sums of random fluctuations. Lognormal distributions arise from products of random fluctuations. This property explains why normal distributions are so good at modeling real world data (ontological justification).\nNormal distributions can also be justified by the principle of maximum entropy – if all we are willing to specify about a distribution is its mean and variance, then the normal distribution contains the least amount of information (epistemological justification).\nIndex coding (as opposed to dummy coding or similar methods) makes specification of priors for categorical variables easier.\nThe prior predictive simulation, drawing samples from the distribution implied by the priors, is essential for ensuring that our priors are reasonable. Note that we should not compare the prior predictive simulation to the observed data, only to our preexisting knowledge of constraints on the model.\nMany models which are written in the “plus epsilon” form (see pg 81) (typical for linear models) can be rewritten in this more general framework, which will be easier for non-Gaussian models.\nQuadratic approximation, estimating the peak of the posterior distribution with a multivariate normal distribution, is easier than grid approximation and works well when the posterior is approximately Gaussian (many simple examples are). The peak of the quadratic approximate posterior is the maximum a posteriori estimate.\nRecall that even though grid and quadratic approximate posteriors provide an actual estimate of the posterior distribution, we can (and should) still sample from the posterior. This mimics the process for inference on more complicated models that must be fit with MCMC algorithms.\nA linear model fits the mean, \\(\\mu\\), of an outcome as a linear function of the predictor variable(s) and some parameters. These models are often geocentric – they provide good answers, but often say nothing about causality.\nPlotting simulations of the posterior distribution can provide a lot of information about the posterior (see pg 99), often much more than tables of calculations alone.\nThese types of Bayesian models incorporate two different types of uncertainty – uncertainty in parameter values, which is based on the plausibility of parameter values after seeing the data, and uncertainty from sampling processes.\nWe can extend linear models to fit curved patterns in datas in several ways, but two of the easiest are polynomials and b-splines. Priors can be difficult to fit to both, as these models are also geocentric. They can accurately fit curves, but do not describe the mechanism or process that generates curved data in the first place. See pg. 119 for an example of fitting a b-spline model using rethinking. One further extension is the generalized additive model (GAM) which incorporates smoothing over continuous predictor variables."
  },
  {
    "objectID": "cp4.html#exercises",
    "href": "cp4.html#exercises",
    "title": "4  Geocentric Models",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\nThe first few questions are about the following model. \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n4.2.1 4E1\nIn the model shown, the line \\(y_i \\sim \\mathrm{Normal}(\\mu, \\sigma)\\) is the likelihood.\n\n\n4.2.2 4E2\nThe model contains two parameters (\\(\\mu\\) and \\(\\sigma\\)).\n\n\n4.2.3 4E3\nTo use Bayes’ theorem to calculate the posterior, we would write \\[\n\\frac{\\prod_{i} \\mathrm{Normal}(y_i \\mid \\mu, \\sigma) \\times \\mathrm{Normal}(\\mu \\mid 0, 10) \\times \\mathrm{Exponential}(\\sigma \\mid 1)}{\\int\\int \\prod_{i} \\mathrm{Normal}(y_i \\mid \\mu, \\sigma) \\times \\mathrm{Normal}(\\mu \\mid 0, 10) \\times \\mathrm{Exponential}(\\sigma \\mid 1) \\ \\mathrm{d}\\mu \\ \\mathrm{d}\\sigma}.\n\\]\n\n\n4.2.4 4E4\nIn the model shown below, the linear model is the line \\(\\mu_i = \\alpha + \\beta x_i\\). \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 1) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(2)\n\\end{align*}\\]\n\n\n4.2.5 4E5\nThere are three parameters in the posterior distribution of the model shown (\\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\)) – \\(\\mu\\) is no longer a parameter of the model since it is calculated deterministically.\n\n\n4.2.6 4M1\nFor the model definition below, simulate observed \\(y\\) values from the prior. \\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu, \\sigma) \\\\\n\\mu &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n# Do the simulation\nmu <- rnorm(1000, mean = 0, sd = 10)\nsigma <- rexp(1000, rate = 1)\ny <- rnorm(1000, mu, sigma)\n\n# Plot the results\nlayout(matrix(c(1, 2, 3), ncol = 3))\nhist(y, breaks = \"FD\", main = \"y\")\nhist(mu, breaks = \"FD\", main = \"mu\")\nhist(sigma, breaks = \"FD\", main = \"sigma\")\n\n\n\n\n\n\n4.2.7 4M2\nTranslate the model into a quap formula.\n\ny ~ dnorm(mu, sigma),\nmu ~ dnorm(0, 10),\nsigma ~ dexp(1)\n\n\n\n4.2.8 4M3\nTranslate the quap model formula below into a mathematical model definition.\n\ny ~ dnorm(mu, sigma),\nmu <- a + b * x,\na ~ dnorm(0, 10),\nb ~ dunif(0, 1),\nsigma ~ dexp(1)\n\n\\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= a + b * x_i \\\\\na &\\sim \\mathrm{Normal}(0, 10) \\\\\nb &\\sim \\mathrm{Uniform}(0, 1) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1)\n\\end{align*}\\]\n\n\n4.2.9 4M4\nA sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model, defending any priors you choose.\n\\[\\begin{align*}\ny_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta * (\\mathrm{year}_i - \\min \\mathrm{year}_i) \\\\\n\\alpha &\\sim \\mathrm{Normal}(120, 30) \\\\\n\\beta &\\sim \\mathrm{Log-Normal}(0, 1.5) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(0, 0.2)\n\\end{align*}\\]\nTo create the priors, I assumed that height was measured in centimeters. If it is not, transforming either the data or the prior coefficients is a simple linear transformation. The prior for the intercept is centered at a relatively small height (around four feet) with a large spread to allow for differences in biological sex or age distributions in the population, since these were not specified in the question. I subtracted the minimum year in the model so that the years would be scaled as 0, 1, 2, 3, instead of the actual numeric value of the year.\nIn general, we know that height increases over time, so I used a lognormal prior for the slope to enforce a positivity constraint. The prior has a location parameter of 0, allowing for the chance of no growth over the three years, but a wide spread was chosen by experimenting until the prior predictive simulation represented a wide range of possible trajectories with very few trajectories appearing to be biologically unreasonable. Again, the value was chosen by experimenting with prior predictive simulations until the result appeared to capture a large variety of biologically meaningful trajectories.\nI assigned an exponential prior to sigma to reflect the fact that all variances are positive, and most tend to be small-ish, but can be large. I had a difficult time with this one in particular because this model structure seems to imply that people can shrink in-between years. This would be possible with measurement error, but I think it would be quite difficult to have measurement error this severe in something like height, which is easy to measure. However, in order to include a constraint that height can only increase, I think we would need to change the likelihood in the model, which we haven’t discussed yet in the book, so I didn’t want to worry about that. (For example, we could make height lognormally distributed, so random fluctuations would only increase height, as opposed to the current model where random fluctuations can decrease height.)\n\nset.seed(100)\n\n# Prior predictive simulation\na <- rnorm(1000, 120, 30)\nb <- rlnorm(1000, 0, 1.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma <- rexp(1000, 1 / 5)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\n\n\n4.2.10 4M5\nIf I were reminded that every student got taller each year, this would not really change my choice of priors, but it does make me consider the same issues. I already accounted for this in the prior for \\(\\beta\\). However, it does make me think more about the likelihood I used – I really dislike that this likelihood allows for fluctuations that show up like this. Height is not measured perfectly, but large variations are uncommon. So perhaps it makes more sense to have quite a small variance parameter (\\(\\sigma\\)). Or perhaps we could structure the model so that each student has a common variance parameter that does not change every year. We could also consider making the effect of \\(\\beta\\) stronger, so that there is an assumed growth effect and not growing each year is more rare. So perhaps we could consider the following adjusted priors.\n\nset.seed(100)\n\n# Prior predictive simulation\na <- rnorm(1000, 120, 30)\nb <- rlnorm(1000, 2, 0.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma <- rexp(1000, 1)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\nThese priors still represent a wide range of biologically accurate values, but there is much less internal (within-subject) fluctuation in height between years, and on average, the slope is steeper.\n\n\n4.2.11 4M6\nIf I had the information that variance among heights for students of the same age is never more than 64cm, this would change my choice of variance prior (assuming that this is a priori information and not measured from our sample). If this were measured from the sample, I would not change my priors. But we could consider changing the prior for \\(\\sigma\\) like so. Using the empirical rule for normal distributions, I reasoned that 21 is a reasonable constraint for an upper bound on a Uniform prior for sigma – the resultant likelihood will have approximately 99% of observations within \\(3 \\times 21 = 63\\) cm of the mean.\n\nset.seed(100)\n\n# Prior predictive simulation\na <- rnorm(1000, 120, 30)\nb <- rlnorm(1000, 2, 0.5)\n\n# PPS for mu -- only needs a and b\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of mu\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nfor (i in 1:1000) {\n    curve(a[i] + b[i] * x, from = 0, to = 3, add = TRUE,\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n# PPS for y -- for each a, b, simulate variance around the sampled mu.\nplot(\n    NULL,\n    xlim = c(-0.05, 3.05),\n    ylim = c(-100, 400),\n    xlab = c(\"year\"),\n    ylab = c(\"height\"),\n    main = \"prior predictive simulation of y\"\n)\nabline(h = 0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\nsigma <- runif(1000, 0, 21)\nfor (i in 1:1000) {\n    lines(x = 0:3, y = rnorm(4, a[i] + b[i] * 0:3, sigma[i]),\n          col = rethinking::col.alpha(\"black\", 0.1))\n}\n\n\n\n\nThis variance is quite large, which I think intensifies the problem I was previously discussing. Since we consider each annual measurement to be independent, without any “clustering” (we haven’t used that word yet so I don’t want to use it wrong) by inviduals, following this model to the letter allows for wide fluctuations within the predicted trajectory – to me it just doesn’t make sense for the simulated trajectory to be lower in year \\(N + 1\\) than in year \\(N\\).\n\n\n4.2.12 4M7\nRefit model m4.3 but omit the mean weight xbar. Compare the new model’s posterior to that or the original model, then compare the posterior predictions.\n\nset.seed(100)\nlibrary(rethinking)\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nLoading required package: ggplot2\n\n\nrstan (Version 2.21.5, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/Zane/Documents/.cmdstan/cmdstan-2.30.1\n\n\n- CmdStan version: 2.30.1\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.21)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(Howell1)\nd <- Howell1\nd2 <- d[d$age >= 18, ]\nm_4m7 <-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu <- a + b * weight,\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = d2\n    )\n\nFirst we’ll look at the posterior summary.\n\nprecis(m_4m7)\n\n             mean         sd        5.5%       94.5%\na     114.5302928 1.89715027 111.4982802 117.5623053\nb       0.8908259 0.04174484   0.8241096   0.9575422\nsigma   5.0711281 0.19109884   4.7657152   5.3765409\n\n\nThe intercept estimate is quite different – the previous model reported the following statistics for a: mean 154.6, sd 0.27, 5.5% 154.17, 94.5% 155.05. So our parameter is much smaller with a larger variance. However, the estimates for b and sigma are almost exactly the same as the estimates given for the previous model (see book pg 99). In particular, we should look at the covariance matrix according to the question.\n\nvcov(m_4m7) |> round(digits = 3)\n\n           a      b sigma\na      3.599 -0.078 0.009\nb     -0.078  0.002 0.000\nsigma  0.009  0.000 0.037\n\n\nThe variance for a is much higher, while the variance for b and sigma is exactly the same as the book reports. However, there is now some covariance between a and b, and between a and sigma (but not between b and sigma). Next I’ll plot a sample of prior predictions. Since the uncertainty is so narrow, I decided to only plot 100 posterior samples. The red line shows the maximum a posteriori estimate.\n\npost <- extract.samples(m_4m7, n = 100)\nlayout(1)\nplot(\n    x = d2$weight, y = d2$height,\n    xlim = range(d2$weight),\n    ylim = range(d2$height),\n    col = rangi2,\n    xlab = \"weight\", ylab = \"height\"\n)\nmtext(\"Sampled posterior predictions\")\n\n# Plot the lines\nfor (i in 1:length(post$a)) {\n    curve(post$a[i] + post$b[i] * x,\n                col = col.alpha(\"black\", 0.1),\n                add = TRUE)\n}\n\nabline(\n    a = mean(post$a),\n    b = mean(post$b),\n    col = \"red\",\n    lty = 2\n)\n\n\n\n\nThe posterior predictions look about the same to me. So I guess that even when we use different parametrizations (thus changing the interpretation and scale of our alpha parameter), the predictions still work out to be about the same. I think this is foreshadowing MCMC convergence diagnostics with different parametrizations in the future.\n\n\n4.2.13 4M8\nRefit the cherry blossom spline and experiment with changing the number of knots and the width of the prior on the weights. What do you think the combination of these controls?\nFirst we’ll just refit the example.\n\n# Data import\ndata(\"cherry_blossoms\")\nd <- cherry_blossoms\nd2 <- d[complete.cases(d$doy), ]\n\nset.seed(100)\n# Create the splits\nnk <- 15\nknot_list <- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB <- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm1 <-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu <- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu <- link(m1)\nmu_PI <- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"Original model\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\nNow let’s fit an example with more knots, say 30. This is a dramatic increase but I really want to see the effect.\n\nset.seed(100)\n# Create the splits\nnk <- 30\nknot_list <- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB <- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm2 <-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu <- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu <- link(m2)\nmu_PI <- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"More knots\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\nNow we’ll also increase the width of the prior for \\(w\\). Again, I’ll increase this dramatically to make the effect easier to see.\n\nset.seed(100)\n# Create the splits\nnk <- 20\nknot_list <- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB <- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm3 <-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu <- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 70),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\nCaution, model may not have converged.\n\n\nCode 1: Maximum iterations reached.\n\n# Get the posterior predictions and plot them\nmu <- link(m3)\nmu_PI <- apply(mu, 2, PI, 0.97)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\",\n         main = \"More knots and wider prior\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\nHmm, it’s hard to see a difference. Let’s try plotting the three lines on top of each other.\n\nlayout(1)\nplot(\n    x = d2$year, y = d2$doy,\n    xlab = \"year\", ylab = \"Day in year\",\n    xlim = range(d2$year), ylim = range(d2$doy),\n    col = col.alpha(rangi2, 0.3), pch = 16\n)\nlines(d2$year, y = apply(link(m1), 2, mean), col = \"black\")\nlines(d2$year, y = apply(link(m2), 2, mean), col = \"blue\")\nlines(d2$year, y = apply(link(m3), 2, mean), col = \"red\")\nlegend(\n    x = \"top\",\n    c(\"original\", \"more knots\", \"more knots and wider prior\"),\n    col = c(\"black\", \"blue\", \"red\"),\n    lty = c(1, 1, 1)\n)\n\n\n\n\nYep, from this plot the difference is pretty easy to see. The number of knots and the prior for the weights controls the “wiggliness” (the smoothness or penalty) of the splines. More knots or a wider prior allows for more local variation in the spline curve, whereas constraining the number of knots (or shrinking the weights toward 0) constrains the spline, forcing it to vary less. I guess that the prior on the weights here is equivalent to increasing the penalty term of some other kind of spline, and a higher number of knots allows for a higher degree of interpolation as more inflection points are included.\n\n\n4.2.14 4H1\nWe want to estimate the expected heights of the 5 individuals from the !Kung census with weights given in the text. Since we already saw that the predictions from our fitted model earlier were the same as the predictions in the book, I’ll use that model to do this. All we need to do is create a data frame with the heights of these individuals and pass it to link with the model.\n\n# Create the data for the new individuals\nnew_indiv <-\n    tibble::tribble(\n        ~individual, ~weight,\n        1, 46.95,\n        2, 43.72,\n        3, 64.78,\n        4, 32.59,\n        5, 54.63\n    )\n\n# Get the samples from the posterior at the given weights\npreds <- rethinking::link(fit = m_4m7, data = new_indiv)\n\n# Get the means and round it\nnew_indiv$means <- apply(preds, 2, function(x) round(mean(x, 2)))\n\n# Calculate the PI and format it\nPIs <- apply(preds, 2, PI, 0.89)\nnew_indiv$PIs <- apply(PIs, 2, function(x) paste0(round(x, 2), collapse = \", \"))\n\n# Format names and create the table\nnames(new_indiv) <- c(\"Individial\", \"weight\", \"expected height\", \"89% interval\")\nknitr::kable(\n    new_indiv,\n    caption = paste0(\"Expected heights with 89% equal-tailed posterior intervals\",\n                                     \"for the five individuals wiuthout recorded heights in the\",\n                                     \"!Kung census data.\")\n)\n\n\nExpected heights with 89% equal-tailed posterior intervalsfor the five individuals wiuthout recorded heights in the!Kung census data.\n\n\nIndividial\nweight\nexpected height\n89% interval\n\n\n\n\n1\n46.95\n156\n155.88, 156.77\n\n\n2\n43.72\n153\n153.04, 153.92\n\n\n3\n64.78\n172\n170.85, 173.53\n\n\n4\n32.59\n144\n142.62, 144.51\n\n\n5\n54.63\n163\n162.4, 163.89\n\n\n\n\n\n\n\n4.2.15 4H2\nSelect out all the rows in the Howell1 data with ages below 18 years of age. Fit a linear regression to these data using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?\nOk, first we’ll subset the data and make a quick plot.\n\nd <- Howell1[Howell1$age < 18, ]\nplot(d$weight, d$height, xlab = \"weight\", ylab = \"height\")\n\n\n\nd_xbar <- mean(d$weight)\n\nNow let’s fit the model. I’ll use the same priors as before, since I already looked at the data. We probably want to actually change the prior on the intercept since children have a lower mean height than adults, but I think it will work out alright.\n\nm4h2 <-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu <- a + b * (weight - d_xbar),\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = d\n    )\nprecis(m4h2)\n\n            mean         sd       5.5%      94.5%\na     108.383377 0.60867061 107.410604 109.356151\nb       2.716657 0.06831732   2.607473   2.825841\nsigma   8.437402 0.43060002   7.749220   9.125584\n\n\nWe can see that the estimate slope is \\(\\beta = 2.69\\). So for every 10 units of increase in weight, our model predicts that a child’s height will increase by \\(26.9\\) units.\nNext, we’ll plot the MAP prediction line with an 89% interval, along with the 89% prediction interval for heights.\n\nplot(d$weight, d$height, xlab = \"weight\", ylab = \"height\")\n\n# Weight values to predict over\nweight_seq <- seq(from = min(d$weight), to = max(d$weight), by = 0.5)\n\n# Get the posterior predictions\nset.seed(100)\nlink_4h2 <- link(m4h2, data = data.frame(weight = weight_seq))\nsim_4h2  <-  sim(m4h2, data = data.frame(weight = weight_seq))\n\n# Summarize the samples\nmu_mean <- apply(link_4h2, 2, mean)\nmu_CI <- apply(link_4h2, 2, PI, 0.89)\nmu_PI <- apply(sim_4h2, 2, PI, 0.89)\n\n# Plot the results -- we want to go backwards, starting with the wider\n# interval first\nshade(mu_PI, weight_seq, col = col.alpha(\"black\", 0.5))\nshade(mu_CI, weight_seq, col = col.alpha(\"lightgray\", 0.75))\nlines(weight_seq, y = mu_mean, col = \"red\", lty = 2)\n\n\n\n\nThe observed height and weight of the children in the !Kung census data (circular points) with the MAP linear regression line (red dashed line), 89% equal-tailed posterior interval of this line (light gray shading), and 89% equal-tailed posterior interval for predicted heights ( dark gray shading).\n\n\n\n\nThe main issue with the model is that there is clearly some curvature to the trend in the data. There seems to be an inflection point around a weight of 30 where the trend is no longer linear, which will make the entire model fit worse (particularly because this region has high leverage). There is also some curvature in the lower weight values as well. Perhaps it would be better to fit this model with a spline or with a dummy variable for “age categories” that interacts with the slope term, allowing the model to “bend” at particular points – the same thing could be accomplished using splines with degree 1. Normally I hate discretization, but in this case these trends are likely to map to our approximate knowledge of human growth. Babies grow faster than toddlers, and teenagers grow faster than younger children. So perhaps including more information in our model could help solve this problem as well. But if we don’t want to include age, a spline could probably work better.\n\n\n4.2.16 4H3\nBased on our colleague’s suggestion, we want to use the natural logarithm of weight to model height. We’ll do that using the entire Howell1 dataset. I’ll use the same priors as before.\n\nm4h3 <-\n    quap(\n        flist = alist(\n            height ~ dnorm(mu, sigma),\n            mu <- a + b * log(weight),\n            a ~ dnorm(178, 20),\n            b ~ dlnorm(0, 1),\n            sigma ~ dunif(0, 50)\n        ),\n        data = Howell1\n    )\nprecis(m4h3)\n\n            mean        sd       5.5%      94.5%\na     -22.874317 1.3342911 -25.006772 -20.741862\nb      46.817789 0.3823240  46.206761  47.428816\nsigma   5.137088 0.1558847   4.887954   5.386222\n\n\nThese estimates indicate that an individual from the census with mean weight is expected to have a height of \\(138.27\\) units. The slope (b) indicates that for every 1 unit increase in log-weight, height is expected to increase by about 47 units. I think that log-weight units are a bit difficult to think about, this would probably be easier to understand if we’d used log2 or something without an \\(e\\) in it. This coefficient means that if we multiply an individual’s weight by \\(e\\) (approximately 2.72), we would expect to see a 47 unit increase in their height. So, e.g., if we used log2, we could interpret this as doubling their weight, but we can’t do that now.\n\n# Plot the raw data\nplot(height ~ weight, data = Howell1, col = col.alpha(rangi2, 0.5))\n\n# Get the mean and height samples -- I couldn't get sim to work so I did link\n# and then did the sim part myself\nweight_seq <- seq(from = min(Howell1$weight),\n                                    to = max(Howell1$weight),\n                                    by = 1)\nset.seed(100)\nlink_4h3 <- link(m4h3, data = data.frame(weight = weight_seq))\nsim_4h3  <-  sim(m4h3, data = data.frame(weight = weight_seq))\n\n# Summarize the samples\nmu    <- apply(link_4h3, 2, mean)\nmu_CI <- apply(link_4h3, 2, PI, 0.97)\nmu_PI <- apply( sim_4h3, 2, PI, 0.97)\n\n# Add the predictions to the plot\nlines(weight_seq, y = mu_PI[1, ], lty = 2)\nlines(weight_seq, y = mu_PI[2, ], lty = 2)\nshade(mu_CI, weight_seq, col = col.alpha(\"darkgray\", 0.75))\nlines(weight_seq, y = mu, col = \"red\", lty = 2)\n\n\n\n\nWell, it does look like our colleague’s suggestion produced a better model!\n\n\n4.2.17 4H4\nPlot the prior predictive distribution for the parabolic polynomial regression model in the chapter. We want to modify the prior distributions of the parameters so that the prior predictions lie within the reasonable outcome space we defined.\n\n# Data setup since I keep overwriting names\nd <- Howell1\nd2 <- d[d$age >= 18, ]\n\n# Standardize weight\nd2$weight_s <- scale(d2$weight)\nd2$weight_s2 <- d2$weight_s ^ 2\n\nxbar <- mean(d2$weight)\nsx <- sd(d2$weight)\n\n# Setup\nlayout(matrix(c(1, 2), ncol = 2))\nset.seed(100)\nN <- 100\n\n# Repeat the original priors listed\na <- rnorm(N, 178, 20)\nb1 <- rlnorm(N, 0, 1)\nb2 <- rnorm(N, 0, 1)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"Original priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * ((x - xbar) / sx) + b2[i] * (((x - xbar) / sx) ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n# Repeat the original priors listed\na <- rnorm(N, 178, 20)\nb1 <- rnorm(N, 0, 4)\nb2 <- rnorm(N, 0, 4)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"New priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * ((x - xbar) / sx) + b2[i] * (((x - xbar) / sx) ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n\n\n\nWell, when I looked at the original priors they were all already within the reasonable model space. But since we have a parabola now, we don’t need to constrain the linear part to be positive – we could get an always positive derivative even with a negative linear term (\\(2ax + b > 0\\) for \\(ax^2 + bx + c\\), so e.g. if \\(a = 2, \\ b = -2\\) the derivative will be positive for all \\(x > 1\\), and all of our values are larger than 30). So I changed the priors to get trajectories that cover a bit more of the reasonable space. They approach the edges in some cases, but that’s okay. I wasn’t really sure what else to do with this question once I saw that the parabolas were all already in the reasonable space.\nMaybe I was supposed to NOT standardize the data, even though the chapter did this and the book didn’t explicitly say what model to use. However, assigning a biologically meaningful prior without even centering the data is quite difficult, because in this example it makes sense to constrain the (non-centered) intercept at 0 (i.e. a person with weight zero has height zero).\nHowever, we can quickly see how this model doesn’t make biological sense by thinking about where we should center the data. If we center at the mean, our parabola has to be either concave up or concave down (recall the second derivative of a parabola is constant), which doesn’t make any sense. Why would low weight people be taller, or high weight people be shorter? By centering the data at the mean we force the parabola to take one of these two options. So perhaps standardizing by some reference point would be good, but remember that we don’t want to look at the data to choose where to standardize. So we’re forced to make one of two biologically inaccurate assumptions: either the concavity must be unreasonable or the intercept must be unreasonable. So let’s set an unreasonable intercept and choose not to center the data. That way we can at least constrain the concavity to be positive.\n\n# Setup\nset.seed(100)\nlayout(matrix(c(1, 2), ncol = 2))\n\n# Repeat the original priors listed\na <- rnorm(N, 178, 20)\nb1 <- rlnorm(N, 0, 1)\nb2 <- rnorm(N, 0, 1)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"Original priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * x + b2[i] * (x ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n# Repeat the original priors listed\na <- rnorm(N, 90, 20)\nb1 <- rnorm(N, 0, 0.5)\nb2 <- rnorm(N, 0.02, 0.005)\n\nplot(\n    NULL,\n    xlim = range(d2$weight), ylim = c(-100, 400),\n    xlab = \"weight\", ylab = \"height\",\n    main = \"New priors\"\n)\nabline(h =   0, lty = 2, lwd = 0.5)\nabline(h = 272, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    curve(\n        a[i] + b1[i] * x + b2[i] * (x ^ 2),\n        from = min(d2$weight), to = max(d2$weight),\n        add = TRUE,\n        col = col.alpha(\"black\", 0.2)\n    )\n}\n\n\n\n\nThere. Now our priors for the non-centered data fit inside the reasonable range. Are you happy now? They don’t really make any sense (why would a person with 0 weight have 90 height? why do we have to restrain the coefficient for the b terms so much?) but they fit inside the line. I think the better answer for this question is to either standardize the data or avoid using a quadratic model for this problem, which was maybe the entire point of the question.\n\n\n4.2.18 4H5\nReturn to the cherry blossoms data and model the association between blossom date (doy) and March temperature (temp). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. How well does temperature trend predict the blossom trend?\nI’m leaning towards splines for this question – I expect this relationship to be nonlinear, and given my frustration with polynomial models in the previous question, I think it’s okay to stick with a spline. So first we need to set up the splines. We haven’t talked about missing data at all yet in this book so I’ll just use the complete cases.\n\ndata(\"cherry_blossoms\")\nd <- cherry_blossoms[ , c(\"temp\", \"doy\")]\nd2 <- d[complete.cases(d), ]\n\nLet’s do a prior predictive simulation. I’ll start with the priors in the book for the previous model, but then adjust if they look unreasonable. I looked at the paper cited in the cherry blossoms data description, and it appears that the temperatures were measured in Kyoto. So I googled “Kyoto march temperature” and I guess we can assume that temperatures should be between 3 and 14 degrees celsius. McElreath says that the flowers can bloom any time from March to May, so as long as our predictions are between day 60 (first day of March in regular year) and day 152 (first day of May in leap year), I’ll consider that to be “reasonable”.\n\nN <- 1000\nset.seed(100)\n\n# Create a range of temperature for the PPS splines\ntemp_range <- seq(2, 14, 0.05)\nnk <- 15\n\n# Create the splines\nB <-\n    splines::bs(\n        x = temp_range,\n        knots = quantile(temp_range, probs = seq(0, 1, length.out = nk))[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\n# Simulate a since it is easy\na <- rnorm(N, 100, 10)\n\n# Simulate the weights with a little trickery\nw <- do.call(rbind, purrr::map(1:ncol(B), ~rnorm(N, 0, 15)))\n\n# Calculate the predicted means\nmu <- a + B %*% w\n\n# Plot the pps\nlayout(1)\nplot(\n    NULL,\n    xlim = c(2, 14), ylim = c(40, 170),\n    xlab = \"temperature (Celsius)\", ylab = \"day of year\",\n    xaxs=\"i\", yaxs=\"i\"\n)\nabline(h =  60, lty = 2, lwd = 0.5)\nabline(h = 152, lty = 2, lwd = 0.5)\n\nfor (i in 1:N) {\n    lines(x = temp_range, y = mu[, i], col = col.alpha(\"black\", 0.05))\n}\n\n\n\n\nYeah, the default priors look pretty good to me. The only change I made was to increase the width of the prior for \\(w\\), to potentially allow for more wiggliness. There is a bit of wiggling out of the expected day of year range, but it’s so rare that I think it will be fine, especially after the model sees the data. I think it will be harder to set the correct prior on \\(w\\) with a prior predictive simulation, so I think we better go ahead and fit the model.\nNow let’s fit a quap model. Note that I had to turn up the maximum number of iterations run by the underlying optim call to ensure convergence criteria were met.\n\n# Create the real splines\nB_real <-\n    splines::bs(\n        x = d2$temp,\n        knots = quantile(temp_range, probs = seq(0, 1, length.out = nk))[-c(1, nk)],\n        degree = 3,\n        intercept = TRUE\n    )\n\n# Fit the model\nm4h6 <-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu <- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 15),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B_real),\n        start = list(w = rep(0, ncol(B_real))),\n        control = list(maxit = 200)\n    )\n\nrethinking::precis(m4h6, depth = 2)\n\n             mean         sd       5.5%      94.5%\nw[1]    0.0000000 14.9999514 -23.972819  23.972819\nw[2]    0.0000000 14.9999514 -23.972819  23.972819\nw[3]    0.0000000 14.9999514 -23.972819  23.972819\nw[4]    6.9549283  5.2048778  -1.363472  15.273328\nw[5]    4.7825332  4.8284596  -2.934278  12.499344\nw[6]    5.3724394  4.7117796  -2.157895  12.902773\nw[7]    1.1616676  4.6557218  -6.279075   8.602410\nw[8]   -0.1986208  4.8130958  -7.890877   7.493636\nw[9]   -3.4414421  5.1367824 -11.651012   4.768128\nw[10]  -2.3424900  5.7891404 -11.594655   6.909674\nw[11]  -6.1560988  7.2908763 -17.808327   5.496130\nw[12]  -0.4530242  6.7393765 -11.223849  10.317801\nw[13]   0.0000000 14.9999514 -23.972819  23.972819\nw[14]   0.0000000 14.9999514 -23.972819  23.972819\nw[15]   0.0000000 14.9999514 -23.972819  23.972819\nw[16]   0.0000000 14.9999514 -23.972819  23.972819\nw[17]   0.0000000 14.9999514 -23.972819  23.972819\na     102.5223225  4.5314729  95.280154 109.764491\nsigma   5.8829498  0.1474653   5.647272   6.118628\n\n\nAlright, now we can get the actual predictions and plot them.\n\nplot(d2$doy ~ d2$temp, xlab = \"Temperature (Celsius)\", ylab = \"Day of year\",\n         col = col.alpha(rangi2, 0.4), pch = 16)\n\n# Get the predictions and interval -- have to use the basis over the\n# interpolated temperature data!\nlink_m4h6 <- link(m4h6, data = list(B = B))\nmu <- apply(link_m4h6, 2, mean)\nmu_pi1 <- apply(link_m4h6, 2, PI, 0.97)\nmu_pi2 <- apply(link_m4h6, 2, PI, 0.89)\nmu_pi3 <- apply(link_m4h6, 2, PI, 0.61)\n\n# lines(temp_range, mu_pi3[1, ], lty = 2)\n# lines(temp_range, mu_pi3[2, ], lty = 2)\n# lines(temp_range, mu_pi2[1, ], lty = 2)\n# lines(temp_range, mu_pi2[2, ], lty = 2)\n# lines(temp_range, mu_pi1[1, ], lty = 2)\n# lines(temp_range, mu_pi1[2, ], lty = 2)\nshade(mu_pi1, temp_range, col = col.alpha(\"black\", 0.5))\nshade(mu_pi2, temp_range, col = col.alpha(\"darkgray\", 0.5))\nshade(mu_pi3, temp_range, col = col.alpha(\"gray\", 0.5))\nlines(temp_range, mu, lty = 2, col = \"red\")\n\n\n\n\nOK, so I had some confusion here with plotting the predictions. I forgot that I need to specify a separate \\(B\\) matrix of basis functions for the model fitting and for this part of the predictions. But now it is working and we can see that there is definitely a decent association between warmer temperatures and earlier blooming. However, we would need more high-temperature data to confirm this trend (and high-temperature data is something we wish we didn’t have more of, really).\n\n\n4.2.19 4H6\nQ. Simulate the prior predictive distirbution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weights is doing?\nOk, first we need to simulate the PPD. I’m pretty sure I already have a handle on this from the previous two spline exercises, but it should be fun. (In fact, we really already did this exact exercise, but whatever I guess.)\nLet’s simulate with maybe six different widths: 1, 5, 10 (chosen in the textbook), 15, 20, and 50. I’ll get the predictions from each of these simulations and then plot them.\n\n# Get ready for simulation.\n# Set parameters for sim\nwidths <- c(1, 5, 10, 15, 20, 50)\nN <- 250\nset.seed(100)\nlayout(matrix(c(1, 2, 3, 4, 5, 6), ncol = 3, nrow = 2))\n\n# Create the splits\nnk <- 15\nyear_range <- seq(800, 2010, 1)\nknot_list <- quantile(year_range, probs = seq(0, 1, length.out = nk))\nB <- splines::bs(\n                x = year_range,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\nsim_res <- vector(mode = \"list\", length = length(widths))\n\ndo_pps <- function(width) {\n    # Simulate a since it is easy\n    a <- rnorm(N, 100, 10)\n    \n    # Simulate the weights with a little trickery\n    w <- do.call(rbind, purrr::map(1:ncol(B), ~rnorm(N, 0, width)))\n    \n    # Calculate the predicted means\n    mu <- a + B %*% w\n    \n    plot(\n        NULL,\n        xlim = c(800, 2010), ylim = c(40, 170),\n        xlab = \"year\", ylab = \"day of year\",\n        xaxs=\"i\", yaxs=\"i\",\n        main = paste(\"Width:\", width)\n    )\n    \n    for (i in 1:N) {\n        lines(x = year_range, y = mu[, i], col = col.alpha(\"black\", 0.01))\n    }\n        \n    abline(h =  60, lty = 2, lwd = 0.5)\n    abline(h = 152, lty = 2, lwd = 0.5)\n    \n    return(NULL)\n}\n\nout <- sapply(widths, do_pps)\n\n\n\n\nYep, just like I said in the other two exercises about this, the width of the spline prior determines the amount of smoothness that the spline should have. If we increase the width, the spline is encouraged to vary more, and will “wiggle” (spike up and down) with larger fluctuations. If we constrain this parameter, the spline is encouraged to vary less, staying closer to the intercept.\n\n\n4.2.20 4H8\nQ. The cherry blossom split in the chapter used an intercept \\(\\alpha\\), but technically it doesn’t require one. The first basis functions could substitute for the intercept. Try refitting the cherry blossom spline without the intercept. What else about the model do you need to change to make this work.\nThis question is worded a bit confusingly to me. I assume he means the intercept parameter in the model, because the basis splines have a separate intercept that we also set to be true. Either way, let’s go ahead and try refitting a model without the intercept.\n\n# Data import\ndata(\"cherry_blossoms\")\nd <- cherry_blossoms\nd2 <- d[complete.cases(d$doy), ]\n\nset.seed(100)\n# Create the splits\nnk <- 15\nknot_list <- quantile(d2$year, probs = seq(0, 1, length.out = nk))\nB <- splines::bs(\n                x = d2$year,\n                # I think we are recreating the default knots and could set\n                # df = 13 instead, but nonetheless we persist\n                knots = knot_list[-c(1, nk)],\n                degree = 3,\n                intercept = TRUE\n            )\n\n# Fit the model\nm <-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu <- B %*% w,\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Get the posterior predictions and plot them\nmu <- link(m)\nmu_PI <- apply(mu, 2, PI, 0.97)\n\nlayout(1)\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\")\nshade(mu_PI, d2$year, col = col.alpha(\"darkgray\", 0.5))\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\n\n\n\n\nWell, I’ll be honest. I’m not really sure what else we’re supposed to change here because that model looks perfectly fine to me. It has some curvature at the lowest year values though, which we maybe should have expected since we basically set the intercept to zero. I think we could improve this by adjusting the prior for \\(w\\) to have a mean that was the same mean as the previous intercept prior (that is, 100). Maybe that’s the other thing we need to change. Let’s try it.\n\n# Save the previous results or whatever\nold_mu <- mu\nold_mu_PI <- mu_PI\n\n# Fit the model\nm <-\n    quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu <- B %*% w,\n            w ~ dnorm(100, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\n# Model with intercept\nma <-\n        quap(\n        flist = alist(\n            D ~ dnorm(mu, sigma),\n            mu <- a + B %*% w,\n            a ~ dnorm(100, 10),\n            w ~ dnorm(0, 10),\n            sigma ~ dexp(1)\n        ),\n        data = list(D = d2$doy, B = B),\n        start = list(w = rep(0, ncol(B)))\n    )\n\na_mu <- link(m)\na_mu_PI <- apply(mu, 2, PI, 0.97)\n\n\n# Get the posterior predictions and plot them\nmu <- link(m)\nmu_PI <- apply(mu, 2, PI, 0.97)\n\n# Plot the results with both lines\nplot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16,\n         xlab = \"year\", ylab = \"Day in year\")\nlines(d2$year, y = apply(a_mu, 2, mean), col = \"green\", lty = 2)\nlines(d2$year, y = apply(old_mu, 2, mean), col = \"blue\", lty = 2)\nlines(d2$year, y = apply(mu, 2, mean), col = \"red\", lty = 2)\nlegend(\n    x = \"top\",\n    legend = c(\"intercept\", \"no intercept mean 0\", \"no intercept mean 100\"),\n    lty = c(2, 2, 2),\n    col = c(\"green\", \"blue\", \"red\")\n)\n\n\n\n\nYes indeed, if you look closely, you can see that the red line and the green line are almost exactly the same. Which means that if we adjust the mean of the spline prior, we can fit the exact same model without an intercept. Yay."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "McElreath, Richard. 2020. Statistical Rethinking:\nA Bayesian Course with Examples in\nR and Stan. Second. New\nYork: Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608."
  },
  {
    "objectID": "cp5.html",
    "href": "cp5.html",
    "title": "5  The Many Variables and the Spurious Waffles",
    "section": "",
    "text": "This chapter discusses the “causal salad” issue that is really prevelant in epidemiology (and other sciences) right now. When you “adjust” for variables in models, what are you actually doing? What answers can you get from adjusting? How do you decide what variables should go into a model? We get to talk about confounding, which is one of my favorite subjects, and more generally, other types of biases. These are presented in the framework of graphical causal models using directed acyclic graphs (DAGs)."
  },
  {
    "objectID": "cp5.html#chapter-notes",
    "href": "cp5.html#chapter-notes",
    "title": "5  The Many Variables and the Spurious Waffles",
    "section": "5.1 Chapter notes",
    "text": "5.1 Chapter notes"
  },
  {
    "objectID": "cp5.html#exercises",
    "href": "cp5.html#exercises",
    "title": "5  The Many Variables and the Spurious Waffles",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\n5E1. The linear models\n\\[\\mu_i = \\beta_x x_i + \\beta_z z_i\\] and \\[\\mu_i = \\alpha + \\beta_x x_i + \\beta_z z_i\\]\nare both multiple regression models. The first model \\[\\mu_i = \\alpha + \\beta x_i\\] is a simple linear regression model and while the third model \\[\\left( \\mu_i = \\alpha + \\beta (x_i - z_i) \\right)\\] involves both \\(x\\) and \\(z\\), the model only has one coefficient and treats their difference as a single explanatory variable.\n5E2. We could evaluate the claim animal diversity is linearly related to latitude, but only after controlling for plant diversity using the linear model \\[\\begin{align*}\n\\text{animal diversity}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\mathrm{latitude} \\right) + \\beta_2 \\left( \\text{plant diversity} \\right)\n\\end{align*}\\]\nwhere suitable priors are assigned and other appropriate parameters are given for the likelihood function.\n5E3. We could evaluate the claim neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree using the multiple linear regression \\[\\begin{align*}\n\\text{time to PhD}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\text{amount of funding} \\right) + \\beta_2 \\left( \\text{size of laboratory} \\right)\n\\end{align*}\\]\nwith suitable priors, etc. The slope of both \\(\\beta_j\\) should be positive. Classically, I would probably be inclined to include an interaction term in this model, but we haven’t talked about that yet in the book so I didn’t.\n5E4. If we have a single categorical variable with levels \\(A,\\) \\(B,\\) \\(C,\\) and \\(D,\\) (represented as indicator variables), the following linear models are inferentially equivalent: \\[\\begin{align*}\n\\mu_i &= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha_A A_i + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i, \\quad \\text{ and }\\\\\n\\mu_i &= \\alpha_A \\left( 1 - B_i - C_i - D_i \\right) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i.\n\\end{align*}\\]\n5M1. An example of a spurious correlation: I am happy on days when it is sunny outside, and when I get to leave work early. Both of these things individually make me happy, but the weather doesn’t determine whether I get to leave work early. (At least not fully anyways. The weather definitely determines how much work I get done, but other external factors control the amount of work I have and the deadlines I need to meet.)\n5M2. An example of a masked relationship: fill this in when I think of one.\n5M3. I guess a higher divorce rate could cause a higher marriage rate by making more people available to be married. If divorced people tend to get remarried (potentially to non-divorced people), then the overall marriage rate could go up. Addressing this using a multiple linear regression model would be quite difficult, as you would need more data on remarriage and divorce status. It could be difficult to incorporate remarriages into the regression model, maybe an agent-based model would be more intuitive for this.\n5M4. I found this list of percent LDS population by state. So if we want to add this as a predictor to the divorce rate model, first I’ll join these data to the WaffleDivorce data from the rethinking package.\n\npct_lds <- readr::read_csv(here::here(\"static/pct_lds.csv\"))\n\nRows: 50 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): State\ndbl (3): mormonPop, mormonRate, Pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlibrary(rethinking)\n\nLoading required package: rstan\nLoading required package: StanHeaders\nLoading required package: ggplot2\nrstan (Version 2.21.5, GitRev: 2e1f913d3ca3)\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\nLoading required package: cmdstanr\nThis is cmdstanr version 0.5.3\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n- CmdStan path: C:/Users/Zane/Documents/.cmdstan/cmdstan-2.30.1\n- CmdStan version: 2.30.1\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\nLoading required package: parallel\nrethinking (Version 2.21)\n\nAttaching package: 'rethinking'\n\nThe following object is masked from 'package:rstan':\n\n    stan\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\ndata(\"WaffleDivorce\")\n\ndat_5m4 <-\n    dplyr::left_join(\n        WaffleDivorce,\n        pct_lds,\n        by = c(\"Location\" = \"State\")\n    )\n\ndplyr::glimpse(dat_5m4)\n\nRows: 50\nColumns: 16\n$ Location          <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"Califor…\n$ Loc               <fct> AL, AK, AZ, AR, CA, CO, CT, DE, DC, FL, GA, HI, ID, …\n$ Population        <dbl> 4.78, 0.71, 6.33, 2.92, 37.25, 5.03, 3.57, 0.90, 0.6…\n$ MedianAgeMarriage <dbl> 25.3, 25.2, 25.8, 24.3, 26.8, 25.7, 27.6, 26.6, 29.7…\n$ Marriage          <dbl> 20.2, 26.0, 20.3, 26.4, 19.1, 23.5, 17.1, 23.1, 17.7…\n$ Marriage.SE       <dbl> 1.27, 2.93, 0.98, 1.70, 0.39, 1.24, 1.06, 2.89, 2.53…\n$ Divorce           <dbl> 12.7, 12.5, 10.8, 13.5, 8.0, 11.6, 6.7, 8.9, 6.3, 8.…\n$ Divorce.SE        <dbl> 0.79, 2.05, 0.74, 1.22, 0.24, 0.94, 0.77, 1.39, 1.89…\n$ WaffleHouses      <int> 128, 0, 18, 41, 0, 11, 0, 3, 0, 133, 381, 0, 0, 2, 1…\n$ South             <int> 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1…\n$ Slaves1860        <int> 435080, 0, 0, 111115, 0, 0, 0, 1798, 0, 61745, 46219…\n$ Population1860    <int> 964201, 0, 0, 435450, 379994, 34277, 460147, 112216,…\n$ PropSlaves1860    <dbl> 4.5e-01, 0.0e+00, 0.0e+00, 2.6e-01, 0.0e+00, 0.0e+00…\n$ mormonPop         <dbl> 37765, 33495, 436521, 32307, 756507, 150059, 15946, …\n$ mormonRate        <dbl> 0.0074, 0.0454, 0.0598, 0.0107, 0.0189, 0.0253, 0.00…\n$ Pop               <dbl> 5073187, 738023, 7303398, 3030646, 39995077, 5922618…\n\n\nWe probably would want to consider transformations of this variable, but I’m lazy so I’ll just say logit transformation is probably fine and use that 😁. The model I’ll fit is\n\\[\\begin{align*}\n\\text{Divorce rate}_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot \\text{Marriage rate}_i + \\beta_2 \\cdot \\text{Median age at marriage}_i \\\\\n&\\quad\\quad + \\beta_3 \\cdot \\text{logit(Percent LDS)}_i \\\\\n\\beta_j &\\sim \\mathrm{Normal}(0, 2); \\quad j = \\{0, \\ldots, 3\\} \\\\\n\\sigma &\\sim \\mathrm{Exp}(1)\n\\end{align*}\\]\nwhere \\(i\\) indexes the states. I was also lazy and just chose some weakly uninformative priors, but if this were a problem that we were doing our best to solve with linear regression, it would be better to do a prior predictive simulation and figure out some better assumptions. Now we’ll fit th model with quap (quadratic approximation)."
  }
]