[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Rethinking notes and exercises",
    "section": "",
    "text": "For Summer 2022, I decided to read “Statistical Rethinking” by Richard McElreath (McElreath (2020)). This book contains my notes on the chapter and my solutions to the exercises. I typically don’t take very detailed notes, so those will likely only be what I consider the key points of the chapter or the parts that I find difficult to remember.\nI will attempt to complete every exercise, but if I get stuck for too long or I’m on some other deadline (see my now page to see what else I might be working on), that may not happen.\nMost of all, I plan to “have fun with it.”\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. New York: Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608."
  },
  {
    "objectID": "cp1.html",
    "href": "cp1.html",
    "title": "1  The Golem of Prague",
    "section": "",
    "text": "This chapter covers McElreath’s perspectives on statistical modeling and causal inference, and gives a description of how the book will address issues with current statistical practice and the tools that will be used."
  },
  {
    "objectID": "cp1.html#chapter-notes",
    "href": "cp1.html#chapter-notes",
    "title": "1  The Golem of Prague",
    "section": "1.1 Chapter notes",
    "text": "1.1 Chapter notes\n\nThe statistical model as a golem: models cannot think or see context. They are designed to do specific jobs and carry out their job exactly as they are instructed to, without regard for the consequences. Golems can make numbers, but we have to make golems and interpret the results.\nStatistics often lacks a coherent epistemiology. We need to understand how statistical models related to causal models, and how causal models relate to scientific hypotheses.\n“Folk Popperism:” many scientists believe that null hypothesis significance testing reflects Popper’s belief that scientific hypotheses must be falsifiable. But really, one statistical model is related to multiple process models, and one process model is related to many scientific hypotheses.\nAdditionally, the NHST paradigm often ignores the fallibility of measurements and the idea that falsification can be spurious. And continuous hypotheses cannot simply be falsified.\nMy favorite quote from this chapter (and it is full of great quotes): “So, if attempting to mimic falsification is not a genreally useful approach to statistical methods, what are we to do? We are to model.”\nThe rest of the chapter details the approaches that the book will take with respect to causal modeling and bayesian analysis methods.\nAn equally good title for this chapter might be “the statistical nihilist’s manifesto.”"
  },
  {
    "objectID": "cp1.html#exercises",
    "href": "cp1.html#exercises",
    "title": "1  The Golem of Prague",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\n(This space intentionally left blank.)"
  },
  {
    "objectID": "cp2.html",
    "href": "cp2.html",
    "title": "2  Small Worlds and Large Worlds",
    "section": "",
    "text": "In this chapter, the basics of Bayesian probability models and updating are described using a motivating example where the percent coverage of water on Earth is estimated by tossing an inflatable globe. This chapter covers the mechanical parts of Bayesian models."
  },
  {
    "objectID": "cp2.html#chapter-notes",
    "href": "cp2.html#chapter-notes",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.1 Chapter notes",
    "text": "2.1 Chapter notes\n\nDistinction between the small world, the self-contained world of the model, and the large world, the context in which the model is used. Remember that golems can’t see context.\nBayesian Inference Is Just Counting – explanation using tree diagrams. I think this is a good example for simple problems but I don’t think it generalizes well to real-life data analysis examples.\nBayesian updating: if you observe more data, you can use Bayes’ Rule to update your plausibilities for each of the possible results.\n“There is no free lunch…a Bayesian golem must choose an initial plausibility, and a frequentist golem must choose an estimator. Both golems pay for lunch with their assumptions.”\nBayesian inference doesn’t distinguish between data and parameters in the same way that frequentist inference does. Instead, data are observed variable, and parameters are unobserved variables.\nLikelihoods, priors, posteriors, Bayes’ rule, and other things I didn’t take notes on were covered in this section.\nThere are multiple numerical techniques for approximating Bayesian models, different “motors” for the golems. These include grid approximation, quadratic approximation, and Markov Chain Monte Carlo. How you fit the model is part of the model (the engine is part of the golem), and different fitting routines have different compromises and advantages.\nGrid approximation: estimate the posterior probability of several different values of the parameter via brute force. I did a rough version on this in my blog post on Bayesian updating.\n\nDefine the grid of posterior values. You have to choose the set of points for evaluation.\nCompute the value of the prior at each parameter value on the grid.\nCompute the likelihood at each parameter value.\nCompute the unstandardized posterior at each parameter value, by multiplying the prior and the likliehood.\nStandardize the posterior by dividing each value by the sum of all values.\n\nQuadratic approximation: as the number of parameters increases, the number of evaluations becomes \\(\\text{number of points} ^ \\text{number of parameters}\\). So more efficient methods (that make more assumptions are needed.) Quap assumes that the posterior is approximately Gaussian near the peak, essentially representing the log-posterior density as a quadratic function. N.b. quadratic approximation improves with the number of data points.\n\nFind the posterior mode, usually accomplished by some optimization algorithm based on the gradient of the posterior. “The golem does not know where the peak is, but it does know the slope under its feet.”\nEstimate the curvature near the peak, which is sufficient to compute a quadratic approximation of the entire posterior distribution.\n\nSee pp 41–43 for grid and quadratic approximation examples.\nMarkov chain Monte Carlo: useful for computing many models that fail for grid or quadratic approximation, and may have thousands of parameters. The final posterior may not even have a closed form. MCMC techniques rely on sampling from the posterior distribution rather than directly attempting to approximate the posterior. Since McElreath didn’t run his MCMC example in the book, I’ve included it here because I wanted to see the result.\n\n\nset.seed(370)\nn_samples <- 10000\np <- rep(NA, n_samples)\np[1] <- 0.5\nw <- 6\nl <- 3\nfor (i in 2:n_samples) {\n    p_new <- rnorm(1, p[i-1], 0.1)\n    if (p_new < 0) {p_new <- abs(p_new)}\n    if (p_new > 1) {p_new <- 2 - p_new}\n    q0 <- dbinom(w, w + l, p[i-1])\n    q1 <- dbinom(w, w + l, p_new)\n    p[i] <- ifelse(runif(1) < q1/q0, p_new, p[i-1])\n}\n\nplot(density(p), xlim = c(0, 1))\ncurve(dbeta(x, w + 1, l + 1), lty = 2, add = TRUE)"
  },
  {
    "objectID": "cp2.html#exercises",
    "href": "cp2.html#exercises",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n2E1. The expression corresponding to the statement the probability of rain on Monday is\n\\[\\text{Pr}(\\text{rain} \\mid \\text{Monday}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{Monday})}.\\]\n2E2. The statement corresponding to the expression\n\\[\\text{Pr}(\\text{Monday} \\mid \\text{rain})\\]\nis the probability that it is Monday, given that it is raining.\n2E3. The expression correspondonding to the statement the probability that it is Monday, given that it is raining is\n\\[ \\text{Pr}(\\text{Monday} \\mid \\text{rain}) = \\frac{\\text{Pr}(\\text{rain, Monday})}{\\text{Pr}(\\text{rain})} = \\frac{\\text{Pr}(\\text{rain} \\mid \\text{Monday})\\text{Pr}(\\text{Monday})}{\\text{Pr}(\\text{rain})}\\]\n2E4. Based on Bruno de Finetti’s statement “PROBABILITY DOES NOT EXIST,” the statement the probability of water is 0.7 from the earlier example is a statement about our beliefs. We know that there are several other factors underlying the globe-tossing experiment, but we cannot measure all of those factors, but sweeping them under the rug, we believe that about 70% of the time, our result should be water. The frequentist interpretation of this is as a long-run average probability, but in the bayesian interpretation, this is our prior belief for the next time we perform the experiment.\n2M1. Assuming a uniform prior for \\(p\\), compute the grid approximate posterior for each of the following sets of observations: W, W, W, W, W, W, L, and L, W, W, L, W, W, W.\n\n# Define a function that computes the grid-approximate posterior with uniform\n# prior for p given a sampled number of water and land tosses.\nglobe_post <- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid <- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior <- rep(1, times = length(p_grid))\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood <- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior <- likelihood * prior\n    \n    # Standardize the posterior\n    posterior <- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post(3, 0)\nglobe_post(3, 1)\nglobe_post(5, 2)\n\n\n\n\n2M2. Repeat the grid approximate calculations assuming a prior for \\(p\\) of the form\n\\[f(p) = \\begin{cases} 0, & p < 0.5 \\\\ k, & p \\geq 0.5\\end{cases}.\\]\nNote that for \\(\\int_0^1 f(p) \\ dp = 1,\\) we must have \\(k = 2\\).\n\nglobe_post_step_prior <- function(w, l) {\n    # Define the grid of points to evaluate\n    p_grid <- seq(from = 0, to = 1, by = 0.01)\n    \n    # Uniform prior on p: f(x) = 1 / (1 - 0) = 1 for all p\n    prior <- ifelse(p_grid < 0.5, 0, 1)\n    \n    # Compute the likelihood over the grid given the observed sample\n    likelihood <- dbinom(w, size = w + l, prob = p_grid)\n    \n    # Compute the unstandardized posterior\n    unstd.posterior <- likelihood * prior\n    \n    # Standardize the posterior\n    posterior <- unstd.posterior / sum(unstd.posterior)\n    \n    # Make the plot\n    plot(p_grid, posterior, type = \"b\", xlab = \"P(water)\",\n         ylab = \"Posterior probability\")\n    mtext(paste(length(p_grid), \"points:\", w, \"W,\", l, \"L\"))\n    \n    # Invisibly return posterior density estimate\n    invisible(posterior)\n}\n\npar(mfrow = c(1, 3))\nglobe_post_step_prior(3, 0)\nglobe_post_step_prior(3, 1)\nglobe_post_step_prior(5, 2)\n\n\n\n\n2M3. We want to compute \\(\\text{Pr}(\\text{Earth} \\mid \\text{land})\\) given the following information.\n\n\\(\\text{Pr}(\\text{land} \\mid \\text{Earth}) = 0.3\\)\n\\(\\text{Pr}(\\text{land} \\mid \\text{Mars}) = 1.0\\)\n\\(\\text{Pr}(\\text{Earth}) = \\text{Pr}(\\text{Mars}) = 0.5\\)\n\nWe can deduce that \\[\\begin{align*}\n\\text{Pr}(\\text{land}) &= \\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth}) + \\text{Pr}(\\text{land} \\mid \\text{Mars})\\cdot\\text{Pr}(\\text{Mars}) \\\\\n&= (0.3)(0.5) + (1.0)(0.5) = 0.65.\n\\end{align*}\\]\nSo we compute \\[\\begin{align*}\n\\text{Pr}(\\text{Earth} \\mid \\text{land}) &= \\frac{\\text{Pr}(\\text{land} \\mid \\text{Earth})\\cdot\\text{Pr}(\\text{Earth})}{\\text{Pr}(\\text{land})} \\\\\n&= \\frac{(0.3)(0.5)}{0.65} \\approx 0.23.\n\\end{align*}\\]\n2M4. We have a deck of three cards: one with two white sides, one with a black side and a white side, and one with two black sides. If we draw one card with the black side up, what is the probability that the other side is also black?\nWe can solve this by directly calculating the conditional probability.\n\\[\\begin{align*}\n\\text{Pr}(\\text{black down} \\mid \\text{black up}) &= \\frac{\\text{Pr}(\\text{black down}, \\text{black up})}{ \\text{Pr}(\\text{black up})} \\\\\n&= \\frac{1 / 3}{1 / 2} = \\frac{2}{3}.\n\\end{align*}\\]\nWe get \\(\\frac{1}{3}\\) for the joint probability since there are three cards, and only one of them has black on both sides. We get the individual probability of one black side being up as \\(\\frac{1}{2}\\) by noticing that there are 6 sides that could be facing up, and 3 of them are black sides.\nThe way that I think scales better to the rest of the problems in this section is by counting the number of ways to get this answer.\n\nIf the card we drew was white on both sides, there are 0 ways we could observe a black side facing up.\nIf the card we drew was white on one side, there is 1 way to observe a black side facing up.\nIf the card we drew was black on both sides, there are 2 ways to observe a black side facing up (it could be either side).\n\nSo out of the three possible ways to generate the situation we observed, two of them have a second black side on the bottom, giving us our \\(\\frac{2}{3}\\) probability.\n2M5. If we add an extra card with two black sides, we update our calculations.\n\nStill 0 ways if we draw the white/white card.\nThere’s still only 1 way to observe a black side facing up with a B/W card.\nHowever, there are now 4 different black sides we could observe facing up with a B/B card.\n\nSo out of 5 ways to observe a black side facing up, four of them have the other side black, giving us a \\(\\frac{4}{5}\\) probability.\n2M6. Now, we suppose that the black ink is heavy. For every one way to pull the B/B card, there are two ways to pull the B/W card and three ways to pull the W/W card.\nThe number of ways to get one black side up has not changed from problem 2M4. There’s still one way with a B/W card and 2 ways with a B/B card. However, now there are \\(2 \\times 1 = 2\\) ways to get the B/W card with the black side up, so the probability of the other side being black is now \\(\\frac{2}{4} = \\frac{1}{2}\\).\n2M7. Now suppose we draw one card and get a black side facing up, then we draw a second card and get a white side facing up. We want to find the probability that the first card was the B/B card.\n\nThere are two ways for a black side to face up if the first card is B/B (either side could be face up). If this is the case, there are three ways for the second card to show white (one way if it is B/W or two ways if it is W/W). So, there are \\(2 \\times 3 = 6\\) ways for us to observe what we did if this is true.\nIf the first card is B/W, there is only one way for a black side to be face up. Then, there are two ways for the second card to face up white (either side of the W/W card), giving \\(1 \\times 2 = 2\\) ways for our data to occur if this is the truth.\nThe W/W card cannot be the first card, the data we observed rules this out.\n\nTherefore, there is a \\(6 / 8 = 3 /4\\) probability that the first card is black on the bottom as well.\n2H1. There are two species of panda, A and B, that are equally likely in the wild. In species A, twins occur 10% of the time. In species B, twins occur 20% of the time. Both species only have twins or singleton infants. If a panda of unknown species gives birth to twins, what is the probability her next birth will also be twins?\nSo, the probability we are interested in is \\(P(\\text{twins} \\mid \\text{twins})\\). This is confusing, so I’ll say \\(P(\\text{twins}^* \\mid \\text{twins})\\).\nWe can calculate that \\[\\begin{align*}\nP(\\text{twins}) &= P(\\text{twins} \\mid A) P(A) + P(\\text{twins} \\mid B) P(B) \\\\\n&= (0.1)(0.5) + (0.2)(0.5) = 0.15.\n\\end{align*}\\]\nFrom the definition of conditional probability, we know that\n\\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{P(\\text{twins}^*, \\text{twins})}{P(\\text{twins})},\\]\nso it remains to calculate \\(P(\\text{twins}^*, \\text{twins})\\). If we assume that births are independent, we can calculate\n\\[\\begin{align*}\nP(\\text{twins}^*, \\text{twins}) &= P(\\text{twins}^*, \\text{twins} \\mid A)P(A) + P(\\text{twins}^*, \\text{twins} \\mid B)P(B) \\\\\n&= (0.1)^2(0.5) + (0.2)^2(0.5) = 0.025.\n\\end{align*}\\]\nThen,\n\\[P(\\text{twins}^* \\mid \\text{twins}) = \\frac{0.025}{0.15} = \\frac{1}{6}.\\]\nSo if a panda of unknown species gives birth to twins, the probability that her next birth will also be twins is \\(1/6\\), given the information we have.\n2H2. Now we want to find the probability that the panda is species A, given that she gave birth to twins, i.e. \\(P(A \\mid \\text{twins})\\). Recall that \\(P(\\text{twins}) = 0.15,\\) \\(P(\\text{twins} \\mid A) = 0.1,\\) and \\(P(A) = 0.5.\\) Then,\n\\[P(A \\mid \\text{twins}) = \\frac{(0.5)(0.1)}{0.15} = \\frac{1}{3}.\\]\n2H3. Suppose the panda has a second birth, a singleton infant. What is \\(P(A)\\) now?\nSince we’ve already estimated \\(P(A)\\) for the first birth, I’ll call this \\(P(A_\\text{prior})\\), instead of working out the entire conditional probability, we can update this estimate with our new information. What we want to calculate is\n\\[P(A_\\text{posterior}) = \\frac{P(A_\\text{prior}) P(\\text{singleton} \\mid A)}{P(\\text{singleton})}.\\]\nNow, the probability of having a singleton is mutually exclusive with the probability of having twins (since we know that these pandas never have more than twins), so \\[P(\\text{singleton} \\mid A) = 1 - 0.1 = 0.9. \\]\nWe get the singleton probability for pandas of species B in the same way. Next, we need to calculate the probability of a singleton, taking our prior probability that the panda is species A into account. We get\n\\[P(\\text{singleton}) = \\frac{1}{3} (0.9) + \\frac{2}{3} (0.8) = \\frac{5}{6}.\\]\nTherefore,\n\\[P(A_\\text{posterior}) = \\frac{(1/3)(0.9)}{5/6} = 0.36.\\]\n2H4. The new test for panda species has probabilities \\(P(\\text{test } A \\mid A) = 0.8\\) and \\(P(\\text{test } B \\mid B) = 0.65.\\) We want to know the probability that our panda is species A, given that her test result was A.\nFirst we will ignore our prior probability and calculate the probability that any random panda is species A given that they test A.\nWe calculate that \\(P(\\text{test } A \\mid B) = 1 - 0.65 = 0.35\\) and therefore that \\[\\begin{align*}\nP(\\text{test } A) &= P(A)P(\\text{test } A \\mid A) + P(B)P(\\text{test } B \\mid B) \\\\\n&= (0.5)(0.8) + (0.5)(0.35) = 0.575.\n\\end{align*}\\]\nThen,\n\\[P(A \\mid \\text{test } A) = \\frac{(0.5)(0.8)}{0.575} \\approx 0.6957.\\]\nNow, if we use our prior probability, \\(P(A_\\text{prior}) = 0.36\\), we instead get that\n\\[\\begin{align*}\nP(\\text{test } A) &= (0.36)(0.8) + (0.64)(0.35) = 0.512 \\quad\\text{and } \\\\\nP(A \\mid \\text{test } A) &= \\frac{(0.36)(0.8)}{0.512} \\approx 0.5612.\n\\end{align*}\\]"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "McElreath, Richard. 2020. Statistical Rethinking:\nA Bayesian Course with Examples in\nR and Stan. Second. New\nYork: Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608."
  },
  {
    "objectID": "cp3.html",
    "href": "cp3.html",
    "title": "3  Sampling the Imaginary",
    "section": "",
    "text": "This chapter discusses the basics of sampling – instead of directly approximate the density of the posterior distribution, we can draw samples from it. This seems silly for simple distributions, but scales to otherwise intractable problems. Once we have the samples, we can use those to estimate the posterior density."
  },
  {
    "objectID": "cp3.html#chapter-notes",
    "href": "cp3.html#chapter-notes",
    "title": "3  Sampling the Imaginary",
    "section": "3.1 Chapter notes",
    "text": "3.1 Chapter notes\n\n“Fetishizing precision to the fifth decimal place will not improve your science.”\nThe HDPI and PI methods for constructing credible intervals are similar for bell-shaped curves, but will be different for highly skewed curves where the mode and mean are different. “If the choice of interval affects the inference, you are better off plotting the entire posterior distribution.”\nThe HDPI also has higher simulation variance, that is, it needs more samples than the PI to arrive at a stable result.\nChoosing a point estimate, such as the mean, median, or mode (maximum a posteriori value) can be difficult.\nImportantly, different loss functions imply different estimates. The absolute loss function, \\(L(\\theta, \\hat{\\theta}) = \\left| \\theta - \\hat{\\theta} \\right|\\) is minimized by the median; the quadratic loss function, \\(L(\\theta, \\hat{\\theta}) = \\left( \\theta - \\hat{\\theta} \\right)^2\\), is minimized by the mean; and the 0-1 loss function (different for discrete and continuous problems) is minimized by the mode and corresponds to maximizing the posterior likelihood.\nWhile frequentist methods rely on sampling distributions and the (theoretical) physical act of random sampling, Bayesian models do not! The “sampling” we are doing here is small world sampling – our samples are from the model, we don’t expect them to be “real.”\nDummy data generated by the prior predictive distribution, the distribution of the parameters of interest using only the priors and not the data, help us build models. These simulations can tell us whether the priors are reasonable or not.\nOnce we update the model with the data, we can generate samples from the posterior predictive distribution. These samples can help us check how accurate the model is or how well it fit. This distribution is “honest” because it propagates the uncertainty embodied in the posterior distribution of the parameter of interest."
  },
  {
    "objectID": "cp3.html#exercises",
    "href": "cp3.html#exercises",
    "title": "3  Sampling the Imaginary",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\nFor the easy exercises, we need the following code given in the book.\n\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, times = 1000)\nlikelihood <- dbinom(6, size = 9, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\n3E1. How much posterior probability lies below \\(p = 0.2\\)?\n\nmean(samples <= 0.2)\n\n[1] 4e-04\n\n\n3E2. How much posterior probability lies below \\(p = 0.8\\)?\n\nmean(samples <= 0.8)\n\n[1] 0.8884\n\n\n3E3. How much posterior probability lies between \\(p = 0.2\\) and \\(p = 0.8\\).\nWe could calculate this directly.\n\nmean((samples >= 0.2) & (samples <= 0.8))\n\n[1] 0.888\n\n\nOr if we had stored the previous calculations, we could have used those instead.\n\nmean(samples <= 0.8) - mean(samples <= 0.2)\n\n[1] 0.888\n\n\n3E4. 20% of the posterior probability lies below which value of \\(p\\)?\n\nquantile(samples, probs = c(0.2))\n\n      20% \n0.5185185 \n\n\n3E5. 20% of the posterior probability lies above which value of \\(p\\)?\n\nquantile(samples, probs = c(0.8))\n\n      80% \n0.7557558 \n\n\n3E6. Which values of \\(p\\) contain the narrowest interval equal to 66% of the posterior probability?\n\nrethinking::HPDI(samples, prob = 0.66)\n\n    |0.66     0.66| \n0.5085085 0.7737738 \n\n\n3E7. Which values of \\(p\\) contain 66% of the posterior probability, assuming equal posterior probability both above and below the interval?\n\nrethinking::PI(samples, prob = 0.66)\n\n      17%       83% \n0.5025025 0.7697698 \n\n\n3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\n\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, times = 1000)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\n3M2. Draw 10000 samples from the grid approximate prior abnove. Then use the samples to calculate the 90% HDPI for \\(p\\).\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = 0.90)\n\n     |0.9      0.9| \n0.3343343 0.7217217 \n\n\n3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in \\(p\\). What is the probability of observing 8 water in 15 tosses?\n\nppc <- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.1499\n\n\n3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.\n\nppc2 <- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.1842\n\n\n3M5. Start over at 3M1, this time using a prior that is zero below \\(p = 0.5\\) and a constant above \\(p = 0.5\\).\nFirst we approximate the posterior and take samples.\n\n# I should name these different things but I am not going to.\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- ifelse(p_grid < 0.5, 0, 2)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nNow we’ll do the first posterior predictive check and estimate the probability. Note that the true probability (if \\(p = 0.7\\)) is \\(0.08113\\).\n\nppc <- rbinom(1e4, size = 15, prob = samples)\nmean(ppc == 8)\n\n[1] 0.163\n\n\nIt looks like this estimate is actually slightly worse with this prior than it was with the uniform prior. However, they are fairly similar.\nAnd the second check. Note that the true probability is \\(0.2668279\\)\n\nppc2 <- rbinom(1e4, size = 9, prob = samples)\nmean(ppc2 == 6)\n\n[1] 0.2353\n\n\nThis estimate is much closer to the true value than the previous estimate was. It seems that this prior allows us to more accurate estimate probabilities close to the true value (\\(p = 0.7\\)), but not near the lower boundary for the prior. We can examine the histogram.\n\nrethinking::simplehist(ppc)\n\n\n\n\nWe can see that the low values are extremely low, but so are the high values. We would expect the mode to be around 10 or 11, but since we observed 8 / 15, it makes sense that we get a higher estimate of the probability of this occurring than what we “know” is true.\n3M6. We want to construct a 99% percentile interval of the posterior distribution of \\(p\\) that is only 0.05 wide. How many times will we have to toss the globe to do this?\nTo me, this question seems phrased in the general, but I think it is impossible to answer in general. So we’ll do our best. First let’s look at the width of the current PI.\n\nrethinking::PI(samples, prob = 0.99) |> diff()\n\n     100% \n0.3243243 \n\n\nThat’s much larger than what we want, but we only tossed the ball 15 times. So we’ll need to do some simulating to solve this problem. I know this is not the “true” probably, but for the sake of keeping with this model, I’ll make sure all of our larger samples have (approximately) the same \\(8/15\\) probability of water.\nI’ll also continue using the flat prior. The answer to this question depends on both the “true” value of \\(p\\) and the prior that we used.\n\none_sim <- function(N) {\n    likelihood <- dbinom(floor(N * (8/15)), size = N, prob = p_grid)\n    posterior <- likelihood * prior\n    posterior <- posterior / sum(posterior)\n\n    set.seed(100)\n    samples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n    \n    out <- rethinking::PI(samples, prob = 0.99) |> diff()\n    return(out)\n}\n\nmy_n <- seq(from = 10, to = 5000, by = 10)\nsim_res <- purrr::map_dbl(my_n, one_sim)\nplot(sim_res ~ my_n, type = \"l\")\nabline(h = 0.05, col = \"red\", lty = 2)\n\n\n\n\nVisually, we can see that around 3000 samples are necessary, let’s get the exact estimate.\n\nindex <- min(which(sim_res < 0.05))\ncat(\"n: \", my_n[index], \"; width: \", sim_res[index], sep = \"\")\n\nn: 2740; width: 0.04905405"
  },
  {
    "objectID": "cp3.html#hard-problems",
    "href": "cp3.html#hard-problems",
    "title": "3  Sampling the Imaginary",
    "section": "3.3 Hard Problems",
    "text": "3.3 Hard Problems\nFor the hard problems, we need to load the indicated data set.\n\ndata(homeworkch3, package = \"rethinking\")\ncombined <- c(birth1, birth2)\n\n3H1. Use grid approximation to compute the posterior distribution for the probability of a birth being a boy.\n\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, times = 1000) # Uniform prior\nlikelihood <- dbinom(sum(combined), size = length(combined), prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nplot(posterior ~ p_grid, type = \"l\")\n\n\n\n\n3H2. Draw 10000 random samples from the posterior, and use these to estimate the 50, 89, and 97 percent HDPIs.\n\nset.seed(100)\nsamples <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)\n\nrethinking::HPDI(samples, prob = c(0.5, 0.89, 0.97))\n\n    |0.97     |0.89      |0.5      0.5|     0.89|     0.97| \n0.4824825 0.4994995 0.5265265 0.5725726 0.6076076 0.6296296 \n\n\n3H3. Simulate 10,000 replicates of 200 births. Compare the distribution of predicted counts to the actual count. Does it look like the model fits the data well?\n\nset.seed(100)\nppc <- rbinom(1e4, size = 200, prob = samples)\nrethinking::simplehist(ppc)\nabline(v = sum(combined), col = \"red\", lty = 2)\n\n\n\n\nIn this particular simulation, the observed value (111 boys) is a central, likely outcome of the posterior predictive distribution. The model seems to fit the data well, although there is a fairly large amount of spread.\n3H4. Now compare 10,000 counts of boys from 100 simulated firstborns only to the number of boys in the first births.\n\nset.seed(100)\nb1_samp <- rbinom(10000, size = 100, prob = samples)\nrethinking::simplehist(b1_samp)\nabline(v = sum(birth1), col = \"red\", lty = 2)\n\n\n\n\nThe model seems to overestimate the number of firstborn boys. This could potentially be because our observed count of boys is slightly higher than 50% (\\(0.56 \\%\\)) and this overestimation becomes more prominent in the smaller sample size. However, the true value is not in the tails of our distribution, so we would probably capture it in a CI.\n3H5. Our model assumes that sex of first and second births are independent. We can check this assumption by focusing on second births that followed female firstborns. Compare 10,000 simulated counts of boys to only those second births that followed girls.\n\n# Get the correct count\nn <- sum(birth2[birth1 == 0])\n\n# Run the simulation\n\nb2_samp <- rbinom(1e4, size = n, prob = samples)\n\n# Plot the results\nrethinking::simplehist(b2_samp, xlim = c(min(b2_samp), max(n, max(b2_samp))))\nabline(v = n, col = \"red\", lty = 2)\n\n\n\n\nWow, the number of boys who follow girls is much larger than our model predicts. Either our sample is far away from the “real” value (although this is really more of a frequentist notion), or more likely, the assumption of our model is wrong."
  }
]