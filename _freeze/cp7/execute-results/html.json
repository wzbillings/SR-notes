{
  "hash": "9a2fdf9487638d0f274e45ff8e9c5889",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Ulysses' Compass\n\nPredictive accuracy is the main focus of this chapter. The titular metaphor\n\"Ulysses' compass\" refers to the mythological hero Ulysses navigating the\npath between the two monsters Charybdis and Scylla, who each lived on\neither side of a narrow strait. Sailors attempting to avoid Charybdis would\nsail too close to Scylla, and vice versa. McElreath likens this to the scientist\nnavigating between underfitting and overfitting. Major topics of this chapter\ninclude out-of-sample predictive accuracy estimation via LOOCV, PSIS, and WAIC;\nregularizing priors; and the prediction/inference trade-off.\n\n## Chapter notes\n\n* When we think about Copernicus and the heliocentric model, we have to remember\nthat the geocentric model makes very good predictions, despite being wrong.\nModels that are completely causally wrong can make great predictions.\n* The principle of parsimony is often used to distinguish between models which\nmake good predictions, but even this is not always useful for us in science.\n* $R^2$ is a commonly used method for choosing the \"best\" regression model,\nbut even this is not correct. In general, overfitting and underfitting are\nboth dangerous and we must be wary of methods which can lead us to overfit.\nThe notion of overfitting and underfitting is related to the bias-variance\ntradeoff.\n* To construct useful measures in a Bayesian framework, we need to consider the\nentropy of our models -- that is, how much is our uncertainty reduced if we\nlearn an outcome? The information entropy is the function\n$$h(p) = -\\sum_{i=1}^n p_i \\cdot \\log(p_i).$$\n* We can relate the entropy of the model to the accuracy of our predictions\nusing the **Kullback-Leibler divergence**: the additional uncertainty induced by\nusing probabilities from one distribution to describe another. The divergence\nis given by\n$$D_{KL}(p, q) = \\sum_i p_i \\log \\left( \\frac{p_i}{q_i}\\right).$$\n* Using the divergence, or the related deviance,\nwe cannot estimate how close a model is to the truth. However, we can tell\nwhich of a set of models is closest to the truth, and how much better it is from\nthe others.\n* We estimate the deviance as $-2 \\times \\texttt{lppd}$ where $\\texttt{lppd}$ is\nthe *log pointwise predictive density*. The formula is omitted here but is on\npage 210 of the book.\n* Importantly, we cannot simply score models on the data used to fit the models,\nbecause this leads to overfitting.\n* We can often use regularizing priors, which are skeptical and try to prevent\nthe model from taking on extreme values, to improve our out-of-sample performance.\n* The traditional way to approximate out-of-sample error is by using\nCross-Validation, specifically Leave-One-Out CV (LOOCV).\n* Since LOOCV is computationally very expensive, we want to approximate it. One\nmethod is called Pareto-smoothed importance sampling (PSIS), and another is\ncalled the Widely Applicable Information Criterion (WAIC). See the text, section\n7.4 for details on both methods.\n* As a sidenote, this chapter discusses robust regression using a $t$ likelihood\ninstead of a Gaussian likelihood on page 233.\n* I really enjoyed the metaphor that \"if we search hard enough, we are bound\nto found a Curse of Tippicanoe\" -- if we torture the data enough, it will\nconfess.\n\n## Exercises\n\n### 7E1\n\nThe three motivating criteria which define information entropy are\n\n1. Entropy should be a continuous-valued function;\n1. As the size of the sample space increases, entropy should increase for\nevents that are equally likely; and\n1. If the entropy associated with the event $E_1$ is $h_1$ and the entropy\nassociated with the event $E_2$ is $h_2$, then the entropy associated with the\nevent $E_1 \\cup E_2$ should be $h_1 + h_2$.\n\n### 7E2\n\nIf a coin is weighted such that when the coin is flipped, the probability of\nheads is $70\\%$ is given by\n$$h = -\\left( 0.7 \\cdot \\log(0.7) + 0.3 \\cdot \\log(0.3) \\right) \\approx 0.61,$$\nbecause the only other possibility is that the coin lands on tails, which occurs\nwith probability $0.3$.\n\n### 7E3\n\nSuppose that a four-sided die is weighted so that each possible outcome occurs\nwith the frequency given in the following table.\n\n| _roll_ | _p_  |\n|--------|------|\n| 1      | 0.20 |\n| 2      | 0.25 |\n| 3      | 0.25 |\n| 4      | 0.30 |\n\nThe entropy is then\n$$h = -\\sum_{i = 1}^4 p_i \\cdot \\log p_i \\approx 1.38.$$\n\n### 7E4\n\nSuppose we have another 4-sided die when the sides 1, 2, and 3 occur equally\noften but the side 4 never occurs. If $X$ is the random variable representing\nthe result of the die roll, we calculate the entropy over the support of $X$,\nwhich is $S(X) = \\{1, 2, 3\\}$ and we leave the value of 4 out of the calculation\nentirely. The entropy is then\n\n$$h = -3\\left( \\frac{1}{3}\\cdot\\log\\frac{1}{3}\\right) \\approx 1.10.$$\n\n### 7M1\n\n<!-- END OF FILE -->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}