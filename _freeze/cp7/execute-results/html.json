{
  "hash": "d61d8fdcbc887033d29846d49468e77e",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Ulysses' Compass\n\nPredictive accuracy is the main focus of this chapter. The titular metaphor\n\"Ulysses' compass\" refers to the mythological hero Ulysses navigating the\npath between the two monsters Charybdis and Scylla, who each lived on\neither side of a narrow strait. Sailors attempting to avoid Charybdis would\nsail too close to Scylla, and vice versa. McElreath likens this to the scientist\nnavigating between underfitting and overfitting. Major topics of this chapter\ninclude out-of-sample predictive accuracy estimation via LOOCV, PSIS, and WAIC;\nregularizing priors; and the prediction/inference trade-off.\n\n## Chapter notes\n\n* When we think about Copernicus and the heliocentric model, we have to remember\nthat the geocentric model makes very good predictions, despite being wrong.\nModels that are completely causally wrong can make great predictions.\n* The principle of parsimony is often used to distinguish between models which\nmake good predictions, but even this is not always useful for us in science.\n* $R^2$ is a commonly used method for choosing the \"best\" regression model,\nbut even this is not correct. In general, overfitting and underfitting are\nboth dangerous and we must be wary of methods which can lead us to overfit.\nThe notion of overfitting and underfitting is related to the bias-variance\ntradeoff.\n* To construct useful measures in a Bayesian framework, we need to consider the\nentropy of our models -- that is, how much is our uncertainty reduced if we\nlearn an outcome? The information entropy is the function\n$$h(p) = -\\sum_{i=1}^n p_i \\cdot \\log(p_i).$$\n* We can relate the entropy of the model to the accuracy of our predictions\nusing the **Kullback-Leibler divergence**: the additional uncertainty induced by\nusing probabilities from one distribution to describe another. The divergence\nis given by\n$$D_{KL}(p, q) = \\sum_i p_i \\log \\left( \\frac{p_i}{q_i}\\right).$$\n* Using the divergence, or the related deviance,\nwe cannot estimate how close a model is to the truth. However, we can tell\nwhich of a set of models is closest to the truth, and how much better it is from\nthe others.\n* We estimate the deviance as $-2 \\times \\texttt{lppd}$ where $\\texttt{lppd}$ is\nthe *log pointwise predictive density*. The formula is omitted here but is on\npage 210 of the book.\n* Importantly, we cannot simply score models on the data used to fit the models,\nbecause this leads to overfitting.\n* We can often use regularizing priors, which are skeptical and try to prevent\nthe model from taking on extreme values, to improve our out-of-sample performance.\n* The traditional way to approximate out-of-sample error is by using\nCross-Validation, specifically Leave-One-Out CV (LOOCV).\n* Since LOOCV is computationally very expensive, we want to approximate it. One\nmethod is called Pareto-smoothed importance sampling (PSIS), and another is\ncalled the Widely Applicable Information Criterion (WAIC). See the text, section\n7.4 for details on both methods.\n* As a sidenote, this chapter discusses robust regression using a $t$ likelihood\ninstead of a Gaussian likelihood on page 233.\n* I really enjoyed the metaphor that \"if we search hard enough, we are bound\nto found a Curse of Tippicanoe\" -- if we torture the data enough, it will\nconfess.\n\n## Exercises\n\n### 7E1\n\nThe three motivating criteria which define information entropy are\n\n1. Entropy should be a continuous-valued function;\n1. As the size of the sample space increases, entropy should increase for\nevents that are equally likely; and\n1. If the entropy associated with the event $E_1$ is $h_1$ and the entropy\nassociated with the event $E_2$ is $h_2$, then the entropy associated with the\nevent $E_1 \\cup E_2$ should be $h_1 + h_2$.\n\n### 7E2\n\nIf a coin is weighted such that when the coin is flipped, the probability of\nheads is $70\\%$ is given by\n$$h = -\\left( 0.7 \\cdot \\log(0.7) + 0.3 \\cdot \\log(0.3) \\right) \\approx 0.61,$$\nbecause the only other possibility is that the coin lands on tails, which occurs\nwith probability $0.3$.\n\n### 7E3\n\nSuppose that a four-sided die is weighted so that each possible outcome occurs\nwith the frequency given in the following table.\n\n| _roll_ | _p_  |\n|--------|------|\n| 1      | 0.20 |\n| 2      | 0.25 |\n| 3      | 0.25 |\n| 4      | 0.30 |\n\nThe entropy is then\n$$h = -\\sum_{i = 1}^4 p_i \\cdot \\log p_i \\approx 1.38.$$\n\n### 7E4\n\nSuppose we have another 4-sided die when the sides 1, 2, and 3 occur equally\noften but the side 4 never occurs. If $X$ is the random variable representing\nthe result of the die roll, we calculate the entropy over the support of $X$,\nwhich is $S(X) = \\{1, 2, 3\\}$ and we leave the value of 4 out of the calculation\nentirely. The entropy is then\n\n$$h = -3\\left( \\frac{1}{3}\\cdot\\log\\frac{1}{3}\\right) \\approx 1.10.$$\n\n### 7M1\n\nThe definition of the AIC is\n$$\\mathrm{AIC} = -2(\\mathrm{lppd} - p),$$\nwhile the definition of the WAIC is\n$$\\mathrm{WAIC} = -2 \\left(\\mathrm{lppd} - \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right)\\right).$$\n\nBoth of these formulas involve comparing the lppd to some penalty term,\nwhich is more general for the WAIC than for the AIC. In order for the AIC\nto be similar to the WAIC, we need to make assumptions which lead to\nthe equivalence\n$$ p = \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right), $$\non average.\n\nAccording to the text, this will occur if we assume that the\npriors are flat (or are overwhelmed by the likelihood), the posterior\ndistribution is approximately multivariate Gaussian, and the sample size\nis much greater than the number of parameters. So for models with\ncomplicated, hierarchical likelihoods, which are common in actual research\nquestions, the AIC will likely not be a good approximation to the WAIC,\nand unfortunately the AIC gives us no diagnostic criteria to determine when\nit fails.\n\n### 7M2\n\nModel *selection* concerns selecting one model out of a group and\ndiscarding the others. Typically one would use the remaining model to make\ninferences or predictions. However, model *comparison* involves investigating\nthe differences between multiple models, including comparing the criterion\nvalues and comparing the estimates and predictions. When we choose to do\nselection rather than comparison, we lose all of the information encoded\nin the differences between the model -- knowing which differences in models\nchange criterion values and estimates is crucial information which can inform\nour understanding of the system.\n\n### 7M3\n\nWe need to fit models to the exact same set of data points in order to compare\nthem based on WAIC because changing the data points will change the WAIC even\nif nothing about the model changes. If we were to use the same number of data\npoints, but different sets of data, the WAIC will fluctuate based on properties\nof the data, and we could get different results when we compare the same\nmodels.\n\nThe penalty parameter of the WAIC also depends on the value of $N$, the number\nof data points. If we were to drop data points (for example, due to missing\ndata in some models, but not others), we would expect the WAIC to increase\n(become worse)\nbecause we have less information relative to the complexity of the models.\nConversely, if we increased the number of data points the WAIC could be\nbetter just because of that. We could then make an incorrect decision by\ncomparing the WAICs from models fit on different data.\n\n### 7M4\n\nAs the width of the priors decreases (i.e. the priors become more concentrated), the WAIC penalty term shrinks. The WAIC penalty term is based\non the variances of individual probability estimates across samples. As the\nwidth of a prior is narrowed, the model will tend to produce samples for\neach individual that are closer together on average and thus the penalty term\nwill decrease. However, since the `lppd` also changes when we change the\npriors we cannot say for sure whether this increases or decreases the overall\nWAIC.\n\n### 7M5\n\nWhen we use informative priors, we make the model more skeptical of extreme\nvalues in the data, and less trustworthy of values that would pull the model\naway from the priors. The data thus need to contain a large amount of evidence\nto make extreme values more likely in the posterior distribution. Under these\nconditions, the model is \"excited\" less by the training sample -- and thus,\nthe model fitting process is more robust to variations in the training sample\ndue to sample error. Ideally, the model will capture less of the noise in the\ndata while still capturing strong underlying trends, improving the performance\nof the model at explaining novel data from the same data generating process.\n\n### 7M6\n\nOverly informative priors, or in the worst case, degenerate priors, will\ndominate the model and prevent the model from learning from the data. If\na prior is too narrow, the data cannot provide enough evidence to move the\nmodel away from the priors. Such a model is so skeptical of the data that it\ndoes not pick up the noise from sampling variability in the data, nor does it\npick up any signal from the underlying trends either. Because the model has\nlearned nothing from the data, we could make predictions just as good by\nmaking up random numbers.\n\n### 7H1\n\n\n\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "cp7_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}