{
  "hash": "05fdc293eb5581b86882acfefe91566c",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Ulysses' Compass\n\n\n\n\n\nPredictive accuracy is the main focus of this chapter. The titular metaphor\n\"Ulysses' compass\" refers to the mythological hero Ulysses navigating the\npath between the two monsters Charybdis and Scylla, who each lived on\neither side of a narrow strait. Sailors attempting to avoid Charybdis would\nsail too close to Scylla, and vice versa. McElreath likens this to the scientist\nnavigating between underfitting and overfitting. Major topics of this chapter\ninclude out-of-sample predictive accuracy estimation via LOOCV, PSIS, and WAIC;\nregularizing priors; and the prediction/inference trade-off.\n\n## Chapter notes\n\n* When we think about Copernicus and the heliocentric model, we have to remember\nthat the geocentric model makes very good predictions, despite being wrong.\nModels that are completely causally wrong can make great predictions.\n* The principle of parsimony is often used to distinguish between models which\nmake good predictions, but even this is not always useful for us in science.\n* $R^2$ is a commonly used method for choosing the \"best\" regression model,\nbut even this is not correct. In general, overfitting and underfitting are\nboth dangerous and we must be wary of methods which can lead us to overfit.\nThe notion of overfitting and underfitting is related to the bias-variance\ntradeoff.\n* To construct useful measures in a Bayesian framework, we need to consider the\nentropy of our models -- that is, how much is our uncertainty reduced if we\nlearn an outcome? The information entropy is the function\n$$h(p) = -\\sum_{i=1}^n p_i \\cdot \\log(p_i).$$\n* We can relate the entropy of the model to the accuracy of our predictions\nusing the **Kullback-Leibler divergence**: the additional uncertainty induced by\nusing probabilities from one distribution to describe another. The divergence\nis given by\n$$D_{KL}(p, q) = \\sum_i p_i \\log \\left( \\frac{p_i}{q_i}\\right).$$\n* Using the divergence, or the related deviance,\nwe cannot estimate how close a model is to the truth. However, we can tell\nwhich of a set of models is closest to the truth, and how much better it is from\nthe others.\n* We estimate the deviance as $-2 \\times \\texttt{lppd}$ where $\\texttt{lppd}$ is\nthe *log pointwise predictive density*. The formula is omitted here but is on\npage 210 of the book.\n* Importantly, we cannot simply score models on the data used to fit the models,\nbecause this leads to overfitting.\n* We can often use regularizing priors, which are skeptical and try to prevent\nthe model from taking on extreme values, to improve our out-of-sample performance.\n* The traditional way to approximate out-of-sample error is by using\nCross-Validation, specifically Leave-One-Out CV (LOOCV).\n* Since LOOCV is computationally very expensive, we want to approximate it. One\nmethod is called Pareto-smoothed importance sampling (PSIS), and another is\ncalled the Widely Applicable Information Criterion (WAIC). See the text, section\n7.4 for details on both methods.\n* As a sidenote, this chapter discusses robust regression using a $t$ likelihood\ninstead of a Gaussian likelihood on page 233.\n* I really enjoyed the metaphor that \"if we search hard enough, we are bound\nto found a Curse of Tippicanoe\" -- if we torture the data enough, it will\nconfess.\n\n## Exercises\n\n### 7E1\n\nThe three motivating criteria which define information entropy are\n\n1. Entropy should be a continuous-valued function;\n1. As the size of the sample space increases, entropy should increase for\nevents that are equally likely; and\n1. If the entropy associated with the event $E_1$ is $h_1$ and the entropy\nassociated with the event $E_2$ is $h_2$, then the entropy associated with the\nevent $E_1 \\cup E_2$ should be $h_1 + h_2$.\n\n### 7E2\n\nIf a coin is weighted such that when the coin is flipped, the probability of\nheads is $70\\%$ is given by\n$$h = -\\left( 0.7 \\cdot \\log(0.7) + 0.3 \\cdot \\log(0.3) \\right) \\approx 0.61,$$\nbecause the only other possibility is that the coin lands on tails, which occurs\nwith probability $0.3$.\n\n### 7E3\n\nSuppose that a four-sided die is weighted so that each possible outcome occurs\nwith the frequency given in the following table.\n\n| _roll_ | _p_  |\n|--------|------|\n| 1      | 0.20 |\n| 2      | 0.25 |\n| 3      | 0.25 |\n| 4      | 0.30 |\n\nThe entropy is then\n$$h = -\\sum_{i = 1}^4 p_i \\cdot \\log p_i \\approx 1.38.$$\n\n### 7E4\n\nSuppose we have another 4-sided die when the sides 1, 2, and 3 occur equally\noften but the side 4 never occurs. If $X$ is the random variable representing\nthe result of the die roll, we calculate the entropy over the support of $X$,\nwhich is $S(X) = \\{1, 2, 3\\}$ and we leave the value of 4 out of the calculation\nentirely. The entropy is then\n\n$$h = -3\\left( \\frac{1}{3}\\cdot\\log\\frac{1}{3}\\right) \\approx 1.10.$$\n\n### 7M1\n\nThe definition of the AIC is\n$$\\mathrm{AIC} = -2(\\mathrm{lppd} - p),$$\nwhile the definition of the WAIC is\n$$\\mathrm{WAIC} = -2 \\left(\\mathrm{lppd} - \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right)\\right).$$\n\nBoth of these formulas involve comparing the lppd to some penalty term,\nwhich is more general for the WAIC than for the AIC. In order for the AIC\nto be similar to the WAIC, we need to make assumptions which lead to\nthe equivalence\n$$ p = \\sum_i \\mathrm{var}_{\\theta}\\left( \\log p(y_i\\mid\\theta)\\right), $$\non average.\n\nAccording to the text, this will occur if we assume that the\npriors are flat (or are overwhelmed by the likelihood), the posterior\ndistribution is approximately multivariate Gaussian, and the sample size\nis much greater than the number of parameters. So for models with\ncomplicated, hierarchical likelihoods, which are common in actual research\nquestions, the AIC will likely not be a good approximation to the WAIC,\nand unfortunately the AIC gives us no diagnostic criteria to determine when\nit fails.\n\n### 7M2\n\nModel *selection* concerns selecting one model out of a group and\ndiscarding the others. Typically one would use the remaining model to make\ninferences or predictions. However, model *comparison* involves investigating\nthe differences between multiple models, including comparing the criterion\nvalues and comparing the estimates and predictions. When we choose to do\nselection rather than comparison, we lose all of the information encoded\nin the differences between the model -- knowing which differences in models\nchange criterion values and estimates is crucial information which can inform\nour understanding of the system.\n\n### 7M3\n\nWe need to fit models to the exact same set of data points in order to compare\nthem based on WAIC because changing the data points will change the WAIC even\nif nothing about the model changes. If we were to use the same number of data\npoints, but different sets of data, the WAIC will fluctuate based on properties\nof the data, and we could get different results when we compare the same\nmodels.\n\nThe penalty parameter of the WAIC also depends on the value of $N$, the number\nof data points. If we were to drop data points (for example, due to missing\ndata in some models, but not others), we would expect the WAIC to increase\n(become worse)\nbecause we have less information relative to the complexity of the models.\nConversely, if we increased the number of data points the WAIC could be\nbetter just because of that. We could then make an incorrect decision by\ncomparing the WAICs from models fit on different data.\n\n### 7M4\n\nAs the width of the priors decreases (i.e. the priors become more concentrated), the WAIC penalty term shrinks. The WAIC penalty term is based\non the variances of individual probability estimates across samples. As the\nwidth of a prior is narrowed, the model will tend to produce samples for\neach individual that are closer together on average and thus the penalty term\nwill decrease. However, since the `lppd` also changes when we change the\npriors we cannot say for sure whether this increases or decreases the overall\nWAIC.\n\n### 7M5\n\nWhen we use informative priors, we make the model more skeptical of extreme\nvalues in the data, and less trustworthy of values that would pull the model\naway from the priors. The data thus need to contain a large amount of evidence\nto make extreme values more likely in the posterior distribution. Under these\nconditions, the model is \"excited\" less by the training sample -- and thus,\nthe model fitting process is more robust to variations in the training sample\ndue to sample error. Ideally, the model will capture less of the noise in the\ndata while still capturing strong underlying trends, improving the performance\nof the model at explaining novel data from the same data generating process.\n\n### 7M6\n\nOverly informative priors, or in the worst case, degenerate priors, will\ndominate the model and prevent the model from learning from the data. If\na prior is too narrow, the data cannot provide enough evidence to move the\nmodel away from the priors. Such a model is so skeptical of the data that it\ndoes not pick up the noise from sampling variability in the data, nor does it\npick up any signal from the underlying trends either. Because the model has\nlearned nothing from the data, we could make predictions just as good by\nmaking up random numbers.\n\n### 7H1\n\nFor this exercise, we want to fit a curve to the `Laffer` data. First let's\nload and plot the data. Note that I've gone ahead and standardized the\nindependent variable, tax rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Laffer)\nd <- list(\n\tx = standardize(Laffer$tax_rate),\n\ty = Laffer$tax_revenue\n)\n\nplot(\n\td,\n\txlab = \"Standardized tax rate\",\n\tylab = \"Tax revenue\"\n)\n```\n\n::: {.cell-output-display}\n![](cp7_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nFor this exercise, I'll fit three models to the data: a regular linear model, a\nquadratic polynomial model, and a cubic polynomial model. For a real research\nquestion, I would also probably consider a spline model, but that is too much\nwork for no payoff here. I messed around for a bit and tried to get a model like\nthe curve from the image in the book, but that is laughably wrong and impossible\nto fit, so it didn't work out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123123)\nm1 <- rethinking::quap(\n\talist(\n\t\ty ~ dnorm(mu, sigma),\n\t\tmu <- b0 + b1 * x,\n\t\tb0 ~ dnorm(0, 2),\n\t\tb1 ~ dnorm(0, 2),\n\t\tsigma ~ dexp(0.5)\n\t),\n\tdata = d\n)\nm2 <- rethinking::quap(\n\talist(\n\t\ty ~ dnorm(mu, sigma),\n\t\tmu <- b0 + b1 * x + b2 * I(x ^ 2),\n\t\tb0 ~ dnorm(0, 2),\n\t\tb1 ~ dnorm(0, 2),\n\t\tb2 ~ dnorm(0, 2),\n\t\tsigma ~ dexp(0.5)\n\t),\n\tdata = d\n)\nm3 <- rethinking::quap(\n\talist(\n\t\ty ~ dnorm(mu, sigma),\n\t\tmu <- b0 + b1 * x + b2 * I(x ^ 2) + b3 * I(x ^ 3),\n\t\tb0 ~ dnorm(0, 2),\n\t\tb1 ~ dnorm(0, 2),\n\t\tb2 ~ dnorm(0, 2),\n\t\tb3 ~ dnorm(0, 2),\n\t\tsigma ~ dexp(0.5)\n\t),\n\tdata = d\n)\n```\n:::\n\n\nThe first thing we probably want to do is plot the predictions of these\nmodels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_m <- attr(d$x, \"scaled:center\")\nx_s <- attr(d$x, \"scaled:scale\")\nx_vec <- list(x = seq(min(d$x), max(d$x), length.out = 500))\nx_tf <- x_vec$x * x_s + x_m\n\nplot(\n\tx = d$x * x_s + x_m,\n\ty = d$y,\n\txlab = \"Tax rate\",\n\tylab = \"Tax revenue\"\n)\n\nm1_post <- rethinking::link(m1, data = x_vec) |> colMeans()\nlines(\n\tx = x_tf, y = m1_post,\n\tlty = 1, col = \"black\",\n\tlwd = 2\n)\n\nm2_post <- rethinking::link(m2, data = x_vec) |> colMeans()\nlines(\n\tx = x_tf, y = m2_post,\n\tlty = 2, col = \"blue\",\n\tlwd = 2\n)\n\nm3_post <- rethinking::link(m3, data = x_vec) |> colMeans()\nlines(\n\tx = x_tf, y = m3_post,\n\tlty = 3, col = \"red\",\n\tlwd = 2\n)\n\nlegend(\n\tx = \"topleft\",\n\tlegend = c(\"Linear\", \"Quadratic\", \"Cubic\"),\n\tcol = c(\"black\", \"blue\", \"red\"),\n\tlty = c(1, 2, 3),\n\tlwd = c(2, 2, 2)\n)\n```\n\n::: {.cell-output-display}\n![](cp7_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nOK, so we can first of all see that none of those models are even anywhere\nclose to what that editorial showed. None of them really get pulled towards\nthe point with the high tax revenue, but we'll check that better in the\nnext question. The next thing that we need to do is compare the models. First\nlet's check the WAIC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrethinking::compare(m1, m2, m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       WAIC       SE      dWAIC      dSE    pWAIC    weight\nm2 124.6255 24.81799 0.00000000       NA 7.640306 0.4111828\nm1 124.6665 22.90424 0.04095811 2.739979 6.513822 0.4028478\nm3 126.2125 24.78288 1.58691188 1.341284 8.774990 0.1859694\n```\n:::\n:::\n\n\nAll of the models appear to perform about the same. So even though the linear\nmodel had technically the lowest WAIC, it's probably preferable to use that\nmodel since the gains in performance are smaller than the standard errors of\nthe WAIC estimates. Let's check PSIS as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrethinking::compare(m1, m2, m3, func = \"PSIS\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSome Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are very high (>1). Set pointwise=TRUE to inspect individual points.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n       PSIS       SE     dPSIS      dSE     pPSIS       weight\nm2 128.4209 28.46723  0.000000       NA  9.518310 0.6014412426\nm1 129.2479 27.64550  0.826972  2.08552  8.797377 0.3977576351\nm3 141.6631 38.33021 13.242141 10.10142 16.458178 0.0008011224\n```\n:::\n:::\n\n\nWe'll address the issues with the high Pareto $k$ values in the next question.\nFor now we can see that model 1, the linear model, has the lowest PSIS, although\nagain we can see that the standard errors are much higher than the differences\nin performance. But since we have few data points, adopting the simpler model\nwill likely be a better choice and it seems that there is some linear\nrelationship between tax revenue and tax rate. Let's look at the model summary\nfor `m1`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          mean        sd       5.5%    94.5%\nb0    3.228658 0.3069237 2.73813453 3.719181\nb1    0.566714 0.3116142 0.06869426 1.064734\nsigma 1.669183 0.2150402 1.32550725 2.012859\n```\n:::\n:::\n\n\nWe see that, on average, we would expect for a 1 standard deviation increase\nin the tax rate, we would expect the tax revenue to increase by 1 unit, whatever\nthose units are (I didn't look them up). Almost all of the probability mass\nis above zero, so even if the quantitative effect value is somewhat larger\nor stronger, we can be reasonably confident that increasing the tax rate will\nincrease the tax revenue by some amount.\n\n### 7H2\n\nThis question tells us that there is an outlier, but based on the previous\nPareto $k$ values that I already glanced at (not all listed out here), that\nseems to be correct. Point #12 is identified as incredibly influential in all\nthree models (along with point 1 for the quadratic model, giving us another\nreason to trust that model less than the linear model regardless of what the\nWAIC tells us). Because of the outlier, we'll refit the linear model using\na Student's $t$ regression and see what happens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm4 <- rethinking::quap(\n\talist(\n\t\ty ~ dstudent(2, mu, sigma),\n\t\tmu <- b0 + b1 * x,\n\t\tb0 ~ dnorm(0, 2),\n\t\tb1 ~ dnorm(0, 2),\n\t\tsigma ~ dexp(0.5)\n\t),\n\tdata = d\n)\n\nplot(\n\tx = d$x * x_s + x_m,\n\ty = d$y,\n\txlab = \"Tax rate\",\n\tylab = \"Tax revenue\"\n)\n\nm4_post <- rethinking::link(m4, data = x_vec) |> colMeans()\nlines(\n\tx = x_tf, y = m1_post,\n\tlty = 1, col = \"black\",\n\tlwd = 2\n)\nlines(\n\tx = x_tf, y = m4_post,\n\tlty = 2, col = \"firebrick3\",\n\tlwd = 2\n)\nlegend(\n\tx = \"topleft\",\n\tlegend = c(\"Gaussian\", \"T(2 d.f.)\"),\n\tlwd = c(2, 2),\n\tlty = c(1, 2),\n\tcol = c(\"black\", \"firebrick3\")\n)\n```\n\n::: {.cell-output-display}\n![](cp7_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nInterestingly, we can see that if we use a Student's $t$ likelihood which is\nless susceptible to the influence of outliers, the line is flatter, reflecting\nthe fact that the outlier was dragging the slope upwards. Excluding this\noutlier, which has an unusually high tax revenue compared to its tax rate,\ngives us a line that matches the rest of the points better.\n\nOf course, this change is not too dramatic. Let's look at the summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           mean        sd       5.5%    94.5%\nb0    2.8746595 0.1931870 2.56590941 3.183410\nb1    0.3869709 0.2346150 0.01201094 0.761931\nsigma 0.8011000 0.1689198 0.53113354 1.071067\n```\n:::\n:::\n\n\nThe change in the coefficient is within the standard error of that coefficient\nfrom the first model, so as long as we don't fall victim to pointeffectism (\nAKA point-estimate-is-the-effect syndrome), this doesn't change our conclusions\nreally at all. This is because while that point is an outlier, it is not\nincredibly influential. I.e., in standard frequentist linear regression theory,\nthe leverage of that point would not be exceptionally high, indicating that it\ndoes not have that much power to influence the coefficient estimates on its\nown. An outlier with a more extreme $x$ value would influence the estimates\nmore.\n\n### 7H3\n\nThe first thing we need to do for this problem is type in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbirds <- matrix(\n\tc(0.2, 0.8, 0.05, 0.2, 0.1, 0.15, 0.2, 0.05, 0.7, 0.2, 0.025, 0.05,\n\t\t0.2, 0.025, 0.05),\n\tncol = 5,\n\tnrow = 3,\n\tdimnames = list(\n\t\tpaste(\"Island\", 1:3),\n\t\tpaste(\"Species\", LETTERS[1:5])\n\t)\n)\nbirds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Species A Species B Species C Species D Species E\nIsland 1      0.20      0.20      0.20     0.200     0.200\nIsland 2      0.80      0.10      0.05     0.025     0.025\nIsland 3      0.05      0.15      0.70     0.050     0.050\n```\n:::\n:::\n\n\nNext, we want to compute the entropy of each island's bird distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh <- apply(birds, 1, \\(p) -sum(p * log(p)))\nround(h, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIsland 1 Island 2 Island 3 \n    1.61     0.74     0.98 \n```\n:::\n:::\n\n\nIsland 1 has the highest entropy, followed by Island 3, and finally Island 2.\nThis is because Island 1 has an even distribution of birds, which gives us\nthe maximum possible entropy of an island with five bird species. We can\nthink of this intuitively as a measurement of our uncertainty in which type of\nbird we will see. If we see a random bird on Island 1, there is an equally\nlikely chance for it to be any of the species, so any guess we make should\nbe quite uncertain.\n\nHowever, on Island 2 one scecies represents 80\\% of birds on the island,\nand similar on Island 3, one species represents 70\\% of birds on the island.\nSo if we saw a random bird on Island 3, we could make a decent guess about the\nspecies, and an even stronger guess on Island 2. So the decreasing entropies\nreflect our lowered uncertainty about the type of bird we guess we might see.\n\nNext we want to compute the KL divergence between each pair of islands.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkl <- function(p, q) {return(sum(p * (log(p) - log(q))))}\n\npairs <-\n\ttidyr::expand_grid(p = 1:3, q = 1:3) |>\n\tdplyr::filter(p != q) |>\n\tdplyr::mutate(\n\t\tdiv = purrr::map2_dbl(p, q, \\(x, y) kl(birds[x, ], birds[y, ]))\n\t)\n\npairs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n      p     q   div\n  <int> <int> <dbl>\n1     1     2 0.970\n2     1     3 0.639\n3     2     1 0.866\n4     2     3 2.01 \n5     3     1 0.626\n6     3     2 1.84 \n```\n:::\n:::\n\n\nThese are the KL divergence values if we used the distribution of island $p$\nto predict the distribution of island $q$. We see that it's very difficult\nto predict island 2 from island 3 and vice versa. Predictions using Island 1\nare always much better, regardless of the direction. However, it is much easier\nto predict Island 3 from Island 1 and vice versa, then trying to predict Island\n1 using Island 2 (or vice versa). This is because Island 1 has the maximum\nentropy, so reflects the maximum amount of uncertainty in our predictions --\ntherefore using Island 1 is the safest way to make a prediction in any other\nisland, because we are expressing the highest amount of uncertainty that we\ncan in this situation.\n\nIsland 3 and 1 are more compatible in predictions than\nIsland 2 and 1 because the difference in entropy between Islands 1 and 3 is\nsmaller than between Islands 1 and 2. Islands 2 and 3 are almost completely\ndominated by 2 bird species each, but the dominant species are different\nbetween the islands, so using one to predict the other is very wrong. That is,\nwe are using one distribution with extreme values to predict another\ndistribution with different extreme values, so our predictions are more wrong\non average.\n\n### 7H4\n\nOK, the first thing we need to do is type in all that code from the book\nto recreate the models, so I'll do that without comments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- sim_happiness(seed = 1977, N_years = 1000)\nd2 <- d[d$age > 17, ]\nd2$A <- (d2$age - 18) / (65 - 18)\nd2$mid <- d2$married + 1\n\nm6.9 <- quap(\n\talist(\n\t\thappiness ~ dnorm(mu, sigma),\n\t\tmu <- a[mid] + bA * A,\n\t\ta[mid] ~ dnorm(0, 1),\n\t\tbA ~ dnorm(0, 2),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = d2\n)\n\nm6.10 <- quap(\n\talist(\n\t\thappiness ~ dnorm(mu, sigma),\n\t\tmu <- a + bA * A,\n\t\ta ~ dnorm(0, 1),\n\t\tbA ~ dnorm(0, 2),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = d2\n)\n```\n:::\n\n\nSince McElreath says that WAIC and PSIS produce identical results here, we'll\ngo ahead and use WAIC since it's like one second faster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrethinking::compare(m6.9, m6.10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          WAIC       SE    dWAIC      dSE    pWAIC       weight\nm6.9  2713.971 37.54465   0.0000       NA 3.738532 1.000000e+00\nm6.10 3101.906 27.74379 387.9347 35.40032 2.340445 5.768312e-85\n```\n:::\n:::\n\n\nWe can see that the WAIC for m6.9 is much lower than the WAIC for m6.10, even\nif we consider the magnitude of the standard errors -- the WAICs are multiple\nstandard errors apart, so m6.9 should do a better job at generating predictions\nthan m6.10. Let's look at the model parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(m6.9, depth = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            mean         sd       5.5%      94.5%\na[1]  -0.2350877 0.06348986 -0.3365568 -0.1336186\na[2]   1.2585517 0.08495989  1.1227694  1.3943340\nbA    -0.7490274 0.11320112 -0.9299447 -0.5681102\nsigma  0.9897080 0.02255800  0.9536559  1.0257600\n```\n:::\n\n```{.r .cell-code}\nprecis(m6.10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               mean         sd       5.5%     94.5%\na      1.649248e-07 0.07675015 -0.1226614 0.1226617\nbA    -2.728620e-07 0.13225976 -0.2113769 0.2113764\nsigma  1.213188e+00 0.02766080  1.1689803 1.2573949\n```\n:::\n:::\n\n\nOf course, we know from the previous chapter than `m6.9` is **conditioning\non a collider**! And `m6.10` produces the correct causal inference. However,\nwe have to remember the crucial fact that **colliders and confounders contain\ninformation**. As McElreath says earlier in this chapter, \"highly confounded\nmodels can still make good predictions, at least in the short term.\" So\nthe model that makes the wrong causal conclusion has better predictive accuracy\nthan the correct model, but this should not surprise us too much -- this is\nwhy building a causal model is so important.\n\n### 7H5\n\nFor this exercise, we'll go back to the foxes data. We have five models\nthat we need to fit, using the fox weight as the outcome. I'll go ahead and\nfit those models in the order indicated in the question. I did the same data\nprocessing as in the previous chapter, taking the log of the outcome and then\nstandardizing all of the variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(987865)\ndata(foxes)\nf2 <-\n\tfoxes |>\n\tdplyr::transmute(\n\t\tA = area,\n\t\tF = avgfood,\n\t\tG = groupsize,\n\t\tW = log(weight)\n\t) |>\n\tas.list() |>\n\tlapply(FUN = rethinking::standardize)\n\nmf.1 <- rethinking::quap(\n\talist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- b0 + bF * F + bG * G + bA * A,\n\t\tb0 ~ dnorm(0, 2),\n\t\tbF ~ dnorm(0, 2),\n\t\tbG ~ dnorm(0, 2),\n\t\tbA ~ dnorm(0, 2),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = f2\n)\n\nmf.2 <- rethinking::quap(\n\talist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- b0 + bF * F + bG * G,\n\t\tb0 ~ dnorm(0, 2),\n\t\tbF ~ dnorm(0, 2),\n\t\tbG ~ dnorm(0, 2),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = f2\n)\n\nmf.3 <- rethinking::quap(\n\talist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- b0 + bG * G + bA * A,\n\t\tb0 ~ dnorm(0, 2),\n\t\tbG ~ dnorm(0, 2),\n\t\tbA ~ dnorm(0, 2),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = f2\n)\n\nmf.4 <- rethinking::quap(\n\talist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- b0 + bF * F,\n\t\tb0 ~ dnorm(0, 2),\n\t\tbF ~ dnorm(0, 2),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = f2\n)\n\nmf.5 <- rethinking::quap(\n\talist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- b0 + bA * A,\n\t\tb0 ~ dnorm(0, 2),\n\t\tbA ~ dnorm(0, 2),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = f2\n)\n\nrethinking::compare(mf.1, mf.2, mf.3, mf.4, mf.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         WAIC       SE    dWAIC      dSE    pWAIC      weight\nmf.3 324.7503 17.91089 0.000000       NA 4.320544 0.481904455\nmf.1 325.8046 19.33267 1.054305 3.831498 6.108581 0.284460254\nmf.2 326.2658 19.75153 1.515478 7.641027 4.583343 0.225880692\nmf.4 334.3019 16.42091 9.551600 7.090994 2.946806 0.004063099\nmf.5 334.4937 16.14821 9.743426 6.923563 3.212804 0.003691501\n```\n:::\n:::\n\n\nOk, so the main thing that I can see is that Model 1, 2, and 3 are all similar,\nand model 4 and 5 are similar. But following McElreath's notion to exam\nthe standard error of the differences, we can see that all the models\nare actually not too different. However, we can see that model 1 and model 3\nare quite similar, whereas the others have larger standard errors.\n\nIf we look at the plot we can definitely see two groups of models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(rethinking::compare(mf.1, mf.2, mf.3, mf.4, mf.5))\n```\n\n::: {.cell-output-display}\n![](cp7_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nIf we look back at the DAG, maybe we can understand why there are two groups\nof models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfox_dag <-\n\tdagitty::dagitty(\n\t\t\"dag {\n\t\tA -> F -> G -> W\n\t\tF -> W\n\t\t}\"\n\t)\ndagitty::coordinates(fox_dag) <-\n\tlist(\n\t\tx = c(A = 2, F = 1, G = 3, W = 2),\n\t\ty = c(A = 1, F = 2, G = 2, W = 3)\n\t)\nplot(fox_dag)\n```\n\n::: {.cell-output-display}\n![](cp7_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nSo the groups that are similar are:\n1. (avgfood, groupsize, area) and (avgfood, groupsize) and (groupsize, area);\n2. (avgfood only) and (area only).\n\nSo the main difference between the models is the inclusion of the `groupsize`\nvariable, $G$ in the DAG. We can see that $G$ is a mediator of the relatioship\nbetween $F$ and $W$ here.\n\nWhen we fit only `avgfood` or only `area`, it makes sense that the two models\nare the same -- the effect of `area` is entirely through its effect on\n`avgfood`, so those two models are essentially finding the same effect.\n\nHowever, when we include the mediator we get slightly different results.\nInterestingly, we're still only capturing one causal effect, because the\neffect of everything else comes from `area`. But it seems that the predictive\nmodel is better when we control for `groupsize` -- why would this be, if we\nare still only capturing descendants of `area`? I think that this is because,\nas we also know, conditioning on a variable which is the ancestor of the \noutcome can lead to\nincreased precision in our estimates. So even though we aren't getting any\n\"new\" signal here, and all of the models with `groupsize` in them perform\nsimilarly, we get slightly more efficient estimates, which can increase\nour predictive accuracy on average. It may also be the case that estimating\nthe effect of the `groupsize` is a statistical issue -- this variable is\ninteger valued, and we're modeling it like a continuous value, but because\nwe only have a discrete set of actual observations, this may reduce the\nprecision of our estimate, since there's a lot of \"space\" in the groupsize-axis\nthat isn't covered by any measurements.\n\nWe can also see that model 3 had the best WAIC overall, and it specifically\nincludes the two variables which are direct ancestors of the treatment,\nso maybe that supports my idea or maybe it is a coincidence, I'm not too sure.\n\nAnyways, I think this is mostly a statistical phenomenon. There are no\nconfounders or colliders here, everything is just direct or indirect effects\nof the effect of area on weight.\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "cp7_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}