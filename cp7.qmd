---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
library(rethinking)
```

# Ulysses' Compass

Predictive accuracy is the main focus of this chapter. The titular metaphor
"Ulysses' compass" refers to the mythological hero Ulysses navigating the
path between the two monsters Charybdis and Scylla, who each lived on
either side of a narrow strait. Sailors attempting to avoid Charybdis would
sail too close to Scylla, and vice versa. McElreath likens this to the scientist
navigating between underfitting and overfitting. Major topics of this chapter
include out-of-sample predictive accuracy estimation via LOOCV, PSIS, and WAIC;
regularizing priors; and the prediction/inference trade-off.

## Chapter notes

* When we think about Copernicus and the heliocentric model, we have to remember
that the geocentric model makes very good predictions, despite being wrong.
Models that are completely causally wrong can make great predictions.
* The principle of parsimony is often used to distinguish between models which
make good predictions, but even this is not always useful for us in science.
* $R^2$ is a commonly used method for choosing the "best" regression model,
but even this is not correct. In general, overfitting and underfitting are
both dangerous and we must be wary of methods which can lead us to overfit.
The notion of overfitting and underfitting is related to the bias-variance
tradeoff.
* To construct useful measures in a Bayesian framework, we need to consider the
entropy of our models -- that is, how much is our uncertainty reduced if we
learn an outcome? The information entropy is the function
$$h(p) = -\sum_{i=1}^n p_i \cdot \log(p_i).$$
* We can relate the entropy of the model to the accuracy of our predictions
using the **Kullback-Leibler divergence**: the additional uncertainty induced by
using probabilities from one distribution to describe another. The divergence
is given by
$$D_{KL}(p, q) = \sum_i p_i \log \left( \frac{p_i}{q_i}\right).$$
* Using the divergence, or the related deviance,
we cannot estimate how close a model is to the truth. However, we can tell
which of a set of models is closest to the truth, and how much better it is from
the others.
* We estimate the deviance as $-2 \times \texttt{lppd}$ where $\texttt{lppd}$ is
the *log pointwise predictive density*. The formula is omitted here but is on
page 210 of the book.
* Importantly, we cannot simply score models on the data used to fit the models,
because this leads to overfitting.
* We can often use regularizing priors, which are skeptical and try to prevent
the model from taking on extreme values, to improve our out-of-sample performance.
* The traditional way to approximate out-of-sample error is by using
Cross-Validation, specifically Leave-One-Out CV (LOOCV).
* Since LOOCV is computationally very expensive, we want to approximate it. One
method is called Pareto-smoothed importance sampling (PSIS), and another is
called the Widely Applicable Information Criterion (WAIC). See the text, section
7.4 for details on both methods.
* As a sidenote, this chapter discusses robust regression using a $t$ likelihood
instead of a Gaussian likelihood on page 233.
* I really enjoyed the metaphor that "if we search hard enough, we are bound
to found a Curse of Tippicanoe" -- if we torture the data enough, it will
confess.

## Exercises

### 7E1

The three motivating criteria which define information entropy are

1. Entropy should be a continuous-valued function;
1. As the size of the sample space increases, entropy should increase for
events that are equally likely; and
1. If the entropy associated with the event $E_1$ is $h_1$ and the entropy
associated with the event $E_2$ is $h_2$, then the entropy associated with the
event $E_1 \cup E_2$ should be $h_1 + h_2$.

### 7E2

If a coin is weighted such that when the coin is flipped, the probability of
heads is $70\%$ is given by
$$h = -\left( 0.7 \cdot \log(0.7) + 0.3 \cdot \log(0.3) \right) \approx 0.61,$$
because the only other possibility is that the coin lands on tails, which occurs
with probability $0.3$.

### 7E3

Suppose that a four-sided die is weighted so that each possible outcome occurs
with the frequency given in the following table.

| _roll_ | _p_  |
|--------|------|
| 1      | 0.20 |
| 2      | 0.25 |
| 3      | 0.25 |
| 4      | 0.30 |

The entropy is then
$$h = -\sum_{i = 1}^4 p_i \cdot \log p_i \approx 1.38.$$

### 7E4

Suppose we have another 4-sided die when the sides 1, 2, and 3 occur equally
often but the side 4 never occurs. If $X$ is the random variable representing
the result of the die roll, we calculate the entropy over the support of $X$,
which is $S(X) = \{1, 2, 3\}$ and we leave the value of 4 out of the calculation
entirely. The entropy is then

$$h = -3\left( \frac{1}{3}\cdot\log\frac{1}{3}\right) \approx 1.10.$$

### 7M1

The definition of the AIC is
$$\mathrm{AIC} = -2(\mathrm{lppd} - p),$$
while the definition of the WAIC is
$$\mathrm{WAIC} = -2 \left(\mathrm{lppd} - \sum_i \mathrm{var}_{\theta}\left( \log p(y_i\mid\theta)\right)\right).$$

Both of these formulas involve comparing the lppd to some penalty term,
which is more general for the WAIC than for the AIC. In order for the AIC
to be similar to the WAIC, we need to make assumptions which lead to
the equivalence
$$ p = \sum_i \mathrm{var}_{\theta}\left( \log p(y_i\mid\theta)\right), $$
on average.

According to the text, this will occur if we assume that the
priors are flat (or are overwhelmed by the likelihood), the posterior
distribution is approximately multivariate Gaussian, and the sample size
is much greater than the number of parameters. So for models with
complicated, hierarchical likelihoods, which are common in actual research
questions, the AIC will likely not be a good approximation to the WAIC,
and unfortunately the AIC gives us no diagnostic criteria to determine when
it fails.

### 7M2

Model *selection* concerns selecting one model out of a group and
discarding the others. Typically one would use the remaining model to make
inferences or predictions. However, model *comparison* involves investigating
the differences between multiple models, including comparing the criterion
values and comparing the estimates and predictions. When we choose to do
selection rather than comparison, we lose all of the information encoded
in the differences between the model -- knowing which differences in models
change criterion values and estimates is crucial information which can inform
our understanding of the system.

### 7M3

We need to fit models to the exact same set of data points in order to compare
them based on WAIC because changing the data points will change the WAIC even
if nothing about the model changes. If we were to use the same number of data
points, but different sets of data, the WAIC will fluctuate based on properties
of the data, and we could get different results when we compare the same
models.

The penalty parameter of the WAIC also depends on the value of $N$, the number
of data points. If we were to drop data points (for example, due to missing
data in some models, but not others), we would expect the WAIC to increase
(become worse)
because we have less information relative to the complexity of the models.
Conversely, if we increased the number of data points the WAIC could be
better just because of that. We could then make an incorrect decision by
comparing the WAICs from models fit on different data.

### 7M4

As the width of the priors decreases (i.e. the priors become more concentrated), the WAIC penalty term shrinks. The WAIC penalty term is based
on the variances of individual probability estimates across samples. As the
width of a prior is narrowed, the model will tend to produce samples for
each individual that are closer together on average and thus the penalty term
will decrease. However, since the `lppd` also changes when we change the
priors we cannot say for sure whether this increases or decreases the overall
WAIC.

### 7M5

When we use informative priors, we make the model more skeptical of extreme
values in the data, and less trustworthy of values that would pull the model
away from the priors. The data thus need to contain a large amount of evidence
to make extreme values more likely in the posterior distribution. Under these
conditions, the model is "excited" less by the training sample -- and thus,
the model fitting process is more robust to variations in the training sample
due to sample error. Ideally, the model will capture less of the noise in the
data while still capturing strong underlying trends, improving the performance
of the model at explaining novel data from the same data generating process.

### 7M6

Overly informative priors, or in the worst case, degenerate priors, will
dominate the model and prevent the model from learning from the data. If
a prior is too narrow, the data cannot provide enough evidence to move the
model away from the priors. Such a model is so skeptical of the data that it
does not pick up the noise from sampling variability in the data, nor does it
pick up any signal from the underlying trends either. Because the model has
learned nothing from the data, we could make predictions just as good by
making up random numbers.

### 7H1




<!-- END OF FILE -->
